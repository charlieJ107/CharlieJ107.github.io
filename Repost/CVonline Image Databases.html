<!DOCTYPE html>


<html lang="zh-cn">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    [Repost] CVonline: Image Databases |  Vankyle
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link rel="alternate" href="/atom.xml" title="Vankyle" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-CVonline Image Databases"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  [Repost] CVonline: Image Databases
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/Repost/CVonline%20Image%20Databases.html" class="article-date">
  <time datetime="2020-05-15T16:00:00.000Z" itemprop="datePublished">2020-05-16</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Repost/">Repost</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">31.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">198 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/"><img src="http://homepages.inf.ed.ac.uk/rbf/CVonline/cvlogo.gif" alt="img"></a></p>
<a id="more"></a>

<hr>
<p>Source page: <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline">http://homepages.inf.ed.ac.uk/rbf/CVonline</a></p>
<p>This is a collated list of image and video databases that people    have found useful for computer vision research and algorithm evaluation.</p>
<p>An important article    <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11263-017-1020-z">How Good Is My Test Data? Introducing Safety Analysis for Computer Vision</a>    (by Zendel, Murschitz, Humenberger, and Herzner)    introduces a methodology for ensuring that your dataset has sufficient    variety that algorithm results on the dataset are representative of    the results that one could expect in a real setting.    In particular, the team have produced a    <a target="_blank" rel="noopener" href="https://vitro-testing.com/cv-hazop/">Checklist</a> of potential    hazards (imaging situations) that may cause algorithms to have problems.    Ideally, test datasets should have examples of the relevant hazards.</p>
<h2 id="Index-by-Topic"><a href="#Index-by-Topic" class="headerlink" title="Index by Topic"></a>Index by Topic</h2><ol>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#action">Action Databases</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#agriculture">Agriculture</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#atre">Attribute recognition</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#autodriving">Autonomous Driving</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#biomed">Biological/Medical</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#Caca">Camera calibration</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#face">Face and Eye/Iris Databases</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#fingerprints">Fingerprints</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#images">General Images</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#rgbd">General RGBD and depth datasets</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#videos">General Videos</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#gesture">Hand, Hand Grasp, Hand Action and Gesture Databases</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#retrieve">Image, Video and Shape Database Retrieval</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#object">Object Databases</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#human">People (static and dynamic), human body pose</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#people">People Detection and Tracking Databases</a> (See also <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#surveillance">Surveillance</a>)</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#remote">Remote Sensing</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#robotics">Robotics</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#scene">Scenes or Places, Scene Segmentation or Classification</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation">Segmentation</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#SLAM">Simultaneous Localization and Mapping</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#surveillance">Surveillance and Tracking</a> (See also <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#people">People</a>)</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#texture">Textures</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#urban">Urban Datasets</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#language">Vision and Natural Language</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#collect">Other Collection Pages</a></li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#misc">Miscellaneous Topics</a></li>
</ol>
<p>Other helpful sites are:</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://academictorrents.com/collection/computer-vision">Academic Torrents - computer vision</a> - a set of 30+ large datasets available in BitTorrent form</li>
<li><a target="_blank" rel="noopener" href="https://www.datasetlist.com/">Machine learning datasets</a> - see CV tab</li>
<li><a target="_blank" rel="noopener" href="http://riemenschneider.hayko.at/vision/dataset/index.php">YACVID</a> - a tagged index to some computer vision datasets</li>
</ol>
<hr>
<h2 id="Action-Databases"><a href="#Action-Databases" class="headerlink" title="Action Databases"></a>Action Databases</h2><p>See also:  <a target="_blank" rel="noopener" href="http://www.actionrecognition.net/">Action Recognition’s</a> dataset summary with league tables (Gall, Kuehne, Bhattarai).</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.twentybn.com/datasets/something-something">20bn-Something-Something</a> - densely-labeled video clips that show humans performing predefined  basic actions with everyday objects (Twenty Billion Neurons GmbH)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/skicyyu/rgbd_recognition">3D online action dataset</a> - There are seven action categories (Microsoft and Nanyang Technological University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/">50 Salads</a> - fully annotated 4.5 hour dataset of RGB-D video + accelerometer data, capturing 25 people preparing two mixed salads each (Dundee University, Sebastian Stein) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~andrea/fpvo">A first-person vision dataset of office activities (FPVO)</a> - FPVO contains first-person video segments of office activities  collected using 12 participants. (G. Abebe, A. Catala, A. Cavallaro)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://activity-net.org/">ActivityNet</a> - A Large-Scale  Video Benchmark for Human Activity Understanding (200 classes, 100  videos per class, 648 video hours) (Heilbron, Escorcia, Ghanem and  Niebles) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.merl.com/demos/merl-shopping-dataset">Action Detection in Videos</a> - MERL Shopping Dataset consists of 106 videos, each of which is a  sequence about 2 minutes long (Michael Jones, Tim Marks) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.eecs.umich.edu/~jjcorso/r/a2d/">Actor and Action Dataset</a> - 3782 videos, seven classes of actors performing eight different actions (Xu, Hsieh, Xiong, Corso) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.ubc.ca/~murphyk/videodata.html">An analyzed collation of various labeled video datasets for action recognition</a> (Kevin Murphy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rtis.oit.unlv.edu/datasets.html">AQA-7</a> - Dataset for assessing the quality of 7 different actions. It contains 1106  action samples and AQA scores. (Parmar, Morris) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html">ASLAN Action similarity labeling challenge</a> database (Orit Kliper-Gross) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://yanweifu.github.io/USAA/download/">Attribute Learning for Understanding Unstructured Social Activity</a> - Database of videos containing 10 categories of unstructured social  events to recognise, also annotated with 69 attributes. (Y. Fu  Fudan/QMUL, T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/audiovisualresearch">Audio-Visual Event (AVE) dataset</a>- AVE dataset contains 4143 YouTube videos covering 28 event categories  and videos in AVE dataset are temporally labeled with audio-visual event boundaries. (Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and  Chenliang Xu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/ava/">AVA: A Video Dataset of Atomic Visual Action</a>- 80 atomic visual actions in 430 15-minute movie clips. (Google Machine Perception Research Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/eccv2018bbdb/">BBDB</a> -  Baseball Database (BBDB) is a large-scale baseball video dataset that  contains 4200 hours of full baseball game videos with 400,000 temporally annotated activity segments. (Shim, Minho, Young Hwi, Kyungmin, Kim,  Seon Joo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/index.html">BEHAVE Interacting Person Video Data with markup</a> (Scott Blunsden, Bob Fisher, Aroosha Laghaee) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs-people.bu.edu/sbargal/BU-action/">BU-action Datasets</a> - Three image action datasets (BU101, BU101-unfiltered,  BU203-unfiltered) that have 1:1 correspondence with classes of the video datasets UCF101 and ActivityNet. (S. Ma, S. A. Bargal, J. Zhang, L.  Sigal, S. Sclaroff.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tele-immersion.citris-uc.org/berkeley_mhad">Berkeley MHAD: A Comprehensive Multimodal Human Action Database</a> (Ferda Ofli) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tele-immersion.citris-uc.org/berkeley_mhad">Berkeley Multimodal Human Action Database</a> - five different modalities to expand the fields of application  (University of California at Berkeley and Johns Hopkins University)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/">Breakfast dataset</a> - It’s a dataset with 1712 video clips showing 10 kitchen activities,  which are hand segmented into 48 atomic action classes . (H. Kuehne, A.  B. Arslan and T. Serre ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.cs.bris.ac.uk/~damen/BEOID/">Bristol Egocentric Object Interactions Dataset</a> - Contains videos shot from a first-person (egocentric) point of view  of 3-5 users performing tasks in six different locations (Dima Damen,  Teesid Leelaswassuk and Walterio Mayol-Cuevas, Bristol University)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/">Brown Breakfast Actions Dataset</a> - 70 hours, 4 million frames of 10 different breakfast preparation activities (Kuehne, Arslan and Serre) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pr.cs.cornell.edu/humanactivities/data.php">CAD-120 dataset</a> - focuses on high level activities and object interactions (Cornell University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pr.cs.cornell.edu/humanactivities/data.php">CAD-60 dataset</a> - The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities (Cornell University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rohitgirdhar.github.io/CATER">CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning</a> - A synthetic video understanding benchmark, with tasks that by-design  require temporal reasoning to be solved (Girdhar, Ramanan) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.fe.uni-lj.si/cvbase06/downloads.html">CVBASE06: annotated sports videos</a> (Janez Pers) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://allenai.org/plato/charades/">Charades Dataset</a> -  10,000 videos from 267 volunteers, each annotated with multiple  activities, captions, objects, and temporal localizations. (Sigurdsson,  Varol, Wang, Laptev, Farhadi, Gupta) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/">Composable activities dataset</a> - Different combinations of 26 atomic actions formed 16 activity  classes which were performed by 14 subjects and annotations were  provided (Pontificia Universidad Catolica de Chile and Universidad del  Norte) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.mica.edu.vn/perso/Tran-Thi-Thanh-Hai/CMDFALL.html">Continuous Multimodal Multi-view Dataset of Human Fall</a> - The dataset consists of both normal daily activities and simulated  falls for evaluating human fall detection. (Thanh-Hai Tran) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pr.cs.cornell.edu/humanactivities/data.php">Cornell Activity Datasets CAD 60, CAD 120</a> (Cornell Robot Learning Lab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.uclouvain.be/ispgroup/Softwares/DeepSport">DeepSport</a> - pairs of successive images captured in different basketball arenas  during professional games, with ground ruth annotations of the ball  position. (UC Louvain ISPGroup)</li>
<li><a target="_blank" rel="noopener" href="http://www.demcare.eu/results/datasets">DemCare dataset</a> - DemCare dataset consists of a set of diverse data collection from  different sensors and is useful for human activity recognition from  wearable/depth and static IP camera, speech recognition for Alzheimmer’s disease detection and physiological data for gait analysis and  abnormality detection. (K. Avgerinakis, A.Karakostas, S.Vrochidis, I.  Kompatsiaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mclab.citi.sinica.edu.tw/dataset/dha/dha.html">Depth-included Human Action video dataset</a> - It contains 23 different actions (CITI in Academia Sinica) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ece.ubc.ca/~mohsena/dmlsmartaction.html">DMLSmartActions dataset</a> - Sixteen subjects performed 12 different actions in a natural manner. (University of British Columbia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://robotics.ait.kyushu-u.ac.jp/~yumi/db/first_dog.html">DogCentric Activity Dataset</a> - first-person videos taken from a camera mounted on top of a <em>dog</em> (Michael Ryoo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CEILIDHDATA/">Edinburgh ceilidh overhead video data</a> - 16 ground-truthed dances viewed from overhead, where the 10 dancers  follow a structured dance pattern (2 different dances). The dataset is  useful for highly structured behavior understanding (Aizeboje, Fisher)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://epic-kitchens.github.io/">EPIC-KITCHENS</a> -  egocentric video recorded by 32 participants in their native kitchen  environments, non-scripted daily activities,  11.5M frames, 39.6K  frame-level action segments and 454.2K object bounding boxes (Damen,  Doughty, Fidler, et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://osf.io/d5k38/wiki/home/">EPFL crepe cooking videos</a> - 6 types of structured cooking activity (12) videos in 1920x1080  resolution (Lee, Ognibene, Chang, Kim and Demiris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.etsmtl.ca/Professeurs/ggagnon/Projects/ai-sports">ETS Hockey Game Event Data Set</a> - This data set contains footage of two hockey games captured using  fixed cameras. (M.-A. Carbonneau, A. J. Raymond, E. Granger, and G.  Gagnon) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vlm1.uta.edu/~zhangzhong/fall_detection/">The Falling Detection dataset</a> - Six subjects in two sceneries performed a series of actions continuously (University of Texas) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bigvid.fudan.edu.cn/FCVID/">FCVID: Fudan-Columbia Video Dataset</a> - 91,223 Web videos annotated manually according to 239 categories (Jiang, Wu, Wang, Xue, Chang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dipersec.king.ac.uk/G3D/">G3D</a> - synchronised  video, depth and skeleton data for 20 gaming actions captured with  Microsoft Kinect (Victoria Bloom) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dipersec.king.ac.uk/G3D/">G3Di</a> - This dataset contains 12 subjects split into 6 pairs (Kingston University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dipersec.king.ac.uk/G3D/">Gaming 3D dataset</a> - real-time action recognition in gaming scenario (Kingston University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ai.stanford.edu/~alireza/GTEA_Gaze_Website/">Georgia Tech Egocentric Activities - Gaze(+)</a> - videos of where people look at and their gaze location (Fathi, Li, Rehg) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB: A Large Human Motion Database</a> (Serre Lab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvssp.org/Hollywood3D/">Hollywood 3D dataset</a> - 650 3D video clips, across 14 action classes (Hadfield and Bowden) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.irisa.fr/vista/actions/hollywood2/">Human Actions and Scenes Dataset</a> (Marcin Marszalek, Ivan Laptev, Cordelia Schmid) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://humamalwassel.com/publication/action-search/">Human Searches</a> Search sequences of human annotators that were tasked to spot actions  in AVA and THUMOS14 datasets. (Alwassel, H., Caba Heilbron, F., Ghanem,  B.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.di.ens.fr/willow/research/actionordering/">Hollywood Extended</a> - 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies. (Bojanowski, Lajugie,  Bach, Laptev, Ponce, Schmid, and Sivic) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://humaneva.is.tue.mpg.de/datasets_human_1">HumanEva</a>: Synchronized Video and Motion Capture Dataset for Evaluation of Articulated Human Motion (Brown University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tna.europarchive.org/20100413151426/scienceandresearch.homeoffice.gov.uk/hosdb/cctv-imaging-technology/i-lids/index.html">I-LIDS video event image dataset (Imagery library for intelligent detection systems)</a> (Paul Hosner) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://kahlan.eps.surrey.ac.uk/i3dpost_action/">I3DPost Multi-View Human Action Datasets</a> (Hansung Kim) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://robotics.dei.unipd.it/actions/index.php/overview">IAS-lab Action dataset</a> - contain sufficient variety of actions and number of people performing the actions (IAS Lab at the University of Padua) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.forth.gr/cvrl/evaco/">ICS-FORTH MHAD101 Action Co-segmentation</a> - 101 pairs of long-term action sequences that share one or multiple  common actions to be co-segmented, contains both 3d skeletal and video  related frame-based features (Universtiy of Crete and FORTH-ICS, K.  Papoutsakis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/suriyasingh/IIIT-Extreme-Sports">IIIT Extreme Sports</a> - 160 first person (egocentric) sport videos from YouTube with frame  level annotations of 18 action classes. (Suriya Singh, Chetan Arora, and C. V. Jawahar. Trajectory Aligned) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://4drepository.inrialpes.fr/public/viewgroup/6">INRIA Xmas Motion Acquisition Sequences (IXMAS)</a> (INRIA) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/gaochenqiang/publication/infrared-action-dataset">InfAR Dataset</a> -Infrared Action Recognition at Different Times  Neurocomputing(Chenqiang Gao, Yinhe Du, Jiang Liu, Jing Lv, Luyu Yang,  Deyu Meng, Alexander G. Hauptmann) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://jhmdb.is.tue.mpg.de/">JHMDB: Joints for the HMDB dataset</a> (J-HMDB) based on 928 clips from HMDB51 comprising 21 action categories (Jhuang, Gall, Zuffi, Schmid and Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrc.ece.utexas.edu/mryoo/jpl-interaction.html">JPL First-Person Interaction dataset</a> - 7 types of human activity videos taken from a first-person viewpoint (Michael S. Ryoo, JPL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.inf-cv.uni-jena.de/JAR_Aibo.html">Jena Action Recognition Dataset</a> - Aibo dog actions (Korner and Denzler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.leightley.com/k3da/">K3Da - Kinect 3D Active dataset</a> - K3Da (Kinect 3D active) is a realistic clinically relevant human  action dataset containing skeleton, depth data and associated  participant information (D. Leightley, M. H. Yap, J. Coulson, Y.  Barnouin and J. S. McPhee) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">Kinetics Human Action Video Dataset</a> - 300,000 video clips, 400 human action classe, 10 second clips, single action per clip (Kay, Carreira, et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cvhci.anthropomatik.kit.edu/projects/act/kitchen/">KIT Robo-Kitchen Activity Data Set</a> - 540 clips of 17 people performing 12 complex   kitchen activities.  (L. Rybok, S. Friedberger, U. D. Hanebeck, R. Stiefelhagen) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.nada.kth.se/cvap/actions/">KTH human action recognition database</a> (KTH CVAP lab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cvhci.anthropomatik.kit.edu/projects/act/minta">Karlsruhe Motion, Intention, and Activity Data set (MINTA)</a> - 7 types of activities of daily living including   fully motion  primitive segments. (D. Gehrig, P. Krauthausen, L. Rybok, H. Kuehne, U.  D.  Hanebeck, T. Schultz, R. Stiefelhagen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Leeds Activity Dataset–Breakfast (LAD–Breakfast)</a> - It is composed of 15 annotated videos, representing five different  people having breakfast or other simple meal; (John Folkesson et al.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://liris.cnrs.fr/voir/activities-dataset/">LIRIS Human Activities Dataset</a> - contains (gray/rgb/depth) videos showing people performing various  activities (Christian Wolf, et al, French National Center for Scientific Research) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset">MEXaction2 action detection and localization dataset</a> - To support the development and evaluation of methods for ‘spotting’  instances of short actions in a relatively large video database: 77  hours, 117 videos (Michel Crucianu and Jenny Benois-Pineau) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/piergiaj/mlb-youtube">MLB-YouTube</a> - Dataset for activity recognition in baseball videos (AJ Piergiovanni, Michael Ryoo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://moments.csail.mit.edu/">Moments in Time Dataset</a> - Moments in Time Dataset 1M 3-second videos annotated with action type,  the largest dataset of its kind for action recognition and understanding in video. (Monfort, Oliva, et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/">MPII Cooking Activities Dataset</a> for fine-grained cooking activity recognition, which also includes the  continuous pose estimation challenge (Rohrbach, Amin, Andriluka and  Schiele) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-2-dataset/">MPII Cooking 2 Dataset</a> - A large dataset of fine-grained cooking activities, an extension of  the MPII Cooking Activities Dataset. (Rohrbach, Rohrbach, Regneri, Amin, Andriluka, Pinkal, Schiele) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/">MSR-Action3D</a> - benchmark RGB-D action dataset (Microsoft Research Redmond and University of Wollongong) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.ucf.edu/~oreifej/HON4D.html">MSRActionPair dataset</a> - : Histogram of Oriented 4D Normals for Activity Recognition from  Depth Sequences (University of Central Florida and Microsoft) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/">MSRC-12 Kinect gesture data set</a> - 594 sequences and 719,359 frames from people performing 12 gestures (Microsoft Research Cambridge) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/download/details.aspx?id=52283">MSRC-12 dataset</a> - sequences of human movements, represented as body-part locations, and the associated gesture (Microsoft Research Cambridge and University of  Cambridge) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/">MSRDailyActivity3D Dataset</a> - There are 16 activities (Microsoft and the Northwestern University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dpi.physik.uni-goettingen.de/~eaksoye/dataset.html">ManiAc</a> RGB-D action dataset:  different manipulation actions, 15 different  versions, 30 different objects manipulated, 20 long and complex chained  manipulation sequences (Eren Aksoy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mivia.unisa.it/datasets/video-analysis-datasets/mivia-action-dataset/">Mivia dataset</a> - It consists of 7 high-level actions performed by 14 subjects. (Mivia Lab at the University of Salemo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ParitoshParmar/MTL-AQA">MTL-AQA</a> -  Multitask learning dataset for assessing quality of Olympic Diving. More than 1500 samples. It contains videos of action samples, fine-grained  action class, expert commentary (AQA-oriented captions), AQA scores from judges. Videos from multiple views included wherever available. Can be  used for captioning, and fine-grained action recognition, apart from  AQA. (Parmar, Morris) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dipersec.king.ac.uk/MuHAVi-MAS/">MuHAVi</a> - Multicamera Human Action Video Data (Hossein Ragheb) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://humansensing.cs.cmu.edu/mad/download.html">Multi-modal action detection (MAD) Dataset</a> - It contains 35 sequential actions performed by 20 subjects. (CarnegieMellon University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://users.eecs.northwestern.edu/~jwa368/my_data.html">Multiview 3D Event dataset</a> - This dataset includes 8 categories of events performed by 8 subjects  (University of California at Los Angles) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/muralab/Low-Resolution-FIR-Action-Dataset">Nagoya University Extremely Low-resolution FIR Image Action Dataset</a> - Action recognition dataset captured by a 16x16 low-resolution FIR sensor. (Nagoya University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/shahroudy/NTURGB-D">NTU RGB+D Action Recognition Dataset</a> - NTU RGB+D is a large scale dataset for human action recognition(Amir Shahroudy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://users.eecs.northwestern.edu/~jwa368/my_data.html">Northwestern-UCLA Multiview Action 3D</a> - There are 10 action categories:(Northwestern University and University of California at Los Angles) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://smartcity.csr.unibo.it/activity-recognition/">Office Activity Dataset</a> - It consists of skeleton data acquired by Kinect 2.0 from different  subjects performing common office activities. (A. Franco, A. Magnani, D. Maiop) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/index.html">Oxford TV based human interactions</a> (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/htwang14/PA-HMDB51">PA-HMDB51</a> -  human action video (592) dataset with potential privacy leak attributes  annotated: skin color, gender, face, nudity, and relationship (Wang, Wu, Wang, Wang, Jin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cse.uoi.gr/~mvrigkas/#downloads">Parliament</a> - The Parliament dataset is a collection of 228 video sequences,  depicting political speeches in the Greek parliament. (Michalis Vrigkas, Christophoros Nikou, Ioannins A. kakadiaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://adas.cvc.uab.es/phav/"> Procedural Human Action Videos</a> - This dataset contains about 40,000 videos for human action  recognition that had been generated using a 3D game engine. The dataset  contains about 6 million frames which can be used to train and evaluate  models not only action recognition but also models for depth map  estimation, optical flow, instance segmentation, semantic segmentation,  3D and 2D pose estimation, and attribute learning. (Cesar Roberto de  Souza) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://watchnpatch.cs.cornell.edu/">RGB-D activity dataset</a> - Each video in the dataset contains 2-7 actions involving interaction  with different objects. (Cornell University and Stanford University)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.bris.ac.uk/data/dataset/66qry08cv1fj1eunwxwob3fjz">RGBD-Action-Completion-2016</a> - This dataset includes 414 complete/incomplete object interaction  sequences, spanning six actions and presenting RGB, depth and skeleton  data. (Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1601.05511">RGB-D-based Action Recognition Datasets</a> - Paper that includes the list and links of different rgb-d action  recognition datasets. (Jing Zhang, Wanqing Li, Philip O. Ogunbona,  Pichao Wang, Chang Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.uestcrobot.net/en/?q=download">RGBD-SAR Dataset</a> - RGBD-SAR Dataset (University of Electronic Science and Technology of China and Microsoft) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.rochester.edu/~rmessing/uradl/">Rochester Activities of Daily Living Dataset</a> (Ross Messing) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://saras-esad.grand-challenge.org/Dataset/">SARAS endoscopic vision challenge for surgeon action detection</a> - 22,601 annotated training frames with 28,055 action instances from 21 different action classes (Cuzzolin, Singh Bawa, Skarga-Bandurova,  Singh) [16/4/20]</li>
<li><a target="_blank" rel="noopener" href="http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html">SBU Kinect Interaction Dataset</a> - It contains eight types of interactions (Stony Brook University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/README.txt">SBU-Kinect-Interaction dataset v2.0</a> - It comprises of RGB-D video sequences of humans performing interaction activities (Kiwon Yun etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html">SDHA Semantic Description of Human Activities 2010 contest - Human Interactions</a> (Michael S. Ryoo, J. K. Aggarwal, Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrc.ece.utexas.edu/SDHA2010/Aerial_View_Activity.html">SDHA Semantic Description of Human Activities 2010 contest - aerial views</a> (Michael S. Ryoo, J. K. Aggarwal, Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mostafa-saad/deep-activity-rec">SFU Volleyball Group Activity Recognition</a> - 2 levels annotations dataset (9 players’ actions and 8 scene’s  activity) for volleyball videos. (M. Ibrahim, S. Muralidharan, Z. Deng,  A. Vahdat, and G. Mori / Simon Fraser University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://isee.sysu.edu.cn/resource">SYSU 3D Human-Object Interaction Dataset</a> - Forty subjects perform 12 distinct activities (Sun Yat-sen University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.projects.science.uu.nl/shakefive/">ShakeFive Dataset</a> - contains only two actions, namely hand shake and high five. (Universiteit Utrecht) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.projects.science.uu.nl/shakefive/">ShakeFive2</a> - A dyadic human interaction dataset with limb level  annotations on 8  classes in 153 HD videos (Coert van Gemeren, Ronald Poppe, Remco  Veltkamp) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://silviogiancola.github.io/SoccerNet/">SoccerNet</a> - Scalable dataset for action spotting in soccer videos: 500 soccer games fully annotated with main actions (goal, cards, subs) and more than 13K soccer games annotated with 500K commentaries for event captioning and  game summarization. (Silvio Giancola, Mohieddine Amine, Tarek Dghaily,  Bernard Ghanem) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvlab.cse.msu.edu/project-svw.html">Sports Videos in the Wild (SVW)</a> - SVW is comprised of 4200 videos captured solely with smartphones by  users of Coach Eye smartphone app, a leading app for sports training  developed by TechSmith corporation. (Seyed Morteza Safdarnejad, Xiaoming Liu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.stanford.edu/lijiali/event_dataset/">Stanford Sport Events dataset</a> (Jia Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php">THU-READ(Tsinghua University RGB-D Egocentric Action Dataset)</a> - THU-READ is a large-scale dataset for action recognition in RGBD  videos with pixel-layer hand annotation. (Yansong Tang, Yi Tian, Jiwen  Lu, Jianjiang Feng, Jie Zhou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.thumos.info/home.html">THUMOS</a> - Action  Recognition in Temporally Untrimmed Videos! -  430 hours of video data  and 45 million frames (Gorban, Idrees, Jiang, Zamir, Laptev Shah,  Sukthanka) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://project.inria.fr/toyotasmarthome/">Toyota Smarthome dataset</a> - Dataset for Real-world activities of Daily Living (Toyota Motors Europe &amp; INRIA Sophia Antipolis) [30/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ias.in.tum.de/software/kitchen-activity-data">TUM Kitchen Data Set of Everyday Manipulation Activities</a> (Moritz Tenorth, Jan Bandouch) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~alonso/tv_human_interactions.html">TV Human Interaction Dataset</a> (Alonso Patron-Perez) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://media.tju.edu.cn/tju_dataset.html">The TJU dataset</a> - contains 22 actions performed by 20 subjects in two different  environments; a total of 1760 sequences. (Tianjin University) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.eecs.ucf.edu/data/SmartPhone/">UCF-iPhone Data Set</a> - 9 Aerobic actions were recorded from (6-9) subjects using the  Inertial Measurement Unit (IMU) on an Apple iPhone 4 smartphone. (Corey  McCall, Kishore Reddy and Mubarak Shah) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones">UCI Human Activity Recognition Using Smartphones Data Set</a> - recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial  sensors (Anguita, Ghio, Oneto, Parra, Reyes-Ortiz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rtis.oit.unlv.edu/datasets.html">UNLV Dive &amp; Gymvault</a> - Dataset for assessing quality of Olympic Diving and Olympic Gymnastic Vault. It consists of videos of action samples and corresponding action quality scores. (Parmar, Morris) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.upcv.upatras.gr/personal/kastaniotis/datasets.html">The UPCV action dataset</a> - The dataset consists of 10 actions performed by 20 subjects twice. (University of Patras) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/data/UCF101.php">UCF 101 action dataset</a> 101 action classes, over 13k clips and 27 hours of video data (Univ of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/projects/real-world/">UCF-Crime Dataset: Real-world Anomaly Detection in Surveillance Videos</a> - A large-scale dataset for real-world anomaly detection in  surveillance videos. It consists of 1900 long and untrimmed real-world  surveillance videos (of 128 hours), with 13 realistic anomalies such as  fighting, road accident, burglary, robbery, etc. as well as normal  activities. (Center for Research in Computer Vision, University of  Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.ucf.edu/~smasood/datasets/UCFKinect.zip">UCFKinect</a> - The dataset is composed of 16 actions (University of Central Florida Orlando) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~tianmin.shu/SocialAffordance/index.html">UCLA Human-Human-Object Interaction (HHOI) Dataset Vn1</a> - Human interactions in RGB-D videos (Shu, Ryoo, and Zhu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~tianmin.shu/SocialAffordanceGrammar/">UCLA Human-Human-Object Interaction (HHOI) Dataset Vn2</a> - Human interactions in RGB-D videos (version 2) (Shu, Gao, Ryoo, and Zhu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.ucr.edu/~amitrc/datasets.php">UCR Videoweb Multi-camera Wide-Area Activities Dataset</a> (Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.utdallas.edu/~kehtar/UTD-MHAD.html">UTD-MHAD</a> - Eight subjects performed 27 actions four times. (University of Texas at Dallas) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html">UTKinect dataset</a> - Ten types of human actions were performed twice by 10 subjects (University of Texas) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://staffhome.ecm.uwa.edu.au/~00053650/databases.html">UWA3D Multiview Activity Dataset</a> - Thirty activities were performed by 10 individuals (University of Western Australia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://server.cs.ucf.edu/~vision/data/UCF50.rar">Univ of Central Florida - 50 Action Category Recognition in Realistic Videos (3 GB)</a> (Kishore Reddy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://server.cs.ucf.edu/~vision/data/UCF-ARG.html">Univ of Central Florida - ARG</a> Aerial camera, Rooftop camera and Ground camera (UCF Computer Vision Lab) [Before 28/12/19]</li>
<li>[Univ of Central Florida - Feature Films Action Dataset](<a target="_blank" rel="noopener" href="http://server.cs.ucf.edu/~vision/projects/action_mach/Slaps">http://server.cs.ucf.edu/~vision/projects/action_mach/Slaps</a> and kisses.rar) (Univ of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/data/UCF_Sports_Action.php">Univ of Central Florida - Sports Action Dataset</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://server.cs.ucf.edu/~vision/projects/liujg/YouTube_Action_dataset.html">Univ of Central Florida - YouTube Action Dataset (sports)</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html">Unsegmented Sports News Videos</a> - Database of 74 sports news videos tagged with 10 categories of  sports. Designed to test multi-label video tagging. (T. Hospedales,  Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.projects.science.uu.nl/umpm/">Utrecht Multi-Person Motion Benchmark (UMPM).</a> - a collection of video recordings of people together with a ground  truth based on motion capture data. (N.P. van der Aa, X. Luo, G.J.  Giezeman, R.T. Tan, R.C. Veltkamp.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.viratdata.org/">VIRAT Video Dataset</a> - event  recognition from two broad categories of activities (single-object and  two-objects) which involve both human and vehicles. (Sangmin Oh et al)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://profs.sci.univr.it/~cristanm/datasets.html">Verona Social interaction dataset</a> (Marco Cristani) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dipersec.king.ac.uk/VIHASI/">ViHASi: Virtual Human Action Silhouette Data</a> (userID: VIHASI password: virtual$virtual) (Hossein Ragheb, Kingston University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.ucr.edu/~amitrc/datasets.php">Videoweb (multicamera) Activities Dataset</a> (B. Bhanu, G. Denina, C. Ding, A. Ivers, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, B. Varda) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://csee.wvu.edu/~vkkulathumani/wvu-action.html">WVU Multi-view action recognition dataset</a> (Univ. of West Virginia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://visionscience.com/pipermail/visionlist/2013/006219.html">WorkoutSU-10</a> Kinect dataset for exercise actions (Ceyhun Akgul) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vpa.sabanciuniv.edu.tr/phpBB2/vpa_views.php?s=31&serial=36">WorkoutSU-10 dataset</a> - contains exercise actions selected by professional trainers for therapeutic purposes. (Sabanci University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/">Wrist-mounted camera video dataset</a> - object manipulation (Ohnishi, Kanehira, Kanezaki, Harada) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bit.ly/Zy2ZR1">YouCook</a> - 88 open-source YouTube cooking videos with annotations (Jason Corso) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube8m/">YouTube-8M Dataset</a> -A Large and Diverse Labeled Video Dataset for Video Understanding Research(Google Inc.) [Before 28/12/19]</li>
</ol>
<h2 id="Agriculture"><a href="#Agriculture" class="headerlink" title="Agriculture"></a>Agriculture</h2><ol>
<li><a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.168158">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.eng.au.dk/fieldsafe/">Fieldsafe</a> - A multi-modal dataset for obstacle detection in agriculture.  (Aarhus University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://limu.ait.kyushu-u.ac.jp/~agri/komatsuna/">KOMATSUNA dataset</a> - The datasets is designed for instance segmentation, tracking and  reconstruction for leaves using both sequential multi-view RGB images  and depth images. (Hideaki Uchiyama, Kyushu University) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.eng.au.dk/leaf-counting-dataset/">Leaf counting dataset</a> - Dataset for estimating the growth stage of small plants. (Aarhus University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.plant-phenotyping.org/CVPPP2014-dataset">Leaf Segmentation Challenge</a>Tobacco and arabidopsis plant images (Hanno Scharr, Massimo Minervini, Andreas  Fischbach, Sotirios A. Tsaftaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network"> Multi-species fruit flower detection</a> - This dataset consists of four sets of flower images, from three  different tree species: apple, peach, and pear, and accompanying ground  truth images. (Philipe A. Dias, Amy Tabb, Henry Medeiros) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.plant-phenotyping.org/datasets-home">Plant Phenotyping Datasets</a> - plant data suitable for plant and leaf detection, segmentation,  tracking, and species recognition (M. Minervini, A. Fischbach, H.  Scharr, S. A. Tsaftaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.eng.au.dk/plant-seedlings-dataset/">Plant seedlings dataset</a> - High-resolution images of 12 weed species.  (Aarhus University) [Before 28/12/19]</li>
</ol>
<h2 id="Attribute-recognition"><a href="#Attribute-recognition" class="headerlink" title="Attribute recognition"></a>Attribute recognition</h2><ol>
<li><a target="_blank" rel="noopener" href="http://yanweifu.github.io/USAA/download/">Attribute Learning for Understanding Unstructured Social Activity</a> - Database of videos containing 10 categories of unstructured social  events to recognise, also annotated with 69 attributes. (Y. Fu  Fudan/QMUL, T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cvml.ist.ac.at/AwA2/">Animals with Attributes 2</a> - 37322 (freely licensed) images of 50 animal classes with 85 per-class binary attributes. (Christoph H. Lampert, IST Austria) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Birds</a>This database contains 600 images (100 samples each) of six different  classes of birds. (Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Butterflies</a>This database contains 619 images of seven different classes of butterflies. (Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://caer-dataset.github.io/">CAER (Context-Aware Emotion Recognition)</a> - Large scale image and video dataset for emotion recognition, and  facial expression recognition (Lee, Kim, Kim, Park, and Sohn) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://calvin.inf.ed.ac.uk/datasets/">CALVIN research group datasets</a> - object detection with eye tracking, imagenet bounding boxes,  synchronised activities, stickman and body poses, youtube objects,  faces, horses, toys, visual attributes, shape classes (CALVIN ggroup)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> - Large-scale CelebFaces Attributes Dataset(Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/vana77/DukeMTMC-attribute">DukeMTMC-attribute</a> - 23 pedestrian attributes for DukeMTMC-reID (Lin, Zheng, Zheng, Wu and Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://sunai.uoc.edu/emotic/">EMOTIC (EMOTIons in Context)</a> - Images of people (34357) embedded in their natural environments,  annotated with 2 distinct emotion representation. (Ronak kosti, Agata  Lapedriza, Jose Alvarez, Adria Recasens) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://jurie.users.greyc.fr/datasets/hat.html">HAT</a> database of 27 human attributes (Gaurav Sharma, Frederic Jurie) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvit.iiit.ac.in/projects/relativeParts/">LFW-10 dataset for learning relative attributes</a> - A dataset of 10,000 pairs of face images with instance-level  annotations for 10 attributes. (CVIT, IIIT Hyderabad. ) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/vana77/Market-1501_Attribute">Market-1501-attribute</a> - 27 visual attributes for 1501 shoppers. (Lin, Zheng, Zheng, Wu and Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vcc.szu.edu.cn/research/2017/RSCM.html">Multi-Class Weather Dataset</a> - Our multi-class benchmark dataset contains 65,000 images from 6  common categories for sunny, cloudy, rainy, snowy, haze and thunder  weather. This dataset benefits weather classification and attribute  recognition. (Di Lin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://goo.gl/DKuhlY">Person Recognition in Personal Photo Collections</a> - we introduced three harder splits for evaluation and long-term  attribute annotations and per-photo timestamp metadata. (Oh, Seong Joon  and Benenson, Rodrigo and Fritz, Mario and Schiele, Bernt) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/ego_snappoints/">UT-Zappos50K Shoes</a> - Large scale shoe dataset consisting of 50,000 catalog images and over 50,000 pairwise relative attribute labels on 11 fine-grained attributes (Aron Yu, Mark Stephenson, Kristen Grauman, UT Austin) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/s1151656/resources.html">Visual Attributes Dataset</a> visual attribute annotations for over 500 object classes (animate and  inanimate) which are all represented in ImageNet. Each object class is  annotated with visual attributes based on a taxonomy of 636 attributes  (e.g., has fur, made of metal, is round). (Before 30/12/19) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://tribhuvanesh.github.io/vpa/">The Visual Privacy (VISPR) Dataset</a> - Privacy Multilabel Dataset (22k images, 68 privacy attributes) (Orekondy, Schiele, Fritz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
</ol>
<h2 id="Autonomous-Driving"><a href="#Autonomous-Driving" class="headerlink" title="Autonomous Driving"></a>Autonomous Driving</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/amuse/">AMUSE</a> -The automotive multi-sensor (AMUSE) dataset taken in real traffic  scenes during multiple test drives. (Philipp Koschorrek etc.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://apolloscape.auto/car_instance.html">ApolloCar3D</a> - 5000 labelled images with 60K car instances (Song, Wang, Zhou, Zhu, Guan, Dai, Su, Li, Yang) [26/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://apolloscape.auto/">ApolloScape</a> - high resolution cameras and a Riegl acquisition system. Our dataset is collected in  different cities under various traffic conditions. 74555 video frames  and their pixel-level and instance-level annotations (Peking University / Baido) [18/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://www.argoverse.org/">Argoverse</a> - Two public  datasets supported by highly detailed maps to test, experiment, and  teach self-driving vehicles how to understand the world around them;  more than 300,000 curated scenarios, 3D tracking annotations for 113  scenes and 324,557 interesting vehicle trajectories for motion  forecasting (Chang, Lambert, Sangkloy, Singh, Bak, Hartnett, Wang, Carr, Lucey, Ramanan, Hays) [18/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://adas.cvc.uab.es/site/elektra">Autonomous Driving</a> - Semantic segmentation, pedestrian detection, virtual-world data, far  infrared, stereo,driver monitoring. (CVC research center and the UAB and UPC universities) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://hci.iwr.uni-heidelberg.de/node/6132">Bosch Small Traffic Lights Dataset (BSTLD)</a> - A dataset for traffic light detection, tracking, and classification. [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drivingstereo-dataset.github.io/">DrivingStereo</a> - A Large-Scale Dataset for Stereo Matching in Autonomous Driving  Scenarios. 180k stereo images covering a diverse set of driving  scenarios (Yang, Song, Huang, Deng, Shi, Zhou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://boxy-dataset.com/">Boxy vehicle detection dataset</a> - A vehicle detection dataset with 1.99 million annotated vehicles in  200,000 images. It contains AABB and keypoint labels. [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/VRU-intention/casr/blob/master/readme.txt">CASR: Cyclist Arm Sign Recognition</a> - Small clips of ~10 seconds showing cyclists performing arm signs. The videos are acquired with a consumer-graded  camera. There are  219 arm  sign  actions annotated. (Zhijie Fang, Antonio M. Lopez) [13/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/event_driving_datasets.html">Driving Event Camera Datasets</a> - sequences that were recorded with a VGA (640x480) event camera  (Samsung DVS Gen3) and a conventional RGB camera (Huawei P20 Pro) placed on the windshield of a car driving through Zurich. (Davide Scaramuzza,  Henri Rebecq) [23/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://robots.engin.umich.edu/SoftwareData/Ford">Ford Campus Vision and Lidar Data Set</a> - time-registered data from professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3  omnidirectional camera system (Pandey, McBride, Eustice) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/perso.lcpc.fr/tarel.jean-philippe/bdd/frida.html">FRIDA (Foggy Road Image DAtabase) Image Database</a> - images for performance evaluation of visibility and contrast  restoration algorithms. FRIDA: 90 synthetic images of 18 urban road  scenes. FRIDA2: 330 synthetic images of 66 diverse road scenes, with  viewpoint closed to that of the vehicle’s driver. (Tarel, Cord,  Halmaoui, Gruyer, Hautiere) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://usa.honda-ri.com/H3D">H3D - Honda Research 3D dataset</a> - 360 degree LiDAR dataset (dense pointcloud from Velodyne-64), 160  crowded and highly interactive traffic scenes, 1,071,302 3D bounding box labels, 8 common classes of traffic participants (Patil, Malla, Gang,  Chen) [18/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/House3D">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://idd.insaan.iiit.ac.in/">India Driving Dataset (IDD)</a> - unstructured driving conditions from India with 50,000 frames (10,000 semantic, and 40,000 coarse annotations) for training autonomous cars  to see using object detection, scene-level and instance-level semantic  segmentation (CVIT, IIIT Hyderabad and Intel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/">Joint Attention in Autonomous Driving (JAAD)</a> -  The dataset includes instances of pedestrians and cars  intended  primarily for the purpose of behavioural studies and  detection in the  context of autonomous driving. (Iuliia Kotseruba, Amir Rasouli and John  K. Tsotsos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrr.ucsd.edu/LISA/vehicledetection.html">LISA Vehicle Detection Dataset</a> - colour first person driving video under various lighting and traffic conditions (Sivaraman, Trivedi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://unsupervised-llamas.com/">LLAMAS Unsupervised dataset</a> - A lane marker detection and segmentation dataset of 100,000 images  with 3d lines, pixel level dashed markers, and curves for individual  lines. [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.6d-vision.com/lostandfounddataset">Lost and Found Dataset</a> - The Lost and Found Dataset addresses the problem of detecting  unexpected small road hazards (often caused by lost cargo) for  autonomous driving applications. (Sebastian Ramos, Peter Pinggera,  Stefan Gehrig, Uwe Franke, Rudolf Mester, Carsten Rother) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://daniilidis-group.github.io/mvsec/">Multi Vehicle Stereo Event Camera Dataset</a> - Multiple sequences containing a stereo pair of DAVIS 346b event  cameras with ground truth poses, depth maps and optical flow. (lex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, Kostas  Daniilidis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=3D0">RESIDE (Realistic Single Image DEhazing)</a> - The current largest-scale benchmark consisting of both synthetic and  real-world hazy images, for image dehazing research.  RESIDE highlights  diverse data sources and image contents, and serves various training or  evaluation purposes. (Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan  Feng, Wenjun Zeng, Zhangyang Wang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://semantic-kitti.org/">semanticKITTI</a> - A Dataset  for Semantic Scene Understanding using LiDAR Sequences (Behley, Garbade, Milioto, Quenzel, Behnke, Stachniss, Gall) [18/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://synthia-dataset.net/dataset/">SYNTHetic collection of Imagery and Annotations</a> - The purpose of aiding semantic segmentation and related scene  understanding problems in the context of driving scenarios. (Computer  vision center,UAB) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://synthia-dataset.net/">SYNTHIA</a> - Large set (~half million) of virtual-world images for training autonomous cars to see.  (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.tromai.icoc.me/">TRoM: Tsinghua Road Markings</a> - This is a dataset which contributes to the area of road marking  segmentation for Automated Driving and ADAS. (Xiaolong Liu, Zhidong  Deng, Lele Cao, Hongchao Lu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.pf.bgu.tum.de/pub/testdaten.html">TUM City Campus</a> - Urban point clouds taken by Mobile Laser Scanning (MLS) for  classification, object extraction and change detection (Stilla, Hebel,  Xu, Gehrung) [3/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://robots.engin.umich.edu/SoftwareData/NCLT">University of Michigan North Campus Long-Term Vision and LIDAR Dataset</a> - 27 sessions spaced approximately biweekly over the course of 15  months, indoors and outdoors,  varying trajectories, different times of  the day across all four seasons. Includes: moving obstacles (e.g.,  pedestrians, bicyclists, and cars), changing lighting, varying  viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction. Includes  ground-truth pose. (Carlevaris-Bianco, Ushani, Eustice) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/uzh-fpv.html">UZH-FPV Drone Racing Dataset</a> - for visual inertial odometry and SLAM. 28 real-world first-person  view sequences both indoors and outdoors, cintaining images, IMU, and  events and ground truth (Delmerico, Cieslewski, Rebecq, Faessler,  Scaramuzza) [Before 28/12/19]</li>
</ol>
<h2 id="Biological-Medical"><a href="#Biological-Medical" class="headerlink" title="Biological/Medical"></a>Biological/Medical</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.nitrc.org/projects/msseg">2008 MICCAI MS Lesion Segmentation Challenge</a> (National Institutes of Health Blueprint for Neuroscience Research) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ragavvenkatesan/np-mil/blob/master/data/DR_data.mat">ASU DR-AutoCC Data</a> -  a Multiple-Instance Learning feature space for a diabetic  retinopathy classification dataset (Ragav Venkatesan, Parag Chandakkar,  Baoxin Li - Arizona State University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.168158">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dsp.utoronto.ca/projects/ADP/ADP_Database/">ADP: Atlas of Digital Pathology</a> - 17,668 histological patch images extracted from 100 slides annotated  with up to 57 hierarchical tissue types (HTTs) from different organs -  the aim is to provide training data for supervised multi-label learning  of tissue types in a digitized whole slide image (Hosseini, Chan, Tse,  Tang, Deng, Norouzi, Rowsell, Plataniotis, Damaskinos) [14/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://spineweb.digitalimaginggroup.ca/spineweb/index.php?n=Main.Datasets">Annotated Spine CT Database</a> for Benchmarking of Vertebrae Localization,  125 patients, 242 scans (Ben Glockern) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://braintumorsegmentation.org/">BRATS</a> - the  identification and segmentation of tumor structures in multiparametric  magnetic resonance images of the brain (TU Munchen etc.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">Breast Ultrasound Dataset B</a> - 2D Breast Ultrasound Images with 53 malignant lesions and 110 benign  lesions. (UDIAT Diagnostic Centre, M.H. Yap, R. Marti) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/calgary-campinas-dataset/home">Calgary-Campinas Public Brain MR Dataset</a>: T1-weighted brain MRI volumes acquired in 359 subjects on scanners from three different vendors (GE, Philips, and Siemens) and at two magnetic  field strengths (1.5 T and 3 T). The scans correspond to older adult  subjects. (Souza, Roberto, Oeslle Lucena, Julia Garrafa, David Gobbi,  Marina Saluzzi, Simone Appenzeller, Leticia Rittner, Richard Frayne, and Roberto Lotufo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ThoroughImages/CAMEL">CAMEL colorectal adenoma dataset</a> - image-level labels for weakly supervised learning containing 177  whole slide images (156 contain adenoma) gathered and labeled by  pathologists (Song and Wang) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://stanfordmlgroup.github.io/competitions/chexpert/">CheXpert</a> - a large dataset of chest X-rays and competition for automated chest  x-ray interpretation, which features uncertainty labels and  radiologist-labeled reference standard evaluation sets (Irvin, Rajpurkar et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://camma.u-strasbg.fr/datasets">Cholec80</a>: 80 gallbladder laparoscopic videos annotated with phase and tool information. (Andru Putra Twinanda) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.warwick.ac.uk/BIAlab/data/CRChistoLabeledNucleiHE/">CRCHistoPhenotypes - Labeled Cell Nuclei Data</a> - colorectal cancer?histology images?consisting of nearly 30,000 dotted nuclei with over 22,000 labeled with the cell type (Rajpoot +  Sirinukunwattana) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.inf-cv.uni-jena.de/Research/Datasets/Cavy+Dataset.html">Cavy Action Dataset</a> - 16 sequences with 640 x 480 resolutions recorded at 7.5 frames per  second (fps) with approximately 31621506 frames in total (272 GB) of  interacting cavies (guinea pig) (Al-Raziqi and Denzler) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Datasets.html">Cell Tracking Challenge Datasets</a> - 2D/3D time-lapse video sequences  with ground truth(Ma et al., Bioinformatics 30:1609-1617, 2014) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://image.diku.dk/emphysema_database/">Computed Tomography Emphysema Database</a> (Lauge Sorensen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bigr.nl/research/projects/copd">COPD Machine Learning Dataset</a> - A collection of feature datasets derived from lung computed  tomography (CT) images, which can be used in diagnosis of chronic  obstructive pulmonary disease (COPD). The images in this database are  weakly labeled, i.e. per image, a diagnosis(COPD or no COPD) is given,  but it is not known which parts of the lungs are affected. Furthermore,  the images were acquired at different sites and with different scanners. These problems are related to two learning scenarios in machine  learning, namely multiple instance learning or weakly supervised  learning, and transfer learning or domain adaptation. (Veronika  Cheplygina, Isabel Pino Pena, Jesper Holst Pedersen, David A. Lynch,  Lauge S., Marleen de Bruijne) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cremi.org/data">CREMI: MICCAI 2016 Challenge</a> - 6 volumes of electron microscopy of neural tissue,neuron and synapse  segmentation, synaptic partner annotation. (Jan Funke, Stephan Saalfeld, Srini Turaga, Davi Bock, Eric Perlman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/Video_Datasets/CRIM13/CRIM13/Main.html">CRIM13 Caltech Resident-Intruder Mouse dataset</a> - 237 10 minute videos (25 fps) annotated with actions (13 classes)  (Burgos-Artizzu, Dollar, Lin, Anderson and Perona) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mv.cvc.uab.es/projects/colon-qa/cvccolondb">CVC colon DB</a> - annotated video sequences of colonoscopy video. It contains 15 short  colonoscopy sequences, coming from 15 different studies. In each  sequence one polyp is shown. (Bernal, Sanchez, Vilarino) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://diademchallenge.org/">DIADEM: Digital Reconstruction of Axonal and Dendritic Morphology Competition</a> (Allen Institute for Brain Science et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.it.lut.fi/project/imageret/diaretdb1/">DIARETDB1 - Standard Diabetic Retinopathy Database</a> (Lappeenranta Univ of Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.isi.uu.nl/Research/Databases/DRIVE/">DRIVE: Digital Retinal Images for Vessel Extraction</a> (Univ of Utrecht) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.sfu.ca/~hamarneh/software/DeformIt/index.html">DeformIt 2.0</a> - Image Data Augmentation Tool: Simulate novel images with ground truth segmentations from a single image-segmentation pair (Brian Booth and  Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.dir-lab.com/">Deformable Image Registration Lab dataset</a> -  for objective and rigrorous evaluation of deformable image  registration (DIR) spatial accuracy performance. (Richard Castillo et  al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/DERMOFIT/datasets.htm">DERMOFIT Skin Cancer Dataset</a> - 1300 lesions from 10 classes captured under identical controlled  conditions. Lesion segmentation masks are included (Fisher, Rees,  Aldridge, Ballerini, et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dermoscopic.blogspot.com/">Dermoscopy images</a> (Eric Ehrsam) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://eatmint.unige.ch/home.php">EATMINT (Emotional Awareness Tools for Mediated INTeraction) database</a> - The EATMINT database contains multi-modal and multi-user recordings  of affect and social behaviors in a collaborative setting. (Guillaume  Chanel, Gaelle Molinari, Thierry Pun, Mireille Betrancourt) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.engr.oregonstate.edu/~tgd/bugid/ept29/">EPT29.</a>This database contains 4842 images of 1613 specimens of 29 taxa of EPTs:(Tom etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eyepacs.com/data-analysis">EyePACS</a> - retinal image database is comprised of over 3 million retinal images of diverse populations with various degrees of diabetic retinopathy (EyePACS)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.forth.gr/cvrl/fire">FIRE Fundus Image Registration Dataset</a> - 134 retinal image pairs and groud truth for registration. (FORTH-ICS) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1aygMzSDdoq63IqSk-ly8cMq0_owup8UM">FMD - Fluorescence Microscopy Denoising dataset</a> - 12,000 real fluorescence microscopy images (Zhang, Zhu, Nichols, Wang, Zhang, Smith, Howard) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mahdihosseini/FoucsPath">FocusPath</a> - Focus Quality Assessment for Digital Pathology (Microscopy) Images.   864 image pathes are naturally blurred by 16 levels of out-of-focus lens provided with GT scores of focus levels. (Hosseini, Zhang, Plataniotis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://medisp.bme.teiath.gr/hicl/index.html">Histology Image Collection Library (HICL)</a> - The HICL is a compilation of 3870histopathological images (so far)  from various diseases, such as brain cancer,breast cancer and HPV (Human Papilloma Virus)-Cervical cancer. (Medical Image and Signal Processing  (MEDISP) Lab., Department of BiomedicalEngineering, School of  Engineering, University of West Attica) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://groups.oist.jp/bptu/honeybee-tracking-dataset">Honeybee segmentation dataset</a> - It is a dataset containing positions and orientation angles of  hundreds of bees on a 2D surface of honey comb. (Bozek K, Hebert L,  Mikheyev AS, Stephesn GJ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/531-mice-behaviour-analysis">IIT MBADA mice</a> - Mice behavioral data. FLIR A315, spacial resolution of 320??240px at  30fps, 50x50cm open arena, two experts for three different mice pairs,  mice identities. (Italian Inst. of Technology, PAVIS lab) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid">Indian Diabetic Retinopathy Image Dataset</a> - This dataset consists of retinal fundus images annotated at  pixel-level for lesions associated with Diabetic Retinopathy. Also, it  provides the disease severity of diabetic retinopathy and diabetic  macular edema. This dataset is useful for development and evaluation of  image analysis algorithms for early detection of diabetic retinopathy.  (Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish  Deshmukh, Vivek Sahasrabuddhe, Fabrice Meriaudeau) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ganymed.imib.rwth-aachen.de/irma/datasets_en.php?SELECTED=00009">IRMA(Image retrieval in medical applications)</a> - This collection compiles anonymous radiographs (Deserno TM, Ott B) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ivdm3seg.weebly.com/data.html">IVDM3Seg</a> - 24 3D multi-modality MRI data sets of at least 7 IVDs of the lower spine,  collected from 12 subjects in two different stages (Zheng, Li, Belavy)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/">JIGSAWS</a> - JHU-ISI Surgical Gesture and Skill Assessment Working Set (a surgical activity dataset for human motion modeling, captured using the da Vinci Surgical System from eight surgeons with different levels of skill  performing five repetitions of three elementary surgical tasks. It  contains: kinematic and video data, plus manual annotations. (Carol  Reiley and Balazs Vagvolgyi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://is-innovation.eu/kid">KID</a> - A capsule endoscopy database for medical decision support (Anastasios Koulaouzidis and Dimitris Iakovidis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.plant-phenotyping.org/CVPPP2014-dataset">Leaf Segmentation Challenge</a>Tobacco and arabidopsis plant images (Hanno Scharr, Massimo Minervini, Andreas  Fischbach, Sotirios A. Tsaftaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI">LIDC-IDRI</a> - Lung Image Database Consortium image collection (LIDC-IDRI) consists  of diagnostic and lung cancer screening thoracic computed tomography  (CT) scans with marked-up annotated lesions. (Before 30/12/19)  [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.lits-challenge.com/">LITS Liver Tumor Segmentation</a> - 130 3D CT scans with segmentations of the liver and liver tumor.  Public benchmark with leaderboard at Codalab.org (Patrick Christ)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mammoimage.org/databases/">Mammographic Image Analysis Homepage</a> - a collection of databases links [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://onlinemedicalimages.com/">Medical image database</a> - Database of ultrasound images of breast abnormalities with the ground  truth. (Prof. Stanislav Makhanov, biomedsiit.com) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mammoimage.org/databases/">MiniMammographic Database</a> (Mammographic Image Analysis Society) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/hueihan/">MIT CBCL Automated Mouse Behavior Recognition datasets</a> (Nicholas Edelman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.inf-cv.uni-jena.de/fgvcbiodiv">Moth fine-grained recognition</a> - 675 similar classes, 5344 images (Erik Rodner et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://celltracking.bio.nyu.edu/">Mouse Embryo Tracking Database</a> - cell division event detection (Marcelo Cicconet, Kris Gunsalus) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbia.fi.muni.cz/datasets/">MUCIC: Masaryk University Cell Image Collection</a> - 2D/3D synthetic images of cells/tissues for benchmarking(Masaryk University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/nih-chest-xrays/data">NIH Chest X-ray Dataset</a> - 112,120 X-ray images with disease labels from 30,805 unique patients. (NIH) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.oasis-brains.org/">OASIS</a> - Open Access  Series of Imaging Studies - 500+ MRI data sets of the brain (Washington  University, Harvard University, Biomedical Informatics Research Network) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.plant-phenotyping.org/datasets-home">Plant Phenotyping Datasets</a> - plant data suitable for plant and leaf detection, segmentation,  tracking, and species recognition (M. Minervini, A. Fischbach, H.  Scharr, S. A. Tsaftaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.noldus.com/innovationworks/datasets/ratsi">RatSI: Rat Social Interaction Dataset</a> - 9 fully annotated (11 class) videos (15 minute, 25 FPS) of two rats  interacting socially in a cage (Malte Lorbach, Noldus Information  Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.rug.nl/~imaging/databases/retina_database/retinalfeatures_database.html">Retinal fundus images - Ground truth of vascular bifurcations and crossovers</a> (Univ of Groningen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://saras-esad.grand-challenge.org/Dataset/">SARAS endoscopic vision challenge for surgeon action detection</a> - 22,601 annotated training frames with 28,055 action instances from 21 different action classes (Cuzzolin, Singh Bawa, Skarga-Bandurova,  Singh) [16/4/20]</li>
<li><a target="_blank" rel="noopener" href="https://scorhe.nih.gov/">SCORHE</a> - 1, 2 and 3 mouse behavior videos, 9 behaviors, (Ghadi H. Salem, et al, NIH) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://web.northeastern.edu/ostadabbas/2019/06/27/multimodal-in-bed-pose-estimation/">SLP (Simultaneously-collected multimodal Lying Pose)</a> -  large scale dataset on in-bed poses includes: 2 Data Collection  Settings: (a) Hospital setting: 7 participants, and (b) Home setting:  102 participants (29 females, age range: 20-40). 4 Imaging Modalities:  RGB (regular webcam), IR (FLIR LWIR camera), DEPTH (Kinect v2) and  Pressure Map (Tekscan Pressure Sensing Map). 3 Cover Conditions:  uncover, bed sheet, and blanket. Fully labeled poses with 14 joints.  (Ostadabbas and Liu) [2/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://brainiac2.mit.edu/SNEMI3D/">SNEMI3D</a> - 3D Segmentation of neurites in EM images [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cecas.clemson.edu/~ahoover/stare/">STructured Analysis of the Retina</a> - DESCRIPTION(400+ retinal images, with ground truth segmentations and  medical annotations) (Before 30/12/19)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.digitalimaginggroup.ca/members/shuo.php">Spine and Cardiac data</a> (Digital Imaging Group of London Ontario, Shuo Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.engr.oregonstate.edu/~tgd/bugid/stonefly9/">Stonefly9</a>This database contains 3826 images of 773 specimens of 9 taxa of Stoneflies (Tom etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.phagosight.org/synData.php">Synthetic Migrating Cells</a> -Six artificial migrating cells (neutrophils) over 98 time frames,  various levels of Gaussian/Poisson noise and different paths  characteristics with ground truth. (Dr Constantino Carlos Reyes-Aldasoro et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/ybenezeth/ubfcrppg">UBFC-RPPG Dataset</a> -  remote photoplethysmography (rPPG) video data and ground truth  acquired with a CMS50E transmissive pulse oximeter (Bobbia, Macwan,  Benezeth, Mansouri, Dubois) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cgvr.informatik.uni-bremen.de/research/asula/index.shtml">Uni Bremen Open, Abdominal Surgery RGB Dataset</a> - Recording of a complete, open, abdominal surgery using a Kinect v2  that was mounted directly above the patient looking down at patient and  staff. (Joern Teuber, Gabriel Zachmann, University of Bremen) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://marathon.csee.usf.edu/Mammography/Database.html">Univ of Central Florida - DDSM: Digital Database for Screening Mammography</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vascusynth.cs.sfu.ca/">VascuSynth</a> - 120 3D vascular tree like structures with ground truth (Mengliu Zhao, Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vascusynth.cs.sfu.ca/Data.html">VascuSynth</a> - Vascular Synthesizer generates vascular trees in 3D volumes. (Ghassan Hamarneh, Preet Jassi, Mengliu Zhao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cse.yorku.ca/~mridataset/">York Cardiac MRI dataset</a> (Alexander Andreopoulos) [Before 28/12/19]</li>
</ol>
<h2 id="Camera-calibration"><a href="#Camera-calibration" class="headerlink" title="Camera calibration"></a>Camera calibration</h2><ol>
<li><a target="_blank" rel="noopener" href="http://cvrg.iyte.edu.tr/datasets.htm">Catadioptric camera calibration images</a> (Yalin Bastanlar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/">GoPro-Gyro Dataset</a> - This dataset consists of a number of wide-angle rolling shutter video sequences with corresponding gyroscope measurements (Hannes etc.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cmp.felk.cvut.cz/software/LO-RANSAC/index.xhtml">LO-RANSAC</a> - LO-RANSAC library for estimation of homography and epipolar geometry(K. Lebeda, J. Matas and O. Chum) [Before 28/12/19]</li>
</ol>
<h2 id="Face-and-Eye-Iris-Databases"><a href="#Face-and-Eye-Iris-Databases" class="headerlink" title="Face and Eye/Iris Databases"></a>Face and Eye/Iris Databases</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Juyong/3DFace">2D-3D face dataset</a> -  This dataset includes pairs of 2D face image and its corresponding 3D  face geometry model with geometry details. (Yudong Guo, Juyong Zhang,  Jianfei Cai, Boyi Jiang, Jianmin Zheng) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ibug.doc.ic.ac.uk/resources/300-VW/">300 Videos in the Wild (300-VW)</a> - 68 Facial Landmark Tracking (Chrysos, Antonakos, Zafeiriou, Snape, Shen, Kossaifi, Tzimiropoulos, Pantic) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/D-X-Y/landmark-detection/blob/master/dataset.md">300W-Style</a> - enhanced version of 300W by applying three style changes to the  original images. It is used to facilitate the analysis of the facial  landmark detection problem. (Xuanyi Dong) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/3dmad">3D Mask Attack Database (3DMAD)</a> - 76500 frames of 17 persons using Kinect RGBD with eye positions (Sebastien Marcel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html">3D facial expression</a> - Binghamton University 3D Static and Dynamic Facial Expression  Databases (Lijun Yin, Jeff Cohn, and teammates) [Before 28/12/19]</li>
<li><a href="ttps://github.com/D-X-Y/landmark-detection/blob/master/dataset.md">AFLW-Style</a> - enhanced version of AFLW by applying three style changes to the  original images. It is used to facilitate the analysis of the facial  landmark detection problem. (Xuanyi Dong) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/171iZQ8dqx3Yyp5t2gq06DtSWv9RMnNjG/view">AginG Faces in the Wild v2</a> Database description: AGFW-v2 consists of 36,299 facial images divided  into 11 age groups with a span of five years between groups. On average, there are 3,300 images per group. Facial images in AGFW-v2 are not  public figures and less likely to have significant make-up or facial  modifications, helping embed accurate aging effects during the learning  process. (Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/mobio">Audio-visual database for face and speaker recognition</a> (Mobile Biometry MOBIO <a target="_blank" rel="noopener" href="http://www.mobioproject.org/">http://www.mobioproject.org/</a>) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://spandh.dcs.shef.ac.uk/avlombard/">Audiovisual Lombard grid speech corpus</a> - a bi-view audiovisual Lombard speech corpus which can be used to  support joint computational-behavioral studies in speech perception  (Alghamdi, Maddock, Marxer, Barker and Brown) [31/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.surrey.ac.uk/CVSSP/banca/">BANCA face and voice database</a> (Univ of Surrey) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html">Binghampton Univ 3D static and dynamic facial expression database</a> (Lijun Yin, Peter Gerhardstein and teammates) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://fera2015-db.sspnet.eu/media/doc/eula.pdf">Binghamton-Pittsburgh 4D Spontaneous Facial Expression Database</a> - consist of 2D spontaneous facial expression videos and FACS codes. (Lijun Yin et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://support.bioid.com/downloads/facedb/index.php">BioID face database</a> (BioID group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iikt.ovgu.de/BioVid.html">BioVid Heat Pain Database</a> - This video (and biomedical signal) dataset contains facial and  physiopsychological reactions of 87 study participants who were  subjected to experimentally induced heat pain. (University of Magdeburg  (Neuro-Information Technology group) and University of Ulm (Emotion  Lab)) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://zbum.ia.pw.edu.pl/EN/node/46">Biometric databases</a> - biometric databases related to iris recognition (Adam Czajka) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.ee.ethz.ch/datasets/b3dac2.en.html">Biwi 3D Audiovisual Corpus of Affective Communication</a> - 1000 high quality, dynamic 3D scans of faces, recorded while pronouncing a set of English sentences.  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bosphorus.ee.boun.edu.tr/default.aspx">Bosphorus 3D/2D Database of FACS annotated facial expressions, of head poses and of face occlusions</a> (Bogazici University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://caer-dataset.github.io/">CAER (Context-Aware Emotion Recognition)</a> - Large scale image and video dataset for emotion recognition, and  facial expression recognition (Lee, Kim, Kim, Park, and Sohn) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.gag.itu.edu.tr/CPdatabase/">Caricature/Photomates dataset</a> - a dataset with frontal faces and corresponding Caricature line drawings (Tayfun Akgul) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/IrisDatabase.htm">CASIA-IrisV3</a> (Chinese Academy of Sciences, T. N. Tan, Z. Sun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mrl.isr.uc.pt/experimentaldata/public/gaze-casir/">CASIR Gaze Estimation Database</a> - RGB and depth images (from Kinect V1.0) and ground truth values of  facial features corresponding to experiments for gaze estimation  benchmarking: (Filipe Ferreira etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html">Celeb-DF</a> - A new large-scale and challenging DeepFake video dataset, Celeb-DF,  for the development and evaluation of DeepFake detection algorithms (Li, Yang, Sun, Qi and Lyu) [30/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vasc.ri.cmu.edu/idb/html/face/facial_expression/">CMU Facial Expression Database</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html">The CMU Multi-PIE Face Database</a> - more than 750,000 images of 337 people recorded in up to four  sessions over the span of five months. (Jeff Cohn et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ri.cmu.edu/publication_view.html?pub_id=3462">CMU Pose, Illumination, and Expression (PIE) Database</a> (Simon Baker) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/FaceData2.html">CMU/MIT Frontal Faces</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vasc.ri.cmu.edu/idb/html/face/frontal_images/">CMU/MIT Frontal Faces</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://coma.is.tue.mpg.de/">CoMA 3D face dataset</a> -  20,466 meshes (3D head scans and registrations in FLAME topology) of  extreme facial expressions captured from 12 different subjects (Ranjan,  Bolkart, Sanyal, Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.csse.uwa.edu.au/~ajmal/databases.html">CSSE Frontal intensity and range images of faces</a> (Ajmal Mian) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> - Large-scale CelebFaces Attributes Dataset(Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cfpw.io/">Celebrities in Frontal-Profile in the Wild</a> - 500+ images of celebrities in frontal and profile views (Sengupta,  Cheng, Castillo, Patel, Chellappa, Jacobs) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.pitt.edu/~emotion/ck-spread.htm">Cohn-Kanade AU-Coded Expression Database</a> - 500+ expression sequences of 100+ subjects, coded by activated Action Units (Affect Analysis Group, Univ. of Pittsburgh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.pitt.edu/~emotion/ck-spread.htm">Cohn-Kanade AU-Coded Expression Database</a> - for research in automatic facial image analysis and synthesis and for perceptual studies (Jeff Cohn et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.columbia.edu/CAVE/databases/columbia_gaze/">Columbia Gaze Data Set</a> - 5,880 images of 56 people over 5 head poses and 21 gaze directions  (Brian A. Smith, Qi Yin, Steven K. Feiner, Shree K. Nayar) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lrv.fri.uni-lj.si/facedb.html">Computer Vision Laboratory Face Database (CVL Face Database)</a> - Database contains 798 images of 114 persons, with 7 images per person and is freely available for research purposes. (Peter Peer etc.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Mengmi/deepfuturegaze_gan">Deep future gaze</a> - This dataset consists of 57 sequences on search and retrieval tasks  performed by 55 subjects. Each video clip lasts for around 15 minutes  with the frame rate 10 fps and frame resolution 480 by 640. Each subject is asked to search for a list of 22 items (including lanyard, laptop)  and move them to the packing location (dining table). (National  University of Singapore, Institute for Infocomm Research) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mohammadmahoor.com/databases-codes/">DISFA+:Extended Denver Intensity of Spontaneous Facial Action Database</a> - an  extension of DISFA (M.H. Mahoor) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mohammadmahoor.com/databases-codes/">DISFA:Denver Intensity of Spontaneous Facial Action Database</a> - a non-posed facial expression database for those who are interested  in developing computer algorithms for automatic action unit detection  and their intensities described by FACS. (M.H. Mahoor) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/wenguanwang/DHF1K">DHF1K</a> - 1000 elaborately selected video sequences with fixation annotations from 17 viewers. (Prof. Jianbing Shen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://fcd.eurecom.fr/">EURECOM Facial Cosmetics Database</a> - 389 images, 50 persons with/without make-up, annotations about the  amount and location of applied makeup. (Jean-Luc DUGELAY et al) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rgb-d.eurecom.fr/">EURECOM Kinect Face Database</a> - 52 people, 2 sessions, 9 variations, 6 facial landmarks. (Jean-Luc DUGELAY et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.idiap.ch/dataset/eyediap">EYEDIAP dataset</a> -  The EYEDIAP dataset was designed to train and evaluate gaze estimation  algorithms from RGB and RGB-D data.It contains a diversity of  participants, head poses, gaze targets and sensing conditions. (Kenneth  Funes and Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://face2bmi.csail.mit.edu/">Face2BMI Dataset</a> The  Face2BMI dataset contains 2103 pairs of faces, with corresponding  gender, height and previous and current body weights, which allows for  training computer vision models that can predict body-mass index (BMI)  from profile pictures. (Enes Kocabey, Ferda Ofli, Yusuf Aytar, Javier  Marin, Antonio Torralba, Ingmar Weber) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/fddb/">FDDB: Face Detection Data set and Benchmark - studying unconstrained face detection</a> (University of Massachusetts Computer Vision Laboratory) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://researchdata.sfu.ca/islandora/object/sfu:2722">FDDB-360</a> - face detection in 360 degree fisheye images (Fu, Alvar, Bajic, and Vaughan) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.fgnet.rsunit.com/">FG-Net Aging Database of faces at different ages</a> (Face and Gesture Recognition Research Network) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nist.gov/itl/iad/image-group/face-recognition-vendor-test-frvt-2013">Face Recognition Grand Challenge datasets</a> (FRVT - Face Recognition Vendor Test) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.qirt.org/liens/FMTV.htm">FMTV</a> - Laval Face  Motion and Time-Lapse Video Database. 238 thermal/video subjects with a  wide range of poses and facial expressions acquired over 4 years  (Ghiass, Bendada, Maldague) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ies.anthropomatik.kit.edu/publ.php?key=ies_2016_qu_capturing">Face Super-Resolution Dataset</a> - Ground truth HR-LR face images captured with a dual-camera setup (Chengchao Qu etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vintage.winklerbros.net/facescrub.html">FaceScrub</a> - A Dataset With Over 100,000 Face Images of 530 People (50:50 male and female) (H.-W. Ng, S. Winkler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www1.cs.columbia.edu/CAVE/databases/facetracer/">FaceTracer Database - 15,000 faces</a> (Neeraj Kumar, P. N. Belhumeur, and S. K. Nayar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.affectiva.com/facial-expression-dataset/">Facial Expression Dataset</a> - This dataset consists of 242 facial videos (168,359 frames) recorded  in real world conditions. (Daniel McDuff et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.micc.unifi.it/resources/datasets/florence-3d-faces/">Florence 2D/3D Hybrid Face Dataset</a> - bridges the gap between 2D, appearance-based recognition techniques,  and fully 3D approaches (Bagdanov, Del Bimbo, and Masi) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.itl.nist.gov/iad/humanid/feret/feret_master.html">Facial Recognition Technology (FERET) Database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gi4e.unavarra.es/databases/gi4e/">Gi4E Database</a> -  eye-tracking database with 1300+ images acquired with a standard  webcam, corresponding to different subjects gazing at different points  on a screen, including ground-truth 2D iris and corner points  (Villanueva, Ponz, Sesma-Sanchez, Mikel Porta, and Cabeza) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ai.google/tools/datasets/google-facial-expression/">Google Facial Expression Comparison dataset</a> - a large-scale facial expression dataset consisting of face image  triplets along with human annotations that specify which two faces in  each triplet form the most similar pair in terms of facial expression,  which is different from datasets that focus mainly on discrete emotion  classification or action unit detection (Vemulapalli, Agarwala) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.isca-speech.org/iscapad/iscapad.php?module=article&id=11770">Hannah and her sisters database</a> - a dense audio-visual person-oriented ground-truth annotation of  faces, speech segments, shot boundaries (Patrick Perez, Technicolor)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www-users.cs.york.ac.uk/~nep/research/Headspace/">Headspace dataset</a> - The Headspace dataset is a set of 3D images of the full human head,  consisting of 1519 subjects wearing tight fitting latex caps to reduce  the effect of hairstyles. (Christian Duncan, Rachel Armstrong, Alder Hey Craniofacial Unit, Liverpool, UK) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/facesketch.html">Hong Kong Face Sketch Database</a> [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/headpose">IDIAP Head Pose Database (IHPD)</a> - The dataset contains a set of meeting videos along with the head  groundtruth of individual participants (around 128min)(Sileye Ba and  Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nist.gov/programs-projects/face-challenges">IARPA Janus Benchmark datasets</a> - IJB-A, IJB-B, IJB-C, FRVT (NIST) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.ee.ethz.ch/en/datasets/">IMDB-WIKI</a> - 500k+ face images with age and gender labels (Rasmus Rothe, Radu Timofte, Luc Van Gool ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvit.iiit.ac.in/projects/IMFDB/">Indian Movie Face database (IMFDB)</a> - a large unconstrained face database consisting of 34512 images of 100 Indian actors collected from more than 100 videos (Vijay Kumar and C V  Jawahar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iranprc.org/en/ifdb.php">Iranian Face Database</a> - IFDB is the first image database in middle-east, contains color  facial images with age, pose, and expression whose subjects are in the  range of 2-85. (Mohammad Mahdi Dehshibi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.kasrl.org/jaffe.html">Japanese Female Facial Expression (JAFFE) Database</a> (Michael J. Lyons) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://childrenfacialexpression.projet.liris.cnrs.fr/">LIRIS Children Spontaneous Facial Expression Video Database</a> - pontaneous / natural facial expressions of 12 children in diverse  settings with variable video recording scenarios showing six universal  or prototypic emotional expressions (happiness, sadness, anger,  surprise, disgust and fear). Children are recorded in constraint free  environment (no restriction on head movement, no restriction on hands  movement, free sitting setting, no restriction of any sort) while they  watched specially built / selected stimuli. This constraint free  environment allowed us to record spontaneous / natural expression of  children as they occur. The database has been validated by 22 human  raters. (Khan, Crenn, Meyer, Bouakaz) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">LFW: Labeled Faces in the Wild</a> - unconstrained face recognition [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://adrianbulat.com/face-alignment">LS3D-W</a> - a  large-scale 3D face alignment dataset annotated with 68 points  containing faces captured in a “in-the-wild” setting. (Adrian Bulat,  Georgios Tzimiropoulos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.escience.cn/people/geshiming/mafa.html">MAFA: MAsked FAces</a> - 30,811 images with  35,806 labeled MAsked FAces, six main attributes  of each masked face. (Shiming Ge, Jia Li,  Qiting Ye, Zhao Luo) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.antitza.com/makeup-datasets.html">Makeup Induced Face Spoofing (MIFS)</a> - 107 makeup-transformations attempting to spoof a target identity. Also other datasets. (Antitza Dantcheva) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.labri.fr/projet/AIV/MexCulture142.php">Mexculture142</a> - Mexican Cultural heritage objects and eye-tracker gaze fixations  (Montoya Obeso, Benois-Pineau, Garcia-Vazquez, Ramirez Acosta) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/heisele/facerecognition-database.html">MIT CBCL Face Recognition Database</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.mit.edu/emeyers/www/face_databases.html">MIT Collation of Face Databases</a> (Ethan Meyers) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html">MIT eye tracking database (1003 images)</a> (Judd et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmifacedb.eu/">MMI Facial Expression Database</a> - 2900 videos and high-resolution still images of 75 subjects, annotated for FACS AUs. [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.faceaginggroup.com/projects.html">MORPH (Craniofacial Longitudinal Morphological Face Database)</a> (University of North Carolina Wilmington) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mpi-inf.mpg.de/MPIIGazeDataset">MPIIGaze dataset</a> - 213,659 samples with eye images and gaze target under different  illumination conditions and nature head movement, collected from 15  participants with their laptop during daily using. (Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/talking_face/talking_face.html">Manchester Annotated Talking Face Video Dataset</a> (Timothy Cootes) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://megaface.cs.washington.edu/dataset/download.html">MegaFace</a> - 1 million faces in bounding boxes (Kemelmacher-Shlizerman, Seitz, Nech, Miller, Brossard) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://shunzhang.me.pn/papers/eccv2016/">Music video dataset</a> - 8 music videos from YouTube for developing multi-face tracking  algorithms in unconstrained environments (Shun Zhang, Jia-Bin Huang,  Ming-Hsuan Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.nist.gov/itl/iad/ig/frgc.cfm">NIST Face Recognition Grand Challenge (FRGC)</a> (NIST) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.nist.gov/srd/nistsd18.cfm">NIST mugshot identification database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.videorecognition.com/db/video/faces/cvglab/">NRC-IIT Facial Video Database</a> - this database contains pairs of short video clips each showing a face of a computer user sitting in front of the monitor exhibiting a  wide  range of facial expressions and  orientations (Dmitry Gorodnichy)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.nd.edu/~cvrl/CVRL/Data_Sets.html">Notre Dame Iris Image Dataset</a> (Patrick J. Flynn) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets">Notre Dame face, IR face, 3D face, expression, crowd, and eye biometric datasets</a> (Notre Dame) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">ORL face database: 40 people with 10 views</a> (ATT Cambridge Labs) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.openu.ac.il/home/hassner/Adience/data.html">OUI-Adience Faces</a> - unfiltered faces for gender and age classification plus 3D faces (OUI) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data.html">Oxford: faces, flowers, multi-view, buildings, object categories, motion segmentation, affine covariant regions, misc</a> (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://imagelab.ing.unimore.it/pandora/">Pandora</a> - POSEidon: Face-from-Depth for Driver Pose (Borghi, Venturelli, Vezzani, Cucchiara) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www1.cs.columbia.edu/CAVE/databases/pubfig/">PubFig: Public Figures Face Database</a> (Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://qmul-survface.github.io/">QMUL-SurvFace</a> - A  large-scale face recognition benchmark dedicated for real-world  surveillance face analysis and matching. (QMUL Computer Vision Group)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/#deepfunnel-anchor">Re-labeled Faces in the Wild</a> - original images, but aligned using “deep funneling” method. (University of Massachusetts, Amherst) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://zenodo.org/record/2529036">RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments</a> 122,531 images with the subjects’ ground truth eye gaze and head pose  labels under free-viewing conditions and large camera-subject distances  (Fischer, Chang, Demiris, Imperial College London) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.inf.ed.ac.uk/trimbot2020/DYNAMICFACES/index.html">S3DFM</a> - Edinburgh Speech-driven 3D Facial Motion Database. 77 people with 10  repetitions of speaking a passphrase: 1 second of 500 frame per second  600x600 pixels of {IR intensity video, registered depth images} plus  synchronized 44.1 Khz audio. There are an additional 26 people (10  repetitions) moving their heads while speaking (Zhang, Fisher) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://jov.arvojournals.org/article.aspx?articleid=2193194">Salient features in gaze-aligned recordings of human visual input</a> - TB of human gaze-contingent data “in the wild” (Frank Schumann etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">SAMM Dataset of Micro-Facial Movements</a> - The dataset contains 159 spontaneous micro-facial movements obtained  from 32 participants from 13 different ethnicities. (A.Davison,  C.Lansley, N.Costen, K.Tan, M.H.Yap) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.scface.org/">SCface</a> - Surveillance Cameras Face Database (Mislav Grgic, Kresimir Delac, Sonja Grgic, Bozidar Klimpak) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.polito.it/cgvg/siblingsDB.html">SiblingsDB</a> - The SiblingsDB contains two datasets depicting images of individuals  related by sibling relationships. (Politecnico di Torino/Computer  Graphics &amp; Vision Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/sof-dataset">SoF dataset</a> - 42,592 face images with glasses under different illumination  conditions; provided with face region, facial landmarks, facial  expression, subject ID, gender, and age information (Afifi, Abdelhamed)  [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.nal.usda.gov/dataset/data-solving-robot-world-hand-eyes-calibration-problem-iterative-methods">Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods</a> - These datasets were generated for calibrating robot-camera systems. (Amy Tabb) [Before 28/12/19]</li>
<li>[Spontaneous Emotion Multimodal Database (SEM-db)](<a target="_blank" rel="noopener" href="http://staffnet.kingston.ac.uk/~ku43576/?page">http://staffnet.kingston.ac.uk/~ku43576/?page</a> id=414) - non-posed reactions to visual stimulus data recorded with HD RGB,  depth  and IR frames of the face, EEG signal and eye gaze data  (Fernandez. Montenegro, Gkelias, Argyriou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.pitt.edu/~emotion/um-spread.htm">The UNBC-McMaster Shoulder Pain Expression Archive Database</a> - Painful data: The UNBC-McMaster Shoulder Pain Expression Archive Database (Lucy et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://voca.is.tue.mpg.de/">VOCASET</a> - 4D face dataset  with about 29 minutes of 3D head scans captured at 60 fps and  synchronized audio from 12 speakers (Cudeiro, Bolkart, Laidlaw, Ranjan,  Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets">Trondheim Kinect RGB-D Person Re-identification Dataset</a> (Igor Barros Barbosa) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm">UB KinFace Database</a> - University of Buffalo kinship verification and recognition database [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://iris.di.ubi.pt/">UBIRIS: Noisy Visible Wavelength Iris Image Databases</a> (University of Beira) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://umdfaces.io/">UMDFaces</a> - About 3.7 million  annotated video frames from 22,000 videos and 370,000 annotated still  images. (Ankan Bansal et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gi4e.unavarra.es/databases/hpdb/">UPNA Head Pose Database</a> - head pose database, with 120 webcam videos containing guided-movement sequences and free-movement sequences, including ground-truth head pose and automatically annotated 2D facial points. (Ariz, Bengoechea,  Villanueva, Cabeza) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gi4e.unavarra.es/databases/shpdb/">UPNA Synthetic Head Pose Database</a> - a synthetic replica of the UPNA Head Pose Database, with 120 videos  with their 2D ground truth landmarks projections, their corresponding  head pose ground truth, 3D head models and camera parameters. (Larumbe,  Segura, Ariz, Bengoechea, Villanueva, Cabeza) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://utiris.wordpress.com/">UTIRIS cross-spectral iris image databank</a> (Mahdi Hosseini) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.uva-nemo.org/">UvA-NEMO Smile Database</a> -   1240 smile videos (597 spontaneous and 643 posed) from 400 subjects,  including age, gender, and kinship annotations (Gevers, Dibeklioglu,  Salah) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">VGGFace2</a> - VGGFace2 is a large-scale face recognition dataset covering large  variations in pose, age, illumination, ethnicity and profession. (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ihitworld.com/index.php/vipsl-database">VIPSL Database</a> - VIPSL Database is for research on face sketch-photo synthesis and  recognition, including 200 subjects (1 photo and 5 sketches per  subject). (Nannan Wang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/kreimanlab/VisualSearchZeroShot">Visual Search Zero Shot Database</a> - Collection of human eyetracking data in three increasingly complex  visual search tasks: object arrays, natural images and Waldo images.  (Kreiman lab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://sufficiency.ece.vt.edu/VT-KFER/">VT-KFER</a>: A  Kinect-based RGBD+Time Dataset for Spontaneous and Non-Spontaneous  Facial Expression Recognition - 32 subjects, 1,956 sequences of RGBD,  six facial expressions in 3 poses (Aly, Trubanova, Abbott, White, and  Youssef) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://grail.cs.washington.edu/projects/deepexpr/ferg-db.html">Washington Facial Expression Database (FERG-DB)</a> - a database of 6 stylized (Maya) characters with 7 annotated facial  expressions (Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro,  and Barbara Mones) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs.nju.edu.cn/rl/WebCaricature.htm">WebCaricature Dataset</a> - The WebCaricature dataset is a large photograph-caricature dataset  consisting of 6042 caricatures and 5974 photographs from 252 persons  collected from the web. (Jing Huo, Wenbin Li, Yinghuan Shi, Yang Gao and Hujun Yin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/">WIDER FACE: A Face Detection Benchmark</a> - 32,203 images with 393,703 labeled faces, 61 event classes (Shuo  Yang, Ping Luo, Chen Change Loy, Xiaoou Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://researchdata.sfu.ca/pydio_public/c09804">Wider-360</a> - Datasets for face and object detection in fisheye images (Fu, Bajic, and Vaughan) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/">XM2VTS Face video sequences (295): The extended M2VTS Database (XM2VTS) -</a> (Surrey University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.face-rec.org/databases/">Yale Face Database - 11 expressions of 10 people</a> (A. Georghaides) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.face-rec.org/databases/">Yale Face Database B - 576 viewing conditions of 10 people</a> (A. Georghaides) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www-users.cs.york.ac.uk/~nep/research/YEM/">York 3D Ear Dataset</a> - The York 3D Ear Dataset  is a set of 500 3D ear images, synthesized  from detailed 2D landmarking, and available in both Matlab format (.mat) and PLY format (.ply). (Nick Pears, Hang Dai, Will Smith, University of York) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-sop.inria.fr/members/Neil.Bruce/eyetrackingdata.zip">York Univ Eye Tracking Dataset (120 images)</a> (Neil Bruce) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces DB</a> - 3,425 videos of 1,595 different people. (Wolf, Hassner, Maoz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.tu-chemnitz.de/physik/PHKP/ZurichNatImgDB.html.en">Zurich Natural Image</a> - the image material used for creating natural stimuli in a series of eye-tracking studies (Frey et al.) [Before 28/12/19]</li>
</ol>
<h2 id="Fingerprints"><a href="#Fingerprints" class="headerlink" title="Fingerprints"></a>Fingerprints</h2><ol>
<li><a target="_blank" rel="noopener" href="http://bias.csr.unibo.it/fvc2002/databases.asp">FVC fingerpring verification competition 2002 dataset</a> (University of Bologna) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bias.csr.unibo.it/fvc2004/databases.asp">FVC fingerpring verification competition 2004 dataset</a> (University of Bologna) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ekds.gov.tr/bio/databases.html"> Fingerprint Manual Minutiae Marker (FM3) Databases:</a> - Fingerprint Manual Minutiae Marker (FM3) Databases( Mehmet Kayaoglu, Berkay Topcu and Umut Uludag) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://srdata.nist.gov/gateway/gateway?keyword=fingerprint">NIST fingerprint databases</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.inescporto.pt/ip-en/news-events/events/spd2010-fingerprint-singular-points-detection/">SPD2010 Fingerprint Singular Points Detection Competition</a> (SPD 2010 committee) [Before 28/12/19]</li>
</ol>
<h2 id="General-Images"><a href="#General-Images" class="headerlink" title="General Images"></a>General Images</h2><ol>
<li><a target="_blank" rel="noopener" href="http://adrianbarburesearch.blogspot.com/p/renoir-dataset.html">A Dataset for Real Low-Light Image Noise Reduction</a> - It contains pixel and intensity aligned pairs of images corrupted by  low-light camera noise and their low-noise counterparts. (J. Anaya, A.  Barbu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://dx.doi.org/10.6084/m9.figshare.3370627">A database of paintings related to Vincent van Gogh</a> - This is the dataset VGDB-2016 built for the paper “From Impressionism to Expressionism: Automatically Identifying Van Gogh’s Paintings”  (Guilherme Folego and Otavio Gomes and Anderson Rocha) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cse.wustl.edu/~jacobsn/projects/webcam_dataset/">AMOS: Archive of Many Outdoor Scenes (20+m)</a> (Nathan Jacobs) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.dropbox.com/s/eqs536fcnj6ns4x/colorbuilding_dataset.zip?dl=0">Aerial images</a>Building detection from aerial images using invariant color features and shadow information. (Beril Sirmacek) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/0B_3Nh0OK9BclUko4ZjkxRDFaMkU/view">Approximated overlap error dataset</a>Image pairs with sparse sets of ground-truth matches for evaluating local image descriptors (Fabio Bellavia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://auto-da.github.io/">AutoDA (Automatic Dataset Augmentation)</a> - An automatically constructed image dataset including 12.5 million  images with relevant textual information for the 1000 categories of  ILSVRC2012 (Bai, Yang, Ma, Zhao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://icvl.cs.bgu.ac.il/hyperspectral/">BGU Hyperspectral Image Database of Natural Scenes</a> (Ohad Ben-Shahar and Boaz Arad) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.lems.brown.edu/content/available-software-and-databases">Brown Univ Large Binary Image Database</a> (Ben Kimia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.dropbox.com/sh/3p4x1oc5efknd69/AABwnyoH2EKi6H9Emcyd0pXCa?dl=0">Butterfly-200</a> - Butterfly-20 is a image dataset for fine-grained image  classification, which contains 25,279 images and covers four levels  categories of 200 species, 116 genera, 23 subfamilies, and 5 families.  (Tianshui Chen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mahmoudnafifi/WB_color_augmenter">CIFAR-10 classes with different WB settings</a> - 15,098 rendered images that reflect real in-camera white-balance settings (Afifi, Brown) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cmp.felk.cvut.cz/~tylecr1/facade/">CMP Facade Database</a> - Includes 606 rectified images of facades from various places with 12  architectural classes annotated. (Radim Tylecek) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">Caltech-UCSD Birds-200-2011</a> (Catherine Wah) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/drive/u/1/folders/0B_3Nh0OK9BclQkt4empQVU5yVE0">Color correction dataset</a> - Homography-based registered images for evaluating color correction  algorithms for image stitching. (Fabio Bellavia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www1.cs.columbia.edu/CAVE/databases/multispectral/">Columbia Multispectral Image Database</a> (F. Yasuma, T. Mitsunaga, D. Iso, and S.K. Nayar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mpii.de/visual_turing_test">DAQUAR (Visual Turing Challenge)</a> - A dataset containing questions and answers about real-world indoor  scenes. (Mateusz Malinowski, Mario Fritz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://noise.visinf.tu-darmstadt.de/">Darmstadt Noise Dataset</a> - 50 pairs of real noisy images and corresponding ground truth images  (RAW and sRGB) (Tobias Plotz and Stefan Roth) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tadarsh/movie-trailers-dataset">Dataset of American Movie Trailers 2010-2014</a> - Contains links to 474 hollywood movie trailers along with associated metadata (genre, budget, runtime,  release, MPAA rating, screens released, sequel indicator) (USC Signal  Analysis and Interpretation Lab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://diml.yonsei.ac.kr/~srkim/DASC/">DIML Multimodal Benchmark</a> - To evaluate matching performance under photometric and geometric  variations, 100 images of 1200 x 800 size. (Yonsei University) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dped-photos.vision.ee.ethz.ch/">DSLR Photo Enhancement Dataset (DPED)</a> - 22K photos taken synchronously in the wild by three smartphones and  one DSLR camera, useful for comparing infered high quality images from  multiple low quality images (Ignatov, Kobyshev, Timofte, Vanhoey, and  Van Gool). [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vislab.berkeleyvision.org/">Flickr-style</a> - 80K  Flickr photographs annotated with 20 curated style labels, and 85K  paintings annotated with 25 style/genre labels (Sergey Karayev) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://yingqianwang.github.io/Flickr1024/">Flickr1024: A Dataset for Stereo Image Super-resolution</a> - 1024 high-quality images pairs and covers diverse senarios (Wang, Wang, Yang, An, Guo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.forth.gr/cvrl/msi/">Forth Multispectral Imaging Datasets</a> - images from 23 spectral bands each from 5 paintings. Images are  annotated with ground truth data. (Karamaoynas Polykarpos et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html">General 100 Dataset</a> - General-100 dataset contains 100 bmp-format images (with no  compression), which are well-suited for super-resolution training(Dong,  Chao and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/SeungjunNah/DeepDeblur_release">GOPRO dataset</a> - Blurred image dataset with sharp image ground truth (Nah, Kim, and Lee) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/typelib.htm">HIPR2 Image Catalogue of different types of images</a> (Bob Fisher et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://hpatches.github.io/">HPatches</a> - A benchmark and evaluation of handcrafted and learned local descriptors (Balntas, Lenc, Vedaldi, Mikolajczyk) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Local_Illumination_HSIs/Local_Illumination_HSIs_2015.html">Hyperspectral images for spatial distributions of local illumination in natural scenes</a> - Thirty calibrated hyperspectral radiance images of natural scenes  with probe spheres embedded for local illumination estimation.  (Nascimento, Amano &amp; Foster) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_02.html">Hyperspectral images of natural scenes - 2002</a> (David H. Foster) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_04.html">Hyperspectral images of natural scenes - 2004</a> (David H. Foster) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.isprs.org/commissions/comm1/icwg15b/benchmark_main.html">ISPRS multi-platform photogrammetry dataset</a> - 1: Nadir and oblique aerial images plus 2: Combined UAV and  terrestrial images (Francesco Nex and Markus Gerke) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://live.ece.utexas.edu/research/Quality/index.htm">Image &amp; Video Quality Assessment at LIVE</a> - used to develop picture quality algorithms (the University of Texas at Austin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.image-net.org/challenges/LSVRC/">ImageNet Large Scale Visual Recognition Challenges</a> - Currently 200 object classes and 500+K images (Alex Berg, Jia Deng, Fei-Fei Li and others) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.image-net.org/">ImageNet Linguistically organised (WordNet) Hierarchical Image Database - 10E7 images, 15K categories</a> (Li Fei-Fei, Jia Deng, Hao Su, Kai Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://collections.durham.ac.uk/catalog?utf8=%E2%9C%93&q=breckon">Improved 3D Sparse Maps for High-performance Structure from Motion with Low-cost Omnidirectional Robots - Evaluation Dataset</a> - Data set used in research paper doi:10.1109/ICIP.2015.7351744 (Breckon, Toby P., Cavestany, Pedro) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://database.mmsp-kn.de/">Konstanz visual quality databases</a> - Large-scale image and video databases for the development and  evaluation of visual quality assessment algorithms. (MMSP group,  University of Konstanz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www4.comp.polyu.edu.hk/~cslzhang/CDM_Dataset.htm">Kodak McMaster demosaic dataset</a> - (Zhang, Wu, Buades, Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.inf-cv.uni-jena.de/Research/Datasets/LabelMeFacade+Database.html">LabelMeFacade Database</a> - 945 labeled building images (Erik Rodner et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://online.uminho.pt/pessoas/smcn/hsi_spatial/HSI_illumination_2015.html">Local illumination hyperspectral radiance images</a> - Thirty hyperspectral radiance images of natural scenes with embedded  probe spheres for local illumination estimates(Sgio M. C. Nascimento,  Kinjiro Amano, David H. Foster) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pirsquared.org/research/mcgilldb/">McGill Calibrated Colour Image Database</a> (Adriana Olmos and Fred Kingdom) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.sz.tsinghua.edu.cn/labs/vipl/mdid.html">Multiply Distorted Image Database</a> -a database for evaluating the results of image quality assessment  metrics on multiply distorted images. (Fei Zhou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/D-X-Y/NAS-Projects/blob/master/NAS-Bench-102.md">NAS-Bench-102</a> - An algorithm-agnostic nas benchmark with detailed information  (training/validation/test loss/accuracy etc) of 15,625 architectures on  three datasets. (Xuanyi Dong) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gigl.scs.carleton.ca/benchmark_npr_general">NPRgeneral</a> - A standardized collection of images for evaluating image stylization algorithms. (David Mould, Paul Rosin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://symmetry.cs.nyu.edu/">NYU Symmetry Database</a> - 176 single-symmetry and 63 multyple-symmetry images (Marcelo Cicconet and Davi Geiger) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/oceandark/home">OceanDark dataset</a> - 100 low-lighting underwater images from underwater sites in the  Northeast Pacific Ocean. 1400x1000 pixels, varying lighting and  recording conditions (Ocean Networks Canada) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vcipl-okstate.org/pbvs/bench/">OTCBVS Thermal Imagery Benchmark Dataset Collection</a> (Ohio State Team) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/passta/">PAnorama Sparsely STructured Areas Datasets</a> - the PASSTA datasets used for evaluation of the image alignment (Andreas Robinson) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://qmul-openlogo.github.io/">QMUL-OpenLogo</a> - A  logo detection benchmark for testing the model generalisation capability in detecting a variety of logo objects in natural scenes with the  majority logo classes unlabelled. (QMUL Computer Vision Group) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=3D0">RESIDE (Realistic Single Image DEhazing)</a> - The current largest-scale benchmark consisting of both synthetic and  real-world hazy images, for image dehazing research.  RESIDE highlights  diverse data sources and image contents, and serves various training or  evaluation purposes. (Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan  Feng, Wenjun Zeng, Zhangyang Wang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://figshare.com/articles/Rijksmuseum_Challenge_2014/5660617">Rijksmuseum Challenge 2014</a> - It consist of 100K art objects from the rijksmuseum and comes with an extensive xml files describing each object. (Thomas Mensink and Jan van Gemert) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.engr.illinois.edu/~cchen156/SID.html">See in the Dark</a> - 77 Gb of dark images (Chen, Chen, Xu, and Koltun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.eecs.yorku.ca/~kamel/sidd/">Smartphone Image Denoising Dataset (SIDD)</a> - The Smartphone Image Denoising Dataset (SIDD) consists of about  30,000 noisy images with corresponding high-quality ground truth in both raw-RGB and sRGB spaces obtained from 10 scenes with different lighting conditions using five representative smartphone cameras. (Abdelrahman  Abdelhamed, Stephen Lin, Michael S. Brown) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html">Rendered WB dataset</a> - 100,000+ rendered sRGB images with different white balance (WB) settings (Afifi, Price, Cohen, Brown) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://3drepresentation.stanford.edu/">Stanford Street View Image, Pose, and 3D Cities Dataset</a> - a large scale dataset of street view images (25 million images and  118 matching image pairs) with their relative camera pose, 3D models of  cities, and 3D metadata of images. (Zamir, Wekel, Agrawal, Malik,  Savarese) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://testimages.org/">TESTIMAGES</a> - Huge and free  collection of sample images designed for analysis and quality assessment of different kinds of displays (i.e. monitors, televisions and digital  cinema projectors) and image processing techniques. (Nicola Asuni)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html">Time-Lapse Hyperspectral Radiance Images of Natural Scenes</a> - Four time-lapse sequences of 7-9 calibrated hyperspectral radiance  images of natural scenes taken over the day. (Foster, D.H., Amano, K.,  &amp; Nascimento, S.M.C.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html">Time-lapse hyperspectral radiance images</a> - Four time-lapse sequences of 7-9 calibrated hyperspectral images of  natural scenes, spectra at 10-nm intervals(David H. Foster, Kinjiro  Amano, Sgio M. C. Nascimento) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://horatio.cs.nyu.edu/mit/tiny/data/index.html">Tiny Images Dataset</a> 79 million 32x32 color images (Fergus, Torralba, Freeman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://amandaduarte.com.br/turbid/">TURBID Dataset</a> -  five different subsets of degraded images with its respective  ground-truth. Subsets Milk and DeepBlue have 20 images each and the  subset Chlorophyll has 42 images (Amanda Duarte) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/snapangle/">UT Snap Angle 360˚ Dataset</a> - A list of 360˚ videos of four activities (disney, parade, ski,  concert) from youtube (Kristen Grauman, UT Austin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.utexas.edu/projects/ego_snappoints/">UT Snap Point Dataset</a> - Human judgement on snap point quality of a subset of frames from UT  Egocentric dataset and a newly collected mobile robot dataset (frames    are also included) (Bo Xiong, Kristen Grauman, UT Austin) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ivi.fnwi.uva.nl/cv/intrinseg">UVA Intrinsic Images and Semantic Segmentation Dataset</a> - RGB dataset with ground-truth albedo, shading, and semantic annotations (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="https://visualdialog.org/">Visual Dialog</a> - 120k  human-human dialogs on COCO images, 10 rounds of QA per dialog (Das,  Kottur, Gupta, Singh, Yadav, Moura, Parikh, Batra) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://visualqa.org/">Visual Question Answering</a> - 254K imags, 764K questions, ground truth (Agrawal, Lu, Antol, Mitchell, Zitnick, Batra, Parikh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/download/details.aspx?id=53670">Visual Question Generation</a> - 15k images (including both object-centric and event-centric images),  75k natural questions asked about the images which can evoke further  conversation(Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret  Mitchell, Xiao dong He, Lucy Vanderwende) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://computing.ece.vt.edu/~abhshkdz/vqa-hat/">VQA Human Attention</a> - 60k human attention maps for visual question answering i.e. where  humans choose to look to answer questions about images (Das, Agrawal,  Zitnick, Parikh, Batra) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mklab.iti.gr/project/wild-web-tampered-image-dataset">Wild Web tampered image dataset</a> - A large collection of tampered images from Web and social media  sources, including ground-truth annotation masks for tampering  localization (Markos Zampoglou, Symeon Papadopoulos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.yfcc100m.org/">YFCC100M: The New Data in Multimedia Research</a> - This publicly available curated dataset of 100 million photos and  videos is free and legal for all. (Bart Thomee, Yahoo Labs and Flickr in San Francisco,etc.) [Before 28/12/19]</li>
</ol>
<h2 id="General-RGBD-and-Depth-Datasets"><a href="#General-RGBD-and-Depth-Datasets" class="headerlink" title="General RGBD and Depth Datasets"></a>General RGBD and Depth Datasets</h2><p>Note: there are 3D datasets elsewhere as well, <em>e.g.</em> in  <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#object">Objects</a>, <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#scene">Scenes</a>, and <a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#action">Actions</a>.</p>
<p>See also: <a target="_blank" rel="noopener" href="http://www.michaelfirman.co.uk/RGBDdatasets/">List of RGBD datasets</a>.</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://vcl3d.github.io/3D60/">3D60: 3D Vision Indoor Spherical Panoramas</a> - A multimodal dataset of 360 spherical panoramas containing paired  color images, depth and normal maps, as well as vertical and horizontal  stereo pairs (with their assorted depth and normal maps as well) that  can be used to train or evaluate a variety of 3D vision tasks. (Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Petros Daras) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://campar.in.tum.de/personal/slavcheva/3d-printed-dataset/index.html">3D-Printed RGB-D Object Dataset</a> - 5 objects with groundtruth CAD models and camera trajectories,  recorded with various quality RGB-D sensors. (Siemens &amp; TUM) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.rovit.ua.es/dataset/3dcomet/">3DCOMET</a> -  3DCOMET is a dataset for testing 3D data compression methods. (Miguel  Cazorla, Javier Navarrete,Vicente Morell, Miguel Cazorla, Diego Viejo,  Jose Garcia-Rodriguez, Sergio Orts.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/open?id=14h8dGmx3-CTCpTIiF6yzyfVTMI3u71GB">3D articulated body</a> - 3D reconstruction of an articulated body with rotation and  translation. Single camera, varying focal. Every scene may have an  articulated body moving. There are four kinds of data sets included. A  sample reconstruction result included which uses only four images of the scene. (Prof Jihun Park) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lgdv.cs.fau.de/publications/publication/Pub.2016.tech.IMMD.IMMD9.volume_6/">A Dataset for Non-Rigid Reconstruction from RGB-D Data</a> - Eight scenes for reconstructing non-rigid geometry from RGB-D data,  each containing several hundred frames along with our results. (Matthias Innmann, Michael Zollhoefer, Matthias Niessner, Christian Theobalt,  Marc Stamminger) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://redwood-data.org/3dscan/">A Large Dataset of Object Scans</a> - 392 objects in 9 casses, hundreds of frames each (Choi, Zhou, Miller, Koltun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvlab-dresden.de/iccv2015-articulation-challenge/">Articulated Object Challenge</a> -  4 articulated objects consisting of rigids parts connected by 1D  revolute and prismatic joints, 7000+ RGBD images with annotations for 6D pose estimation(Frank Michel, Alexander Krull, Eric Brachmann, Michael. Y. Yang,Stefan Gumhold, Carsten Rother) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rll.eecs.berkeley.edu/bigbird">BigBIRD</a> - 100  objects with for each object, 600 3D point clouds and 600  high-resolution color images spanning all views (Singh, Sha, Narayan,  Achim, Abbeel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://store.sae.org/caesar/">CAESAR</a> Civilian American  and European Surface Anthropometry Resource Project - 4000 3D human body scans (SAE International) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/browatbn.html">CIN 2D+3D object classification dataset</a> - segmented color and depth images of objects from 18 categories of  common household and office objects (Bjorn Browatzki et al) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://corbs.dfki.uni-kl.de/">CoRBS</a> - an RGB-D SLAM   benchmark, providing the combination of real depth and color data  together with a ground truth trajectory of the camera and a ground truth 3D model of the scene (Oliver Wasenmuller) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.csiro.au/robotics/databases/synthetic-data-non-rigid-3d-reconstruction/">CSIRO synthetic deforming people</a> - synthetic RGBD dataset for evaluating non-rigid 3D reconstruction: 2  subjects and 4 camera trajectories (Elanattil and Moghadam) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://clopema.felk.cvut.cz/garment_folding_photo_dataset.html">CTU Garment Folding Photo Dataset</a> - Color and depth images from various stages of garment folding.  (Sushkov R., Melkumov I., Smutn y V. (Czech Technical University in  Prague)) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://clopema.felk.cvut.cz/garment_sorting_dataset.html">CTU Garment Sorting Dataset</a> - Dataset of garment images, detailed stereo images, depth images and  weights. (Petrik V., Wagner L. (Czech Technical University in Prague))  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iri.upc.edu/groups/perception/#clothingDataset">Clothing part dataset</a> - The clothing part dataset consists  of image and depth scans,  acquired with a Kinect, of garments laying on  a table, with over a  thousand part annotations (collar, cuffs, hood, etc) using polygonal  masks. (Arnau Ramisa, Guillem Aleny, Francesc Moreno-Noguer and Carme  Torras) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pr.cs.cornell.edu/sceneunderstanding/data/data.php">Cornell-RGBD-Dataset</a> - Office Scenes (Hema Koppula) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvssp.org/projects/4d/dynamic_rgbd_modelling/">CVSSP Dynamic RGBD Modelling 2015</a> - This dataset contains eight RGBD sequences of general dynamic scenes  captured using the Kinect V1/V2 as well as two synthetic sequences.  (Charles Malleson, CVSSP, University of Surrey) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://campar.in.tum.de/personal/slavcheva/deformable-dataset/index.html">Deformable 3D Reconstruction Dataset</a> - two single-stream RGB-D sequences of dynamically moving mechanical  toys together with ground-truth 3D models in the canonical rest pose.  (Siemens, TUM) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://data.4tu.nl/repository/uuid:daea472d-2ca5-4765-9f1b-bd3200de4b41">Delft Windmill Interior and Exterior Laser Scanning Point Clouds</a> (Beril Sirmacek) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/PatrickChrist/diabetes60">Diabetes60</a> - RGB-D images of 60 western dishes, home made. Data was recorded using a Microsoft Kinect V2. (Patrick Christ and Sebastian Schlecht) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.eth3d.net/">ETH3D</a> - Benchmark for  multi-view stereo and 3D reconstruction, covering a variety of indoor  and outdoor scenes, with ground truth acquired by a high-precision laser scanner. (Thomas Sch??ps, Johannes L. Sch??nberger, Silvano Galliani,  Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rgb-d.eurecom.fr/">EURECOM Kinect Face Database</a> - 52 people, 2 sessions, 9 variations, 6 facial landmarks. (Jean-Luc DUGELAY et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">G4S meta rooms</a> - RGB-D data 150 sweeps with 18 images per sweep. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dream.georgiatech-metz.fr/?q=node/76">Georgiatech-Metz Symphony Lake Dataset</a> - 5 million RGBD outdoor images over 4 years from 121 surveys of a lakeshore. (Griffith and Pradalier) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/google/goldfinch">Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges</a> - a largescale dataset for finegrained bird (11K species),butterfly  (14K species), aircraft (409 types), and dog (515 breeds) recognition.  (Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander  Toshev, Tom Duerig, James Philbin, Li Fei-Fei) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www-users.cs.york.ac.uk/~nep/research/Headspace/">Headspace dataset</a> - The Headspace dataset is a set of 3D images of the full human head,  consisting of 1519 subjects wearing tight fitting latex caps to reduce  the effect of hairstyles. (Christian Duncan, Rachel Armstrong, Alder Hey Craniofacial Unit, Liverpool, UK) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/House3D">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvssp.org/impart/">IMPART multi-view/multi-modal 2D+3D film production dataset</a> - LIDAR, video,  3D models, spherical camera, RGBD, stereo, action,  facial expressions, etc. (Univ. of Surrey) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mvtec.com/company/research/datasets/mvtec-itodd/">Industrial 3D Object Detection Dataset (MVTec ITODD)</a> - depth and gray value data of 28 objects in 3500 labeled scenes for 3D object detection and pose estimation with a strong focus on industrial  settings and applications (MVTec Software GmbH, Munich) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/kinect2-dataset/">Kinect v2 Dataset</a> - Efficient Multi-Frequency Phase Unwrapping using Kernel Density Estimation (Felix etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://limu.ait.kyushu-u.ac.jp/~agri/komatsuna/">KOMATSUNA dataset</a> - The datasets is designed for instance segmentation, tracking and  reconstruction for leaves using both sequential multi-view RGB images  and depth images. (Hideaki Uchiyama, Kyushu University) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://make3d.cs.cornell.edu/data.html#make3d">Make3D Laser+Image data</a> - about 1000 RGB outdoor images with aligned laser depth images (Saxena, Chung, Ng, Sun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cim.mcgill.ca/~apl/database/">McGill-Reparti Artificial Perception Database</a> - RGBD data from four cameras and unfiltered Vicon skeletal data of two human subjects performing simulated assembly tasks on a car door  (Andrew Phan, Olivier St-Martin Cormier, Denis Ouellet, Frank P.  Ferrie). [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Meta rooms</a> - RGB-D data comprised of 28 aligned depth camera images collected by  having robot go to specific place and do 360 degrees of pan with various tilts. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://kovan.ceng.metu.edu.tr/MMStereoDataset/">METU Multi-Modal Stereo Datasets ???Benchmark Datasets for Multi-Modal Stereo-Vision???</a> - The METU Multi-Modal Stereo Datasets includes benchmark datasets for  for Multi-Modal Stereo-Vision which is composed of two datasets: (1) The synthetically altered stereo image pairs from the Middlebury Stereo  Evaluation Dataset and (2) the visible-infrared image pairs captured  from a Kinect device. (Dr. Mustafa Yaman, Dr. Sinan Kalkan) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">MHT RGB-D</a> - collected by a robot every 5 min over 16 days by the University of Lincoln. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://s.fhg.de/mini-rgbd">Moving INfants In RGB-D (MINI-RGBD)</a> - A synthetic, realistic RGB-D data set for infant pose estimation  containing 12 sequences of moving infants with ground truth joint  positions. (N. Hesse, C. Bodensteiner, M. Arens, U. G. Hofmann, R.  Weinberger, A. S. Schroeder) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dtic.ua.es/~agarcia/dataset">Multi-sensor 3D Object Dataset for Object Recognition with Full Pose Estimation</a> - Multi-sensor 3D Object Dataset for Object Recognition and Pose  Estimation(Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu  Oprea,etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/shahroudy/NTURGB-D">NTU RGB+D Action Recognition Dataset</a> - NTU RGB+D is a large scale dataset for human action recognition(Amir Shahroudy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth Dataset V2</a> - Indoor Segmentation and Support Inference from RGBD Images [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/">Oakland 3-D Point Cloud Dataset</a> (Nicolas Vandapel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.pacman-project.eu/datasets/">Pacman project</a> - Synthetic RGB-D images of 400 objects from 20 classes. Generated from  3D mesh models (Vladislav Kramarev, Umit Rusen Aktas, Jeremy L. Wyatt.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://adas.cvc.uab.es/phav/"> Procedural Human Action Videos</a> - This dataset contains about 40,000 videos for human action  recognition that had been generated using a 3D game engine. The dataset  contains about 6 million frames which can be used to train and evaluate  models not only action recognition but also models for depth map  estimation, optical flow, instance segmentation, semantic segmentation,  3D and 2D pose estimation, and attribute learning. (Cesar Roberto de  Souza) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1601.05511">RGB-D-based Action Recognition Datasets</a> - Paper that includes the list and links of different rgb-d action  recognition datasets. (Jing Zhang, Wanqing Li, Philip O. Ogunbona,  Pichao Wang, Chang Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.umiacs.umd.edu/~amyers/part-affordance-dataset/">RGB-D Part Affordance Dataset</a> - RGB-D images and ground-truth affordance labels for 105 kitchen,  workshop and garden tools, and 3 cluttered scenes (Myers, Teo,  Fermuller, Aloimonos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.scan-net.org/">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</a> - ScanNet is a dataset of richly-annotated RGB-D scans of real-world  environments containing 2.5M RGB-D images in more than 1500 scans,  annotated with 3D camera poses, surface reconstructions, and  instance-level semantic segmentations. (Angela Dai, Angel X. Chang,  Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Niessner)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.scenenn.net/">SceneNN: A Scene Meshes Dataset with aNNotations</a> - RGB-D scene dataset with 100+ indoor scenes, labeled triangular mesh, voxel and pixel. (Hua, Pham, Nguyen, Tran, Yu, and Yeung) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.semantic3d.net/">Semantic-8</a>: 3D point cloud classification with 8 classes (ETH Zurich) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Small office data sets</a> - Kinect depth images every 5 seconds beginning in April 2014 and on-going. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lttm.dei.unipd.it/paper_data/fusion/">Stereo and ToF dataset with ground truth</a> - The dataset contains 5 different scenes acquired with a  Time-of-flight sensor and a stereo setup. Ground truth information is  also provided. (Carlo Dal Mutto, Pietro Zanuttigh, Guido M. Cortelazzo)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.synthia-dataset.net/">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://taskonomy.stanford.edu/">Taskonomy</a> - Over 4.5  million real images each with ground truth for 25 semantic, 2D, and 3D  tasks. (Zamir, Sax, Shen, Guibas, Malik, Savarese) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~harel/TAUAgent/home.html">TAU Agent Dataset</a> - a high-resolution RGB-D dataset, created using Blender. Contains 530  high-resolution RGB images with corresponding pixel-wise ground truth  depth maps (Haim, Elmalem, Giryes, Bronstein, and Marom) [30/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php">THU-READ(Tsinghua University RGB-D Egocentric Action Dataset)</a> - THU-READ is a large-scale dataset for action recognition in RGBD  videos with pixel-lever hand annotation. (Yansong Tang, Yi Tian, Jiwen  Lu, Jianjiang Feng, Jie Zhou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.research.ed.ac.uk/portal/en/datasets/trimbot2020-dataset-for-garden-navigation-and-bush-trimming(9f9de786-5e58-4bca-9279-f1d7ffddda41).html">TrimBot2020 Dataset for Garden Navigation</a> – sensor RGBD data recorded from cameras and other sensors mounted on a robotic platform as well as additional external sensors capturing the  garden (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/data/datasets/rgbd-dataset">TUM RGB-D Benchmark</a> - Dataset and benchmark for the evaluation of RGB-D visual odometry and SLAM algorithms (Jorgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard and Daniel Cremers) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cgvr.informatik.uni-bremen.de/research/asula/index.shtml">Uni Bremen Open, Abdominal Surgery RGB Dataset</a> - Recording of a complete, open, abdominal surgery using a Kinect v2  that was mounted directly above the patient looking down at patient and  staff. (Joern Teuber, Gabriel Zachmann, University of Bremen) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://marathon.csee.usf.edu/range/DataBase.html">USF Range Image Database</a> - 400+ laser range finder and structured light camera images, many with ground truth segmentations (Adam et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rgbd-dataset.cs.washington.edu/">Washington RGB-D Object Dataset</a> - 300 common household objects and 14 scenes. (University of Washington and Intel Labs Seattle) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Witham Wharf</a> - For RGB-D of eight locations collect by robot every 10 min over ~10  days by the University of Lincoln. (John Folkesson et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www-users.cs.york.ac.uk/~nep/research/YEM/">York 3D Ear Dataset</a> - The York 3D Ear Dataset  is a set of 500 3D ear images, synthesized  from detailed 2D landmarking, and available in both Matlab format (.mat) and PLY format (.ply). (Nick Pears, Hang Dai, Will Smith, University of York) [Before 28/12/19]</li>
</ol>
<h2 id="General-Videos"><a href="#General-Videos" class="headerlink" title="General Videos"></a>General Videos</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www2.compute.dtu.dk/~sohau/augmentations/">AlignMNIST</a> - An artificially extended version of the MNIST handwritten dataset. (en Hauberg) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/audiovisualresearch">Audio-Visual Event (AVE) dataset</a>- AVE dataset contains 4143 YouTube videos covering 28 event categories  and videos in AVE dataset are temporally labeled with audio-visual event boundaries. (Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and  Chenliang Xu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/">Dataset of Multimodal Semantic Egocentric Video (DoMSEV)</a> - Labeled 80-hour Dataset of Multimodal Semantic Egocentric Videos  (DoMSEV) covering a wide range of activities, scenarios, recorders,  illumination and weather conditions. (UFMG, Michel Silva, Washington  Ramos, Jo??o Ferreira, Felipe Chamone, Mario Campos, Erickson R.  Nascimento) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://davischallenge.org/">DAVIS: Video Object Segmentation dataset 2016</a> - A Benchmark Dataset and Evaluation Methodology for Video Object  Segmentation (F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.  Gross, and A. Sorkine-Hornung) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://davischallenge.org/">DAVIS: Video Object Segmentation dataset 2017</a> -  The 2017 DAVIS Challenge on Video Object Segmentation (J.  Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://iplab.dmi.unict.it/EGO-CH/">EGO-CH</a> - a large  egocentric video dataset acquired by real visitors in two different  cultural sites. The dataset includes more than 27 hours of video  acquired by 70 different subjects. The overall dataset includes labels  for 26 environments and over 200 Points of Interest (POIs). (Giovanni  Maria Farinella) [31/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/FAIR-Play">FAIR-Play</a> - 1,871 video clips (~5 hrs) and their corresponding binaural audio clips recorded in a music room (Gao and Grauman) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/">GoPro-Gyro Dataset</a> - ego centric videos (Linkoping Computer Vision Laboratory) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://live.ece.utexas.edu/research/Quality/index.htm">Image &amp; Video Quality Assessment at LIVE</a> - used to develop picture quality algorithms (the University of Texas at Austin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://itee.uq.edu.au/~shenht/UQ_VIDEO/">Large scale YouTube video dataset</a> - 156,823 videos (2,907,447 keyframes) crawled from YouTube videos (Yi Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset">Movie Memorability Dataset</a>  - memorable movie clips and ground truth of detail memorability, 660  short movie excerpts extracted from 100 Hollywood-like movies (Cohendet, Yadati, Duong and Demarty) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://movieqa.cs.toronto.edu/">MovieQA</a> - each machines to understand stories by answering questions about them. 15000 multiple choice QAs, 400+ movies. (M. Tapaswi, Y. Zhu, R. Stiefelhagen, A.  Torralba, R. Urtasun, and S. Fidler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://moments.csail.mit.edu/">Moments in Time Dataset</a> - Moments in Time Dataset 1M 3-second videos annotated with action type,  the largest dataset of its kind for action recognition and understanding in video. (Monfort, Oliva, et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://itee.uq.edu.au/~shenht/UQ_VIDEO/">Near duplicate video retrieval dataset</a> - This database consists of 156,823 videos sequences (2,907,447  keyframes), which were crawled from YouTube during the period of July  2010 to September 2010. (Jingkuan Song, Yi Yang, Zi Huang, Heng Tao  Shen, Richang Hong) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/gyglim/personalized-highlights-dataset">PHD2: Personalized Highlight Detection Dataset</a> - PHD2 is a dataset with personalized highlight information, which  allows to train highlight detection models that use information about  the user, when making predictions. (Ana Garcia del Molino, Michael  Gygli) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/deepvideo/">Sports-1M</a> - Dataset for sports video classification containing 487 classes and  1.2M videos. (Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.nuscenes.org/">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://seungjunnah.github.io/Datasets/reds">REDS (REalistic and Dynamic Scenes)</a> - high-quality realistic blurry video dataset with reference sharp  frames (improved version of GOPRO) (Nah, Baik, Hong, Moon, Son, Timofte  and Lee) [4/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Video Sequences</a>used for research on Euclidean upgrades based on minimal assumptions about the camera(Kenton McHenry) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/stacking-dataset/">Video Stacking Dataset</a> - A Virtual Tripod for Hand-held Video Stacking on Smartphones (Erik Ringaby etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.interdigital.com/data_sets/video-memorability-dataset">VideoMem Dataset</a> - The VideoMem or Video Memorability Database is a collection of  sound-less video excerpts and their corresponding ground-truth  memorability files. The memorability scores are computed based on the  measurement of short-term and long-term memory performances when  recognizing small video excerpts a few minutes after viewing them for  the short-term case, and 24 to 72 hours later, for the long-term case.  It is accompanied with video features extracted from the video excerpts. It is intended to be used for understanding the memorability of videos  and for assessing the quality of methods for predicting the memorability of multimedia content. (Cohendet, Demarty, Duong and Engilberge)  [6/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/videosearch100m/home">YFCC100M videos</a> - A benchmark on the video subset of YFCC100M which includes the  videos, he video content features and the API to a sate-of-the-art video content engine. (Lu Jiang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.yfcc100m.org/">YFCC100M: The New Data in Multimedia Research</a> - This publicly available curated dataset of 100 million photos and  videos is free and legal for all. (Bart Thomee, Yahoo Labs and Flickr in San Francisco,etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube-bb/">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube8m">YouTube-8M</a> -  Dataset for video classification in the wild, containing pre-extracted  frame level features from 8M videos, and 4800 classes. (Sami  Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev,George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.eecs.yorku.ca/research/dynamic-scenes/">YUP++ / Dynamic Scenes dataset</a> - 20 outdoor scene classes, each with 60 colour videos (each 5 seconds, 480 pixels wide, 24-30 fps) from 60 different scenes. Half of the  videos are with a static camera and half with a moving camera  (Feichtenhofer, Pinz, Wildes) [Before 28/12/19]</li>
</ol>
<h2 id="Hand-Hand-Grasp-Hand-Action-and-Gesture-Databases"><a href="#Hand-Hand-Grasp-Hand-Action-and-Gesture-Databases" class="headerlink" title="Hand, Hand Grasp, Hand Action and Gesture Databases"></a>Hand, Hand Grasp, Hand Action and Gesture Databases</h2><ol>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/11khands">11k Hands</a> -  11,076 hand images (1600 x 1200 pixels) of 190 subjects, of varying ages between 18 - 75, with metadata (id, gender, age, skin color,  handedness, which hand, accessories, etc). (Mahmoud Afifi) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.twentybn.com/datasets/jester">20bn-Jester</a> - densely-labeled video clips that show humans performing predefined hand gestures in front of a laptop camera or webcam (Twenty Billion Neurons  GmbH) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iis.ee.ic.ac.uk/~dtang/hand.html">3D Articulated Hand Pose Estimation with Single Depth Images</a> (Tang, Chang, Tejani, Kim, Yu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://hpes.bii.a-star.edu.sg/">A-STAR Annotated Hand-Depth Image Dataset and its Performance Evaluation</a> - depth data and data glove data, 29 images of 30 volunteers, Chinese  number counting and American Sign Language (Xu and Cheng) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bosphorus.ee.boun.edu.tr/hand/">Bosphorus Hand Geometry Database and Hand-Vein Database</a> (Bogazici University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://robocoffee.org/datasets/">A Dataset of Human Manipulation Actions</a> - RGB-D of 25 objects and 6 actions (Alessandro Pieropan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.demcare.eu/results/datasets">DemCare dataset</a> - DemCare dataset consists of a set of diverse data collection from  different sensors and is useful for human activity recognition from  wearable/depth and static IP camera, speech recognition for Alzheimmer’s disease detection and physiological data for gait analysis and  abnormality detection. (K. Avgerinakis, A.Karakostas, S.Vrochidis, I.  Kompatsiaris) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.ibm.com/dvsgesture/">DVS128 Gesture Dataset</a> - Event-based dataset, containing sequences of 11 hand gestures,  performed by 29 subjects under several illumination conditions,captured  using a DVS128 sensor. Each sequence is annotated with the start and  stop times of each gesture. (Amir, Taba, Berg, Melano, McKinstry, Di  Nolfo, Nayak, Andreopoulos, Garreau, Mendoza, Kusnitz, Debole, Esser,  Delbruck, Flickner, and Modha)  [7/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html">EgoGesture Dataset</a> - First-person view gestures with 83 classes, 50 subjects, 6 scenes,  24161 RGB-D video samples (Zhang, Cao, Cheng, Lu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.soic.indiana.edu/projects/egohands/">EgoHands</a> - A large dataset with over 15,000 pixel-level-segmented hands recorded from egocentric cameras of people interacting with each other. (Sven  Bambach) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/aurooj/Hand-Segmentation-in-the-Wild">EgoYouTubeHands dataset</a> - An egocentric hand segmentation dataset consists of 1290 annotated  frames from YouTube videos recorded in unconstrained real-world  settings. The videos have variation in environment, number of  participants, and actions. This dataset is useful to study hand  segmentation problem in unconstrained settings. (Aisha Urooj, A. Borji)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrlcode.ics.forth.gr/handtracking/">FORTH Hand tracking library</a> (FORTH) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://wildhog.ics.uci.edu:9090/full.html">General HANDS: general hand detection and pose challenge</a> - 22 sequences with different gestures, activities and viewpoints (UC Irvine) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.gregrogez.net/research/egovision4health/gun-71/">Grasp UNderstanding (GUN-71) dataset</a> -  12,000 first-person RGB-D images of object manipulation scenes  annotated using a taxonomy of 71 fine-grained grasps. (Rogez, Supancic  and Ramanan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-vpu.eps.uam.es/DS/HGds/">A Hand Gesture Detection Dataset</a> (Javier Molina et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.intelligence.tuc.gr/~petrakis/downloads/spatial-datasets-evaluation.zip">Hand gesture and marine silhouettes</a> (Euripides G.M. Petrakis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.technion.ac.il/~twerd/HandNet/">HandNet: annotated depth images of articulated hands</a> 214971 annotated depth images of hands captured by a RealSense RGBD  sensor of hand poses. Annotations:  per pixel classes, 6D fingertip  pose, heatmap. Train: 202198, Test: 10000, Validation: 2773. Recorded at GIP Lab, Technion.   [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/aurooj/Hand-Segmentation-in-the-Wild">HandOverFace dataset</a> -  A hand segmentation dataset consists of 300 annotated frames from  the web to study the hand-occluding-face problem. (Aisha Urooj, A.  Borji) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.idiap.ch/resource/gestures/">IDIAP Hand pose/gesture datasets</a> (Sebastien Marcel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lttm.dei.unipd.it/downloads/gesture/">Kinect and Leap motion gesture recognition dataset</a> -  The dataset contains 1400 different gestures acquired with both the  Leap Motion and the Kinect devices(Giulio Marin, Fabio Dominio, Pietro  Zanuttigh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lttm.dei.unipd.it/downloads/gesture/">Kinect and Leap motion gesture recognition dataset</a> - The dataset contains several different static gestures acquired with  the Creative Senz3D camera. (A. Memo, L. Minto, P. Zanuttigh) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrr.ucsd.edu/LISA/hand.html">LISA CVRR-HANDS 3D</a> - 19 gestures performed by 8 subjects as car driver and passengers (Ohn-Bar and Trivedi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/dexter1.htm">MPI Dexter 1 Dataset for Evaluation of 3D Articulated Hand Motion Tracking</a> - Dexter 1: 7 sequences of challenging, slow and fast hand motions, RGB + depth (Sridhar, Oulasvirta, Theobalt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/people/yichenw/handtracking/">MSR Realtime and Robust Hand Tracking from Depth</a> - (Qian, Sun, Wei, Tang, Sun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.mutah.edu.jo/biometrix/hand-images-databases.html">Mobile and Webcam Hand images database</a> - MOHI and WEHI - 200 people, 30 images each (Ahmad Hassanat) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1f8tUHid1KmnwbgskGMXmobOxMfbxIgHM/view?usp=sharing">NTU-Microsoft Kinect HandGesture Dataset</a> - This is a RGB-D dataset of hand gestures, 10 subjects x 10 hand  gestures x 10 variations. (Zhou Ren, Junsong Yuan, Jingjing Meng, and  Zhengyou Zhang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.c3imaging.org/?page_id=3D772">NUIG_Palm1</a> -  Database of palmprint images acquired in unconstrained conditions using  consumer devices for palmprint recognition experiments. (Adrian-Stefan  Ungureanu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm">NYU Hand Pose Dataset</a> - 8252 test-set and 72757 training-set frames of captured RGBD data  with ground-truth hand-pose, 3 views (Tompson, Stein, Lecun, Perlin)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://team.inria.fr/stars/praxis-dataset/">PRAXIS gesture dataset</a> - RGB-D upper-body data from 29 gestures, 64 volunteers, several  repetitions, many volunteers have some cognitive impairment (Farhood  Negin, INRIA) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html">Rendered Handpose Dataset</a> - Synthetic dataset for 2D/ 3D Handpose Estimation with RGB, depth,  segmentation masks and 21 keypoints per hand (Christian Zimmermann and  Thomas Brox) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-50.php">RWTH-Boston-50</a> and <a target="_blank" rel="noopener" href="https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-104.php">RWTH-Boston-104</a> - American Sign Language hand gesture video datasets, containing 201  annotated sentences captured by 4 cameras (2 B/W stereo, 1 color, one  side view B/W) atg 30 fps and 312*242 pixels. The 50 dataset has 483  utterances of 50 words. (Dreuw, Keysers, Forster, Deselaers, Rybach,  Zahedi, Ney) [14/3/20]</li>
<li><a target="_blank" rel="noopener" href="http://ee.sut.ac.ir/showcvmain.aspx?id=5">Sahand Dynamic Hand Gesture Database</a> -  This database contains 11 Dynamic gestures designed to convey the  functions of mouse and touch screens to computers. (Behnam Maleki,  Hossein Ebrahimnezhad) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm">Sheffield gesture database</a> - 2160 RGBD hand gesture sequences, 6 subjects, 10 gestures, 3  postures, 3 backgrounds, 2 illuminations (Ling Shao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.hci.iis.u-tokyo.ac.jp/~cai-mj/utgrasp_dataset.html">UT Grasp Data Set</a> - 4 subjects grasping a variety of objectss with a variety of grasps (Cai, Kitani, Sato) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eng.yale.edu/grablab/humangrasping/">Yale human grasping data set</a> - 27 hours of video with tagged grasp, object, and task data from two  housekeepers and two machinists (Bullock, Feix, Dollar) [Before  28/12/19]</li>
</ol>
<h2 id="Image-Video-and-Shape-Database-Retrieval"><a href="#Image-Video-and-Shape-Database-Retrieval" class="headerlink" title="Image, Video and Shape Database Retrieval"></a>Image, Video and Shape Database Retrieval</h2><ol>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/~laehner/Elastic2D3D/">2D-to-3D Deformable Sketches</a> - A collection of deformable 2D contours in pointwise correspondence  with deformable 3D meshes of the same class; around 10 object classes  are provided, including humans and animals. (Lahner, Rodola) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dais.unive.it/~cosmo/deformableclutter/">3D Deformable Objects in Clutter</a> - A dataset for 3D deformable object-in-clutter, with point-wise ground truth correspondence across hundreds of scenes and spanning multiple  classes (humans, animals). (Cosmo, Rodola, Masci, Torsello, Bronstein)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lear.inrialpes.fr/~jegou/data.php">ANN_SIFT1M</a> - 1M Flickr images encoded by 128D SIFT descriptors (Jegou et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.lems.brown.edu/content/available-software-and-databases">Brown Univ 25/99/216 Shape Databases</a> (Ben Kimia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> - 60K 32x32 images from 10 classes, with a 512D GIST descriptor (Alex Krizhevsky) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ir-facility.org/clef-ip">CLEF-IP 2011 evaluation on patent images</a> [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~mengtial/proj/sketch/">Contour Drawing Dataset</a> - a dataset of 5,000 paired images and contour drawings for the study  of visual understanding and sketch generation (Li, Lin, Měch, Yumer, and Ramanan) [9/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html">DeepFashion</a> - Large-scale Fashion Database(Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://diameter.itn.liu.se/emodb/">EMODB</a> - Thumbnails  of images in the picsearch image search engine together with the  picsearch emotion keywords (Reiner Lenz etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mfdemirci.etu.edu.tr/Etu10Silhouette.rar">ETU10 Silhouette Dataset</a> - The dataset consists of 720 silhouettes of 10 objects, with 72 views  per object. (M. Akimaliev and M.F. Demirci) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cvjena/eu-flood-dataset">European Flood 2013</a> - 3,710 images of a flood event in central Europe, annotated with  relevance regarding 3 image retrieval tasks (multi-label) and important  image regions. (Friedrich Schiller University Jena, Deutsches  GeoForschungsZentrum Potsdam) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> - A MNIST-like fashion product database. (Han Xiao, Zalando Research) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cise.ufl.edu/~anand/GatorBait_100.tgz">Fish Shape Database</a> - It’s a Fish Shape Database with 100, 2D point set shapes. (Adrian M. Peter) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://shannon.cs.illinois.edu/DenotationGraph/">Flickr 30K</a> - images, actions and captions (Peter Young et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://personal.ee.surrey.ac.uk/Personal/R.Hu/SBIR.html">Flickr15k - Sketch based Image Retrieval (SBIR) Benchmark</a> - Dataset of 330 sketches and 15,024 photos comprising 33 object  categories,benchmark dataset commonly used to evaluate Sketch based  Image Retrieval (SBIR) algorithms. (Hu and Collomosse, CVIU 2013)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture">Hands in action (HIC) IJCV dataset</a> - Data (images, models, motion) for tracking 1 hand or 2 hands with/o 1 object. Includes both *single-view RGB-D sequences (1 subject, &gt;18  annotated sequences, 4 objects, complete RGB image), and *multi-view RGB sequences (1 subject, HD, 8 views, 8 sequences - 1 annotated, 2  objects). (Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo  Aponte, Marc Pollefeys, Juergen Gall) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-i6.informatik.rwth-aachen.de/imageclef/08/photo/">IAPR TC-12 Image Benchmark</a> (Michael Grubinger) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://imageclef.org/SIAPRdata">IAPR-TC12 Segmented and annotated image benchmark (SAIAPR TC-12):</a> (Hugo Jair Escalante) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.imageclef.org/2010/PhotoAnnotation">ImageCLEF 2010 Concept Detection and Annotation Task</a> (Stefanie Nowak) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.imageclef.org/2011/photo">ImageCLEF 2011 Concept Detection and Annotation Task</a> - multi-label classification challenge in Flickr photos [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://lear.inrialpes.fr/~jegou/data.php#copydays">INRIA Copydays dataset</a> - for evaluation of copy detection: JPEG, cropping and “strong” copy attacks. (INRIA) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://lear.inrialpes.fr/~jegou/data.php#holidays">INRIA Holidays dataset</a> - for evaluation of image search: 500 queries and 991 corresponding relevant images (Jegou, Douze and Schmid) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/333579741_MA14KD_ORIGINAL_Dataset_Description_Visual_Attraction_of_Movie_Trailers">MA14KD (Movie Attraction 14K Dataset) Dataset</a> - 14K movie/TV trailers, 10 features each, links to a rating dataset  (Elahi, Moghaddam, Hosseini, Trattner, Tkalčič) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://kovan.ceng.metu.edu.tr/LogoDataset/">METU Trademark dataset</a>The METU Dataset is composed of more than 900K real logos belonging to  companies worldwide. (Usta Bilgi Sistemleri A.S. and Grup Ofis Marka  Patent A.S) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cim.mcgill.ca/~shape/benchMark/">McGill 3D Shape Benchmark</a> (Siddiqi, Zhang, Macrini, Shokoufandeh, Bouix, Dickinson) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mano.is.tue.mpg.de/">MPI MANO &amp; SMPL+H dataset</a> - Models, 4D scans and registrations for the statistical models MANO  (hand-only) and SMPL+H (body+hands). For MANO there are ~2k static 3D  scans of 31 subjects performing up to 51 poses. For SMPL+H we include 39 4D sequences of 11 subjects. (Javier Romero, Dimitrios Tzionas and  Michael J Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://grail.cs.washington.edu/projects/mview/">Multiview Stereo Evaluation</a> - Each dataset is registered with a “ground-truth” 3D model acquired  via a laser scanning process(Steve Seitz  et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.itl.nist.gov/iad/vug/sharp/contest/2014/SBR/data.html">NIST SHREC - 2014 NIST retrieval contest databases and links</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.itl.nist.gov/iad/vug/sharp/contest/2013/SBR/data.html">NIST SHREC - 2013 NIST retrieval contest databases and links</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.itl.nist.gov/iad/vug/sharp/contest/2010/NonRigidShapes/">NIST SHREC 2010 - Shape Retrieval Contest of Non-rigid 3D Models</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-nlpir.nist.gov/projects/trecvid/">NIST TREC Video Retrieval Evaluation Database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm">NUS-WIDE</a> - 269K Flickr images annotated with 81 concept tags, enclded as a 500D BoVW descriptor (Chau et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://shape.cs.princeton.edu/benchmark/index.cgi">Princeton Shape Benchmark</a> (Princeton Shape Retrieval and Analysis Group) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://files.is.tue.mpg.de/dtzionas/GCPR_2013">PairedFrames - evaluation of 3D pose tracking error</a> - Synthetic and Real dataset to test 3D pose tracking/refinement with  pose initialization close/far to/from minima. Establishes testing frame  pairs of increasing difficulty, to measure the pose estimation error  separately, without employing a full tracking pipeline. (Dimitrios  Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://itee.uq.edu.au/~shenht/UQ_IMH/index.htm">Queensland cross media dataset</a> - millions of images and text documents for “cross-media” retrieval (Yi Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://files.is.tue.mpg.de/dtzionas/Skeleton-Reconstruction">Reconstructing Articulated Rigged Models from RGB-D Videos (RecArt-D)</a> - Dataset of objects deforming during manipulation. Includes 4 RGB-D  sequences (RGB image complete), result of deformable tracking for each  object, as well as 3D mesh and Ground-Truth 3D skeleton for each object. (Dimitrios Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning">Reconstruction from Hand-Object Interactions (R-HOI)</a> - Dataset of one hand interacting with an unknown object. Includes 4  RGB-D sequences, in total 4 objects, the RGB image is complete. Includes tracked 3D motion and Ground-Truth meshes for the objects. (Dimitrios  Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cmp.felk.cvut.cz/revisitop/">Revisiting Oxford and Paris (RevisitOP)</a> - Improved and more challenging version (fixed errors, new annotation  and evaluation protocols, new query images) of the well known  landmark/building retrieval datasets accompanied with 1M distractor  images. (F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, O. Chum)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dais.unive.it/~shrec2016/index.php">SHREC’16 Deformable Partial Shape Matching</a> - A collection of around 400 3D deformable shapes undergoing strong  partiality transformations, with point-to-point ground truth  correspondence included. (Cosmo, Rodola, Bronstein, Torsello) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs.txstate.edu/~yl12/SBR2016/data.html">SHREC 2016 - 3D Sketch-Based 3D Shape Retrieval</a> - data to evaluate the performance of different 3D sketch-based 3D  model retrieval algorithms using a hand-drawn 3D sketch query dataset on a generic 3D model dataset  (Bo Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/shrec17/">SHREC’17 Deformable Partial Shape Retrieval</a> - A collection of around 4000 deformable 3D shapes undergoing severe  partiality transformations, in the form of irregular missing parts and  range data; ground truth class information is provided. (Lahner, Rodola) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://watertight.ge.imati.cnr.it/">SHREC Watertight Models Track (of SHREC 2007)</a> - 400 watertight 3D models (Daniela Giorgi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://partial.ge.imati.cnr.it/">SHREC Partial Models Track (of SHREC 2007)</a> - 400 watertight 3D DB models and 30 reduced watertight query models (Daniela Giorgi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.virginia.edu/~vicente/sbucaptions/">SBU Captions Dataset</a> - image captions collected for 1 million images from Flickr (Ordonez, Kulkarni and Berg) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html">Sketch me That Shoe</a> - Sketch-based object retrieval in a fine-grained setting. Match  sketches to specific shoes and chairs. (Qian Yu, QMUL, T. Hospedales  Edinburgh/QMUL). [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tosca.cs.technion.ac.il/book/resources_data.html">TOSCA 3D shape database</a> (Bronstein, Bronstein, Kimmel) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/totally-looks-like-dataset">Totally Looks Like</a> - A benchmark for assessment of predicting human-based image similarity (Amir Rosenfeld, Markus D. Solbach, John Tsotsos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/data/Cross-View/">UCF-CrossView Dataset: Cross-View Image Matching for Geo-localization in Urban Environments</a> - A new dataset of street view and bird’s eye view images for  cross-view image geo-localization. (Center for Research in Computer  Vision, University of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube8m/">YouTube-8M Dataset</a> - A Large and Diverse Labeled Video Dataset for Video Understanding Research. (Google Inc.) [Before 28/12/19]</li>
</ol>
<h2 id="Object-Databases"><a href="#Object-Databases" class="headerlink" title="Object Databases"></a>Object Databases</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www.csse.uwa.edu.au/~ajmal/databases.html">2.5D/3D Datasets of various objects and scenes</a> (Ajmal Mian) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">3D Object Recognition Stereo Dataset</a>This dataset consists of 9 objects and 80 test images.  (Akash Kushal and Jean Ponce) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">3D Photography Dataset</a>a collection of ten multiview data sets captured in our lab(Yasutaka Furukawa and Jean Ponce) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://campar.in.tum.de/personal/slavcheva/3d-printed-dataset/index.html">3D-Printed RGB-D Object Dataset</a> - 5 objects with groundtruth CAD models and camera trajectories,  recorded with various quality RGB-D sensors(Siemens &amp; TUM) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">3DNet Dataset</a> - The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. (John Folkesson et al.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://deep-geometry.github.io/abc-dataset/">ABC Dataset</a> - A million CAD models, including ground analytical descriptions  (spline patches), dense meshes, point clouds, normals. (Koch, Matveev,  Jiang, Williams, Artemov, Burnaev, Alexa, Zorin, Panozzo) [2/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Yang7879/3D-RecGAN-extended">Aligned 2.5D/3D datasets of various objects</a> - Synthesized and real-world datasets for object reconstruction from a  single depth view. (Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://aloi.science.uva.nl/">Amsterdam Library of Object Images (ALOI): 100K views of 1K objects</a> (University of Amsterdam/Intelligent Sensory Information Systems) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://cvwc2019.github.io/challenge.html">ATRW - Amur Tiger Re-identification in the Wild</a> - 8,000 Amur tiger video clips of 92 individuals (MakerCollider and WWF) [26/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://cvml.ist.ac.at/AwA2/">Animals with Attributes 2</a> - 37322 (freely licensed) images of 50 animal classes with 85 per-class binary attributes. (Christoph H. Lampert, IST Austria) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://hemanthdv.org/OfficeHome-Dataset/">ASU Office-Home Dataset</a> - Object recognition dataset of everyday objects for domain adaptation  (Venkateswara, Eusebio, Chakraborty, Panchanathan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://kinectdata.com/">B3DO: Berkeley 3-D Object Dataset</a> - household object detection (Janoch et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.bris.ac.uk/~damen/BEOID/">Bristol Egocentric Object Interactions Dataset</a> - egocentric object interactions with synchronised gaze (Dima Damen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jcpeterson/cifar-10h">CIFAR-10H</a> - a  new dataset of soft labels reflecting human perceptual uncertainty for  the 10,000-image CIFAR-10 test set (Peterson, Battleday, Griffiths,  Russakovsky)  [14/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.uiuc.edu/CORE/">CORE image dataset</a> -  to help learn more detailed models and for exploring cross-category  generalization in object recognition. (Ali Farhadi, Ian Endres, Derek  Hoiem, and David A. Forsyth) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://clopema.felk.cvut.cz/color_and_depth_dataset.html">CTU Color and Depth Image Dataset of Spread Garments</a> - Images of spread garments with annotated corners. (Wagner, L., Krejov D., and Smutn V. (Czech Technical University in Prague)) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">Caltech 101 (now 256) category object recognition database</a> (Li Fei-Fei, Marco Andreeto, Marc’Aurelio Ranzato) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://perceive.dieei.unict.it/index-dataset.php?name=Fish_Species">Catania Fish Species Recognition</a> - 15 fish species, with about 20,000 sample training images and additional test images (Concetto Spampinato) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/nightrome/cocostuff">COCO-Stuff dataset</a> - 164K images labeled with ‘things’ and ‘stuff’ (Caesar, Uijlings, Ferrari) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php">Columbia COIL-100 3D object multiple views</a> (Columbia University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/data/tvg/sjetley/">Country Flags in the Wild</a> - 12,854 train images and 6,110 test images of the flags of 224  different countries manually cropped to loosely fit to the inlying  flags. (Jetley) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://gdo152.llnl.gov/cowc/">COWC</a> - Cars Overhead  with Context. 32,716 unique annotated cars. 58,247 unique negative  examples. 15 cm per pixel resolution, from six distinct locations.  (Lawrence Livermore National Laboratory) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://domaingeneralization.github.io/">Deeper, Broader and Artier Domain Generalization</a> - Domain generalisation task dataset. (Da Li, QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.inf.fh-dortmund.de/personen/professoren/peters/pages/research/ImageDatabase/ImageDatabase.html">Densely sampled object views: 2500 views of 2 objects, eg for view-based recognition and modeling</a> (Gabriele Peters, Universiteit Dortmund) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/UTENSILS/">Edinburgh Kitchen Utensil Database</a> - 897 raw and binary images of 20 categories of kitchen utensil, a  resource for training future domestic assistance robots (D. Fullerton,  A. Goel, R. B. Fisher) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ub.edu/cvub/edub-obj/">EDUB-Obj</a> - Egocentric dataset for object localization and segmentation. (Marc Bolaños and Petia Radeva.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://docs.google.com/file/d/0B10RxHxW3I92ZUtDU0RkMGlnNkU/edit?pref=2&pli=1">Ellipse finding dataset</a> (Dilip K. Prasad et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/">FGVC-Aircraft Benchmark</a> - 10,200 images of aircraft, with 100 images for each of 102 different  aircraft model variants (Maji, Kannala, Rahtu, Blaschko, Vedaldi)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://etsin.avointiede.fi/dataset/urn-nbn-fi-csc-kata20170615175247247938">FIN-Benthic</a> - This is a dataset for automatic fine-grained classification of  benthic macroinvertebrates. There are 15074 images from 64 categories.  The number of images per category varies from 577 to 7. (Jenni  Raitoharju, Ekaterina Riabchenko, Iftikhar Ahmad, Alexandros Iosifidis,  Moncef Gabbouj, Serkan Kiranyaz, Ville Tirronen, Johanna Arje) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rubi.ucsd.edu/GERMS/">GERMS</a> - The object set we  use for GERMS data collection consists of 136 stuffed toys of different  microorganisms. The toys are divided into 7 smaller categories, formed  by semantic division of the toy microbes. The motivation for dividing  the objects into smaller categories is to provide benchmarks with  different degrees of difficulty. (Malmir M, Sikka K, Forster D, Movellan JR, Cottrell G.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dmery.ing.puc.cl/index.php/material/gdxray/">GDXray:X-ray images for X-ray testing and Computer Vision</a> - GDXray includes five groups of images: Castings,  Welds*,Baggages,  Nature and Settings. (Domingo Mery, Catholic University of Chile)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cs.gmu.edu/~robot/gmu-kitchens.html">GMU Kitchens Dataset</a> - instance level annotation of 11 common household products from  BigBird dataset across 9 different kitchens (George Mason University)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.labri.fr/projet/AIV/dossierSiteRoBioVis/GraspingInTheWildV2.htm">Grasping In The Wild</a> - Egocentric video dataset of natural everyday life objects. 16 objects in 7 kitchens. (Benois-Pineau, Larrousse, de Rugy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.emt.tugraz.at/~pinz/data/GRAZ_02/">GRAZ-02 Database (Bikes, cars, people)</a> (A. Pinz) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://downloads.greyc.fr/Greyc3DColoredMeshDatabase/">GREYC 3D</a> - The GREYC 3D Colored mesh database is a set of 15 real objects with  different colors, geometries and textures that were acquired using a 3D  color laser scanner. (Anass Nouri, Christophe Charrier, Olivier Lezoray) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://benchmark.ini.rub.de/?section=gtsdb&subsection=dataset">GTSDB: German Traffic Sign Detection Benchmark</a> and <a target="_blank" rel="noopener" href="http://benchmark.ini.rub.de/?section=gtsrb&subsection=news">GTSRB: German Traffic Sign Recognition Benchmark</a> (Ruhr-Universitat Bochum) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://robotology.github.io/iCubWorld/">ICubWorld</a> -  iCubWorld datasets are collections of images acquired by recording from  the cameras of the iCub humanoid robot while it observes daily objects.  (Giulia Pasquale, Carlo Ciliberto, Giorgio Metta, Lorenzo Natale,  Francesca Odone and Lorenzo Rosasco.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mvtec.com/company/research/datasets/mvtec-itodd/">Industrial 3D Object Detection Dataset (MVTec ITODD)</a> - depth and gray value data of 28 objects in 3500 labeled scenes for 3D object detection and pose estimation with a strong focus on industrial  settings and applications (MVTec Software GmbH, Munich) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html">Instagram Food Dataset</a> - A database of 800,000 food images and associated metadata posted to  Instagram over 6 week period. Supports food type recognition and social  network analysis. (T. Hospedales. Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://3dinterpreter.csail.mit.edu/">Keypoint-5 dataset</a> - a dataset of five kinds of furniture with their 2D keypoint labels  (Jiajun Wu, Tianfan Xue, Joseph Lim, Yuandong Tian, Josh Tenenbaum,  Antonio Torralba, Bill Freeman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">KTH-3D-TOTAL</a> - RGB-D Data with objects on desktops annotated. 20 Desks, 3 times per  day, over 19 days. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.gel.ulaval.ca/~jflalonde/projects/6dofObjectTracking/index.html">Laval 6 DOF Object Tracking Dataset</a> - A Dataset of 297 RGB-D sequences with 11 objects for 6 DOF object  Tracking. (Mathieu Garon, Denis Laurendeau, Jean-Francois Lalonde)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrr.ucsd.edu/vivachallenge/index.php/traffic-light/traffic-light-detection/">LISA Traffic Light Dataset</a> - 6 light classes in various lighting conditions (Jensen, Philipsen, Mogelmose, Moeslund, and Trivedi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html">LISA Traffic Sign Dataset</a> -  video of 47 US sign types with 7855 annotations on 6610 frames (Mogelmose, Trivedi, and Moeslund) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/research/objrec/posedb/">Linkoping 3D Object Pose Estimation Database</a> (Fredrik Viksten and Per-Erik Forssen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/">Linkoping Traffic Signs Dataset</a> - 3488 traffic signs in 20K images (Larsson and Felsberg) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Longterm Labeled</a> - This dataset contains a subset of the observations from the longterm  dataset (longterm dataset above). (John Folkesson et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/arubior/main-product-dataset">Main Product Detection Dataset</a> - Contains textual metadata of fashion products and their images with  bounding boxes of the main product (the one referred by the text). (A.  Rubio, L. Yu, E. Simo-Serra and F. Moreno-Noguer) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/bircatmcri/MCIndoor20000">MCIndoor20000</a> - 20,000 digital images from three different indoor object categories:  doors, stairs, and hospital signs. (Bashiri, LaRose, Peissig, and Tafti) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.labri.fr/projet/AIV/MexCulture142.php">Mexculture142</a> - Mexican Cultural heritage objects and eye-tracker gaze fixations  (Montoya Obeso, Benois-Pineau, Garcia-Vazquez, Ramirez Acosta) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/CarData.html">MIT CBCL Car Data</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/streetscenes/">MIT CBCL StreetScenes Challenge Framework:</a> (Stan Bileschi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mscoco.org/">Microsoft COCO</a> - Common Objects in Context (Tsung-Yi Lin et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/projects/objectclassrecognition/default.aspx">Microsoft Object Class Recognition image databases</a> (Antonio Criminisi, Pushmeet Kohli, Tom Minka, Carsten Rother, Toby Sharp, Jamie Shotton, John Winn) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/people/jiansun/SalientObject/salient_object.htm">Microsoft salient object databases (labeled by bounding boxes)</a> (Liu, Sun Zheng, Tang, Shum) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Moving Labled</a> - This dataset extends the longterm datatset with more locations within the same office environment at KTH. (John Folkesson et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dl.allaboutbirds.org/nabirds">NABirds Dataset</a> -  70,000 annotated photographs of the 400 species of birds commonly  observed in North America (Grant Van Horn) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ml.nec-labs.com/download/data/videoembed/">NEC Toy animal object recognition or categorization database</a> (Hossein Mobahi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/">NORB 50 toy image database</a> (NYU) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/jingjingmengsite/research/ntu-voi">NTU-VOI: NTU Video Object Instance Dataset</a> - video clips with frame-level bounding box annotations of object  instances for evaluating object instance search and localization in  large scale videos. (Jingjing Meng, et. al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/objrec/posedb/">Object Pose Estimation Database</a> - This database contains 16 objects, each sampled at 5 degrees angle  increments along two rotational axes (F. Viksten etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Object Recognition Database</a>This database features modeling shots of eight objects and 51 cluttered test shots containing multiple objects. (Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/brendenlake/omniglot/">Omniglot</a> - 1623 different handwritten characters from 50 different alphabets (Lake, Salakhutdinov, Tenenbaum) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://storage.googleapis.com/openimages/web/index.html">Open Images Dataset V4</a>15,440,132 boxes on 600 categories, 30,113,078 image-level labels on 19,794  categories. (Ferrari, Duerig, Gomes) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://users.cecs.anu.edu.au/~koniusz/openmic-dataset/index.php">Open Museum Identification Challenge (Open MIC)</a>Open MIC contains photos of exhibits captured in 10 distinct exhibition  spaces (painting, sculptures, jewellery, etc.) of several museums and  the protocols for the domain adaptation and few-shot learning problems.  (P. Koniusz, Y. Tas, H. Zhang, M. Harandi, F. Porikli, R. Zhang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ikw.uos.de/~cv/projects/3Dcubes">Osnabrück Synthetic Scalable Cube Dataset</a> - 830000 different cubes captured from 12 different viewpoints for ANN  training (Schöning, Behrens, Faion, Kheiri, Heidemann &amp; Krumnack)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://modelnet.cs.princeton.edu/">Princeton ModelNet</a> - 127,915 CAD Models, 662 Object Categories, 10 Categories with Annotated Orientation (Wu, Song, Khosla, Yu, Zhang, Tang, Xiao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.pacman-project.eu/datasets/">PacMan datasets</a> - RGB and 3D synthetic and real data for graspable cookware and crockery (Jeremy Wyatt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~dl307/project_iccv2017">PACS (Photo Art Cartoon Sketch)</a> - An object category recognition dataset dataset for testing domain  generalisation: How well can a classifier trained on object images in  one domain recognise objects in another domain? (Da Li QMUL, T.  Hospedales. Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2007 Challange Image Database (motorbikes, cars, cows)</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2008 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2009 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2010 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2011 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL 2012 Challange Image Database</a> Category classification, detection, and segmentation, and still-image  action classification (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL Image Database (motorbikes, cars, cows)</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html">PASCAL Parts dataset</a> - PASCAL VOC with segmentation annotation for semantic parts of objects (Alan Yuille) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.stanford.edu/~roozbeh/pascal-context/">PASCAL-Context dataset</a> - annotations for 400+ additional categories (Alan Yuille) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvgl.stanford.edu/projects/pascal3d.html">PASCAL 3D/Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild</a> - 12 class, 3000+ images each with 3D annotations (Yu Xiang, Roozbeh Mottaghi, Silvio Savarese) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://phys101.csail.mit.edu/">Physics 101 dataset</a> - a  video dataset of 101 objects in five different scenarios (Jiajun Wu,  Joseph Lim, Hongyi Zhang, Josh Tenenbaum, Bill Freeman) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.eng.au.dk/plant-seedlings-dataset/">Plant seedlings dataset</a> - High-resolution images of 12 weed species.  (Aarhus University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://collections.durham.ac.uk/catalog?utf8=%E2%9C%93&q=breckon">Raindrop Detection</a> - Improved Raindrop Detection using Combined Shape and Saliency  Descriptors with Scene Context Isolation - Evaluation Dataset (Breckon,  Toby P., Webster, Dereck D.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.virginia.edu/~vicente/referit/">ReferIt Dataset (IAPRTC-12 and MS-COCO)</a> -  referring expressions for objects in images from the IAPRTC-12 and  MS-COCO datasets (Kazemzadeh, Matten, Ordonez, and Berg) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://sailvos.web.illinois.edu/">SAIL-VOS</a> - The  Semantic Amodal Instance Level Video Object Segmentation (SAIL-VOS)  dataset provides accurate ground truth annotations to develop methods  for reasoning about occluded parts of objects while enabling to take  temporal information into account (Hu, Chen, Hui, Huang, Schwing)  [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.lmars.whu.edu.cn/prof_web/shaozhenfeng/datasets/SeaShips(7000).zip">SeaShips</a> - 31455 side images of boats near land, from 7 classes, extracted from  surveillance video (Shao, Wu, Wang, Du, Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.shapenet.org/">ShapeNet</a> - 3D models of 55  common object categories with about 51K unique 3D models. Also 12K  models over 270 categories. (Princeton, Stanford and TTIC) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.bicv.org/datasets/short-100/">SHORT-100 dataset</a> - 100 categories of products found on a typical shopping list. It aims  to benchmark the performance of algorithms for recognising hand-held  objects from either snapshots or videos acquired using hand-held or  wearable cameras. (Jose Rivera-Rubio, Saad Idrees, Anil A. Bharath)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://sor3d.vcl.iti.gr/">SOR3D</a> - The SOR3D dataset  consists of over 20k instances of human-object interactions, 14 object  types, and 13 object affordances. (pyridon Thermos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://kelvins.esa.int/satellite-pose-estimation-challenge/data/">Space Object Pose Estimation Challenge Dataset</a> - 12000  synthetic images for training, 2998  similar synthetic test  images, and 305 real images (Space Rendezvous Laboratory (SLAB))  [26/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://vision.stanford.edu/aditya86/ImageNetDogs/main.html">Stanford Dogs Dataset</a> - The Stanford Dogs dataset contains images of 120 breeds of dogs from  around the world. This dataset has been built using images and  annotation from ImageNet for the task of fine-grained image  categorization. (Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng  Yao, Li Fei-Fei, Stanford University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ufldl.stanford.edu/housenumbers/">SVHN: Street View House Numbers Dataset</a> - like MNIST, but an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real  world problem (recognizing digits and numbers in natural scene images).  (Netzer, Wang, Coates, Bissacco, Wu, Ng) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/">Swedish Leaf Dataset</a> - These images contains leaves from 15 treeclasses (Oskar J. O. S?derkvist) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cmp.felk.cvut.cz/t-less">T-LESS</a> - An RGB-D  dataset for 6D pose estimation of texture-less objects. (Tomas Hodan,  Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis Lourakis, Xenophon  Zabulis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.sysu-hcp.net/taobao-commodity-dataset/">Taobao Commodity Dataset</a> - TCD contains 800 commodity images (dresses, jeans, T-shirts, shoes  and hats) for image salient object detection from the shops on the  Taobao website. (Keze Wang, Keyang Shi, Liang Lin, Chenglong Li )  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Tencent/tencent-ml-images">TenCent open-source multi-label image database</a> - 17,609,752 training and 88,739 validation image URLs, which are  annotated with up to 11,166 categories (Wu, Chen, Fan, Zhang, Hou, Liu,  Zhang) [16/4/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yaoyao-liu/tiered-imagenet-tools">tieredImageNet dataset</a> - a larger subset of ILSVRC-12 with 608 classes (779,165 images)  grouped into 34 higher-level nodes in the ImageNet human-curated  hierarchy. (Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,  Larochelle, Zemel) [17/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/pauloabelha/ToolArtec">ToolArtec point clouds</a> - 50 kitchen tool 3D scans (ply) from an Artec EVA scanner. See also <a target="_blank" rel="noopener" href="https://github.com/pauloabelha/ToolKinect">ToolKinect</a> - 13 scans using a Kinect 2 and <a target="_blank" rel="noopener" href="https://github.com/pauloabelha/ToolWeb">ToolWeb</a> - 116 point clouds of synthetic household tools with mass and  affordance groundtruth for 5 tasks. (Paulo Abelha) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php">TUW Object Instance Recognition Dataset</a> - Annotations of object instances and their 6DoF pose for cluttered  indoor scenes observed from various viewpoints and represented as Kinect RGB-D point clouds (Thomas, A. Aldoma, M. Zillich, M. Vincze) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">TUW dat sets</a> - Several RGB-D Ground truth and annotated data sets from TUW. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://agamenon.tsc.uah.es/Investigacion/gram/traffic_signs.html">UAH Traffic Signs Dataset</a> (Arroyo etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cogcomp.cs.illinois.edu/Data/Car/">UIUC Car Image Database</a> (UIUC) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.umich.edu/vision/data/3Ddataset.zip">UIUC Dataset of 3D object categories</a> (S. Savarese and L. Fei-Fei) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/bistaumanga/usps-dataset">USPS Handwritten Digits dataset</a> - 7291 train and 2007 test images. The images are 16*16 grayscale pixels (Hull) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vcipl-okstate.org/pbvs/bench/Data/12/VAIS.zip">VAIS</a> - VAIS contains simultaneously acquired unregistered thermal and  visible images of ships acquired from piers, and it was created to  faciliate autonomous ship development. (Mabel Zhang, Jean Choi, Michael  Wolf, Kostas Daniilidis, Christopher Kanan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dsi.unive.it/~rodola/data.html">Venezia 3D object-in-clutter recognition and segmentation</a> (Emanuele Rodola) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/s1151656/resources.html">Visual Attributes Dataset</a> visual attribute annotations for over 500 object classes (animate and  inanimate) which are all represented in ImageNet. Each object class is  annotated with visual attributes based on a taxonomy of 636 attributes  (e.g., has fur, made of metal, is round). [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Visual Hull Data Sets</a>a collection of visual hull datasets (Svetlana Lazebnik, Yasutaka Furukawa, and Jean Ponce) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://researchdata.sfu.ca/islandora/object/sfu:2724">VOC-360</a> - Dataset for object detection and segmentation in fisheye images (Fu, Bajic, and Vaughan) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ycbbenchmarks.com/">YCB Benchmarks – Object and Model Set</a> - 77 objects in 5 categories (food, kitchen, tool, shape, task) each  with 600 RGBD and high-res RGB images, calibration data, segmentation  masks, mesh models (Calli, Dollar, Singh, Walsman, Srinivasa, Abbeel)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube-bb/">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
</ol>
<h2 id="People-static-and-dynamic-human-body-pose"><a href="#People-static-and-dynamic-human-body-pose" class="headerlink" title="People (static and dynamic), human body pose"></a>People (static and dynamic), human body pose</h2><ol>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/open?id=14h8dGmx3-CTCpTIiF6yzyfVTMI3u71GB">3D articulated body</a> - 3D reconstruction of an articulated body with rotation and  translation. Single camera, varying focal. Every scene may have an  articulated body moving. There are four kinds of data sets included. A  sample reconstruction result included which uses only four images of the scene. (Prof Jihun Park) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://buff.is.tue.mpg.de/">BUFF dataset</a> - About 10K  scans of people in clothing and the estimated body shape of people  underneath. Scans contain texture so synthetic videos/images are easy to generate. (Zhang, Pujades, Black and Pons-Moll) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/VRU-intention/casr/blob/master/readme.txt">CASR: Cyclist Arm Sign Recognition</a> - Small clips of ~10 seconds showing cyclists performing arm signs. The videos are acquired with a consumer-graded  camera. There are  219 arm  sign  actions annotated. (Zhijie Fang, Antonio M. Lopez) [13/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://dyna.is.tue.mpg.de/">Dynamic Dyna</a> - More than 40K 4D 60fps high resolution scans and models of people very accurately  registered. Scans contain texture so synthetic videos/images are easy to generate. (Pons-Moll, Romero, Mahmood and Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dfaust.is.tue.mpg.de/">Dynamic Faust</a> - More than  40K 4D 60fps high resolution scans of people very accurately registered. Scans contain texture so synthetic videos/images are easy to generate.  (Bogo, Romero, Pons-Moll and Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://smpl-x.is.tue.mpg.de/">EHF dataset</a> - 100 curated frames (+ code) of one subject in minimal clothing performing various  expressive poses involving the body, hands and face. Each frame contains a full-body RGB image, detected 2D OpenPose features (body, hands,  face), a 3D scan of the subject, and a 3D SMPL-X mesh as pseudo  ground-truth (Pavlakos, Choutas, Ghorbani, Bolkart, Osman, Tzionas,  Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://files.is.tuebingen.mpg.de/classner/gp/">Extended Chictopia dataset</a> - 14K image Chictopia dataset with additional processed annotations  (face) and SMPL body model fits to the images. (Lassner, Pons-Moll and  Gehler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bensapp.github.io/flic-dataset.html">Frames Labeled In Cinema (FLIC)</a> - 20928 frames labeled with human pose (Sapp, Taskar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://wangzheallen.github.io/GPA">GPA: geometric pose affordance dataset</a> - Dataset of real 3D people interacting with real 3D scenes. 300k  static RGB frames of 13 subject in 8 scenes with ground-truth scene  meshes, and motion capture script focus on the interaction between  subject and scene geometry, human dynamics, and mimic of human action  with scene geometry around. (Wang, Chen, Rathore, Shin, Fowlkes)  [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/data/datasets/kids">KIDS dataset</a> - A collection of 30 high-resolution 3D shapes undergoing  nearly-isometric and non-isometric deformations, with point-to-point  ground truth as well as ground truth for left-to-right bilateral  symmetry. (Rodola, Rota Bulo, Windheuser, Vestner, Cremers) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.sysu-hcp.net/kinect2-human-pose-dataset-k2hpd/">Kinect2 Human Pose Dataset (K2HPD)</a> - Kinect2 Human Pose Dataset (K2HPD) includes about 100K depth images  with various human poses under challenging scenarios. (Keze Wang, Liang  Lin, Shengfu Zhai, Dengke Dong) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.comp.leeds.ac.uk/mat4saj/lsp.html">Leeds Sports Pose Dataset</a> - 2000 pose annotated images of mostly sports people (Johnson, Everingham) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://hcp.sysu.edu.cn/lip/">Look into Person Dataset</a> - 50,000 images with elaborated pixel-wise annotations with 19 semantic  human part labels and 2D hposes with 16 key points. (Gong, Liang, Zhang, Shen, Lin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.manga109.org/en/index.html">Manga109: manga (comic) dataset</a> - 109 volumes, more than 21,000 pages, 109 volumes, more than 21,000 pages (Kiyoharu Aizawa) [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.coe.neu.edu/Research/AClab/pose/colorManneNumbered.zip">Mannequin in-bed pose datasets via RGB webcam</a> - This in-bed pose dataset is collected via regular webcam in a  simulated hospital room at Northeastern University. (Shuangjun Liu and  Sarah Ostadabbas, ACLab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.coe.neu.edu/Research/AClab/pose/IRS_MANNE_rawN.zip">Mannequin IRS in-bed dataset</a> - This in-bed pose dataset is collected via our infrared selective  (IRS) system in a simulated hospital room at Northeastern University.  (Shuangjun Liu and Sarah Ostadabbas, ACLab) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/">MoPoTS-3D</a> - Multi-person 3D body pose benchmark for monocular RGB based methods,  with 20 sequences in indoor and outdoor settings (MPI For Informatics)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/3dhp-dataset/">MPI-INF-3DHP</a> - Single-person 3D body pose dataset and evaluation benchmark, with  extensive pose coverage across a broad set of activities, and extensive  scope of appearance augmentation. Multi-view RGB frames are available  for the training set, and monocular view frames for the test set. (MPI  For Informatics) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mano.is.tue.mpg.de/">MPI MANO &amp; SMPL+H dataset</a> - Models, 4D scans and registrations for the statistical models MANO  (hand-only) and SMPL+H (body+hands). For MANO there are ~2k static 3D  scans of 31 subjects performing up to 51 poses. For SMPL+H we include 39 4D sequences of 11 subjects. (Javier Romero, Dimitrios Tzionas and  Michael J Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://human-pose.mpi-inf.mpg.de/">MPII Human Pose Dataset</a> - 25K images containing over 40K people with annotated body joints,   410 human activities {Andriluka, Pishchulin, Gehler, Schiele) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://human-pose.mpi-inf.mpg.de/">MPII Human Pose Dataset</a> - MPII Human Pose dataset is a de-facto standard benchmark for  evaluation of articulated human pose estimation. (Mykhaylo Andriluka,  Leonid Pishchulin, Peter Gehler, Bernt Schiele) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/">MuCo-3DHP</a> - Large scale dataset of composited multi-person RGB images with 3D  pose annotations, generated from MPI-INF-3DHP dataset (MPI For  Informatics) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://camma.u-strasbg.fr/datasets">MVOR: A Multi-view Multi-person RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation</a> - multi-view images captured by 3 RGB-D cameras during real clinical interventions (Padoy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~nzhang/piper.html">People In Photo Albums</a> - Social media photo dataset with images from Flickr, and manual   annotations on person heads and their identities. (Ning Zhang and  Manohar Paluri and Yaniv Taigman and  Rob Fergus and Lubomir Bourdev)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/people-snapshot">People Snapshot Dataset</a> - Monocular video of 24 subjects rotating in front of a fixed camera.  Annotation in form of segmentation and 2D joint positions is provided.  (Alldieck, Magnor, Xu, Theobalt, Pons-Moll) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://goo.gl/DKuhlY">Person Recognition in Personal Photo Collections</a> - we introduced three harder splits for evaluation and long-term  attribute annotations and per-photo timestamp metadata. (Oh, Seong Joon  and Benenson, Rodrigo and Fritz, Mario and Schiele, Bernt) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-prima.inrialpes.fr/Pointing04/data-face.html">Pointing’04 ICPR Workshop Head Pose Image Database</a> [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cidis.espol.edu.ec/es/content/dataset-pose-estimation">Pose estimation</a> - This dataset has a total of 155,530 images. These images were  obtained through the recording of members of CIDIS, in 4 sessions. In  total, 10 videos with a duration of 4 minutes each were obtained. The  participants were asked to bring different clothes, in order to give  variety to the images. After this, the frames of the videos were  separated at a rate of 5 frames per second. All these images were  captured from a top view perspective. The original images have a  resolution of 1280x720 pixels. (CIDIS) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://prox.is.tue.mpg.de/">PROX dataset</a> - Dataset  (+code) of real 3D people interacting with real 3D scenes. “Quantitative PROX”: 180 static RGB-D frames of 1 subject in 1 scene with  ground-truth SMPL-X meshes. “Qualitative PROX”: 100K dynamic RGB-D  sequences of 20 subjects in 12 scenes with pseudo ground-truth SMPL-X  meshes. (Hassan, Choutas, Tzionas, Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/~laehner/shrec2016/">SHREC’16 Topological KIDS</a> - A collection of 40 high-resolution and low-resolution 3D shapes  undergoing nearly-isometric deformations in addition to strong  topological artifacts, self-contacts and mesh gluing, with  point-to-point ground truth. (Lahner, Rodola) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/~laehner/shrec2016/">SURREAL</a> - 60,000 synthetic videos of people under large variations in shape,  texture, view-point and pose. (Varol, Romero, Martin, Mahmood, Black,  Laptev, Schmid) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.tnt.uni-hannover.de/project/TNT15/">TNT 15 dataset</a> - Several sequences of video synchronised by 10 Inertial Sensors (IMU)  worn at the extremities. (von Marcard, Pons-Moll and Rosenhahn) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://up.is.tuebingen.mpg.de/">United People (UP) Dataset</a> - ˜8,000 images with keypoint and foreground segmentation annotations  as well as 3D body model fits. (Lassner, Romero, Kiefel, Bogo, Black,  Gehler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/data/pose/">VGG Human Pose Estimation datasets</a> including the BBC Pose  (20 videos with an overlaid sign language  interpreter), Extended BBC Pose (72 additional training videos), Short  BBC Pose (5 one hour videos with sign language signers), and ChaLearn  Pose (23 hours of Kinect data of 27 persons performing 20 Italian  gestures). (Charles, Everingham, Pfister, Magee, Hogg, Simonyan,  Zisserman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://fsukno.atspace.eu/Data.htm#VLRF">VRLF: Visual Lip Reading Feasibility</a> - audio-visual corpus of 24 speakers recorded in Spanish (Fernandez-Lopez, Martinez and Sukno) [Before 28/12/19]</li>
</ol>
<h2 id="People-Detection-and-Tracking-Databases"><a href="#People-Detection-and-Tracking-Databases" class="headerlink" title="People Detection and Tracking Databases"></a>People Detection and Tracking Databases</h2><ol>
<li><a target="_blank" rel="noopener" href="http://www.cvc.uab.es/DGaitDB/">3D KINECT Gender Walking data base</a> (L. Igual, A. Lapedriza, R. Borr&agrave;s from UB, CVC and UOC, Spain) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/aalborguniversity/trimodal-people-segmentation">AAU VAP Trimodal People Segmentation Dataset</a> - People detection and segmentation dataset captured with depth, RGB,  and thermal sensors (Palmero, Clapés, Bahnsen, Møgelmose, Moeslund,  Escalera) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://asankagp.github.io/aerialgaitdataset/">Aerial Gait Dataset</a> - people walking as viewed from an aerial (moving) platform (Perera, Law, Chahl) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.sites.univ-rennes2.fr/costel/corpetti/agoraset/Site/AGORASET.html">AGORASET: a dataset for crowd video analysis</a> (Nicolas Courty et al) [Before 28/12/19]</li>
<li>[CASIA gait database](<a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/english/Gait">http://www.cbsr.ia.ac.cn/english/Gait</a> Databases.asp) (Chinese Academy of Sciences) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/">CAVIAR project video sequences with tracking and behavior ground truth</a> (CAVIAR team/Edinburgh University - EC project IST-2001-37540) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://domedb.perception.cs.cmu.edu/">CMU Panoptic Studio Dataset</a> - Multiple people social interaction dataset captured by 500+  synchronized video cameras, with 3D full body skeletons and calibration  data. (H. Joo, T. Simon, Y. Sheikh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~jshao/CUHKcrowd_files/cuhk_crowd_dataset.htm">CUHK Crowd Dataset</a> - 474 video clips from 215 crowded scenes (Shao, Loy, and Wang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://docs.google.com/forms/d/1MF0gAXWKeO1hpsuHlSpPBS8D5JR-r-QOPtdUoFQJONo/viewform?formkey=dF9pZ1BFZkNiMG1oZUdtTjZPalR0MGc6MA">CUHK01 Dataset</a> : Person re-id dataset with 3, 884 images of 972 pedestrians (Rui Zhao et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://docs.google.com/forms/d/1lsUmbCvVllfz3zD54ws_I4ZFTkC71ysHBMGwtwKSukk/viewform?formkey=dHZtSGIwTnVDUEdWMFktQWU2bTZ0N3c6MA#gid=0">CUHK02 Dataset</a> : Person re-id dataset with five camera view settings. (Rui Zhao et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://docs.google.com/forms/d/1RZsVgPXCFCEVzmx4HNgC1yxCy5f7o8eTIbvZMhS4sLU/viewform?formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0">CUHK03 Dataset</a> : Person re-id dataset with 13,164 images of 1,360 pedestrians (Rui Zhao et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/">Caltech Pedestrian Dataset</a> (P. Dollar, C. Wojek, B. Schiele and P. Perona) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Pedestrian_Segmentatio/daimler_pedestrian_segmentatio.html">Daimler Pedestrian Detection Benchmark</a> 21790 images with 56492 pedestrians plus empty scenes. (D. M. Gavrila et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.i3a.uclm.es/louise/nais/fusiondatasets_EN.htm">Datasets (Color &amp; Infrared) for Fusion</a> A series of images in color and infrared captured from a parallel  two-camera setup under different environmental conditions. (Juan  Serrano-Cuerda, Antonio Fernandez-Caballero, Maria T. Lopez) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robesafe.com/personal/jnuevo/Datasets.html">Driver Monitoring Video Dataset</a> (RobeSafe + Jesus Nuevo-Chiquero) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC: Duke Multi-Target Multi-Camera tracking dataset</a> - 8 cameras, 85 min, 2m frames, 2000 people of video (Ergys Ristani,  Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo Tomasi) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/">Edinburgh overhead camera person tracking dataset</a> (Bob Fisher, Bashia Majecka, Gurkirt Singh, Rowland Sillito) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://gvvperfcapeva.mpi-inf.mpg.de/">GVVPerfcapEva</a> -  Repository of human shape and performance capture data, including full  body skeletal, hand tracking, body shape, face performance, interactions (Christian Theobalt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://jurie.users.greyc.fr/datasets/hat.html">HAT</a> Database of 27 human attributes (Gaurav Sharma, Frederic Jurie) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html">Immediacy Dataset</a> - This dataset is designed for estimation personal relationships. (Xiao Chu et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dressedhuman.gforge.inria.fr/">Inria Dressed human bodies in motion benchmark</a> - Benchmark containing 3D motion sequences of different subjects,  motions, and clothing styles that allows to quantitatively measure the  accuracy of body shape estimates. (Jinlong Yang, Jean-Sbastien Franco,  Franck H=E9troy-Wheeler, and Stefanie Wuhrer) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://pascal.inrialpes.fr/data/human/">INRIA Person Dataset</a> (Navneet Dalal) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.soic.indiana.edu/firstthird-eccv2018/">IU ShareView</a> - IU ShareView dataset consists of nine sets of synchronized (two  first-person) videos with a total of 1,227 pixel-level ground truth  segmentation maps of 2,654 annotated person instances. (Mingze Xu,  Chenyou Fan, Yuchen Wang, Michael S. Ryoo, David J. Crandall) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrg.iyte.edu.tr/datasets.htm">Izmir</a> -  omnidirectional and panoramic image dataset (with annotations) to be  used for human and car detection (Yalin Bastanlar) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/">Joint Attention in Autonomous Driving (JAAD)</a> -  The dataset includes instances of pedestrians and cars  intended  primarily for the purpose of behavioural studies and  detection in the  context of autonomous driving. (Iuliia Kotseruba, Amir Rasouli and John  K. Tsotsos) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://jtl.lassonde.yorku.ca/2017/05/person-following-cnn/">JTL Stereo Tacking Dataset for Person Following Robots</a> - 11 different indoor and outdoor places for the task of robots  following people under challenging situations (Chen, Sahdev, Tsotsos)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://soonminhwang.github.io/rgbt-ped-detection/">KAIST Multispectral Pedestrian Detection Benchmark</a> - 95k color-thermal pairs (640x480, 20Hz) images, with 103,128 dense  annotations and 1,182 unique pedestrians (Hwang, Park, Kim, Choi, Kweon) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mahnob-db.eu/mimicry/">MAHNOB: MHI-Mimicry database</a> - A 2 person, multiple camera and microphone database for studying  mimicry in human-human interaction scenarios. (Sun, Lichtenauer,  Valstar, Nijholt, and Pantic) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cbcl.mit.edu/software-datasets/PedestrianData.html">MIT CBCL Pedestrian Data</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dyna.is.tue.mpg.de/">MPI DYNA</a> - A Model of Dynamic Human Shape in Motion (Max Planck Tubingen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://faust.is.tue.mpg.de/">MPI FAUST Dataset</a> A data  set containing 300 real, high-resolution human scans, with automatically computed ground-truth correspondences (Max Planck Tubingen) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://jhmdb.is.tue.mpg.de/">MPI JHMDB dataset</a> -  Joint-annotated Human Motion Data Base - 21 actions, 928 clips, 33183  frames (Jhuang, Gall, Zuffi, Schmid and Black) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mosh.is.tue.mpg.de/">MPI MOSH</a> Motion and Shape  Capture from Markers. MOCAP data, 3D shape meshes, 3D high resolution  scans.  (Max Planck Tubingen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://community.wvu.edu/~samotiian/datasets.html">MVHAUS-PI</a> - a multi-view human interaction recognition dataset (Saeid et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.liangzheng.org/Project/project_reid.html">Market-1501 Dataset</a> - 32,668 annotated bounding boxes of 1,501 identities from up to 6 cameras (Liang Zheng et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://imagelab.ing.unimore.it/files/EGO-HPE.zip">Modena and Reggio Emilia first person head motion videos</a> (Univ of Modena and Reggio Emilia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.demcare.eu/results/datasets">Multimodal Activities of Daily Living</a> - including video, audio, physiological, sleep, motion and plug sensors. (Alexia Briasouli) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://motchallenge.net/">Multiple Object Tracking Benchmark</a> - A collection of datasets with ground truth, plus a performance league table (ETHZ, U. Adelaide, TU Darmstadt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://nyx.ethz.ch/">NYU Multiple Object Tracking Benchmark</a> (Konrad Schindler et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.forth.gr/cvrl/fbody/">Occluded Articulated Human Body Dataset</a> - Body pose extraction and tracking under occlusions, 6 RGB-D sequences in total (3500 frames) with one, two and three users, marker-based  ground truth data. (Markos Sigalas, Maria Pateraki, Panos Trahanias)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.oxuva.net/">OxUva</a> - A large-scale long-term  tracking dataset composed of 366 long videos of about 14 hours in total, with separate dev (public annotations) and test sets (hidden  annotations), featuring target object disappearance and continuous  attributes. (Jack Valmadre, Luca Bertinetto, Joao F. Henriques, Ran Tao, Andrea Vedaldi, Arnold Smeulders, Philip Torr, Efstratios Gavves)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.am.sanken.osaka-u.ac.jp/BiometricDB/index.html">OU-ISIR Gait Database</a> - six video-based gait data sets, two inertial sensor-based gait  datasets, and a gait-relevant biometric score data set. (Yasushi  Makihara) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://computing.ece.vt.edu/~santol/projects/zsl_via_visual_abstraction/parse/index.html">PARSE Dataset Additional Data</a> - facial expression, gaze direction, and gender (Antol, Zitnick, Parikh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.uci.edu/~dramanan/papers/parse/index.html">PARSE Dataset of Articulated Bodies</a> - 300 images of humans and horses (Ramanan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.vision.ee.ethz.ch/daid/MOT/pathtrack_release_v1.0.zip">PathTrack dataset: a large-scale MOT dataset</a> - PathTrack is a large scale multi-object tracking dataset of more than 15,000 person trajectories in 720 sequences. (Santiago Manen, Michael  Gygli, Dengxin Dai, Luc Van Gool) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-vpu.eps.uam.es/DS/PDbm/">PDbm: People Detection benchmark repository</a> - realistic sequences, manually annotated people detection ground truth and a complete evaluation framework (Garc??a-Mart??n, Mart??nez,  Besc??s) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-vpu.eps.uam.es/DS/PDds/">PDds: A Person Detection dataset</a> - several annotated surveillance sequences of different levels of  complexity (Garc??a-Mart??n, Mart??nez, Besc??s) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvg.reading.ac.uk/PETS2009/a.html">PETS 2009 Crowd Challange dataset</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvg.reading.ac.uk/PETS2009/a.html">PETS Winter 2009 workshop data</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvg.reading.ac.uk/PETS2015/a.html">PETS: 2015 Performance Evaluation of Tracking and Surveillance</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvg.reading.ac.uk/">PETS: 2015 Performance Evaluation of Tracking and Surveillance</a> (Reading University &amp; Luis Patino) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvg.reading.ac.uk/PETS2016/a.html">PETS 2016 datasets</a> - multi-camera (including thermal cameras) video recordings of human  behavior around a stationary vehicle and around a boat (Thomas Cane)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/piropodatabase/">PIROPO</a> - People in Indoor ROoms with Perspective and Omnidirectional cameras,  with more than 100,000 annotated frames (GTI-UPM, Spain) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/BathVisArtData/PeopleArt">People-Art</a> - a databased containing people labelled in photos and artwork (Qi Wu and Hongping Cai) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/BathVisArtData/PhotoArt50">Photo-Art-50</a> - a databased containing 50 object classes annoted in photos and artwork (Qi Wu and Hongping Cai) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.changedetection.net/">Pixel-based change detection benchmark dataset</a> (Goyette et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://drive.google.com/open?id=0BzU4ETbYHM6faEdhZ0hMNmtqUTA">Precarious Dataset</a> - unusual people detection dataset (Huang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/dasabir/RAiD_Dataset">RAiD</a> - Re-Identification Across Indoor-Outdoor Dataset: 43 people, 4 cameras, 6920 images (Abir Das et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/mengzhengrpi/mengzhengrpi.github.io/blob/master/index.md">RPIfield</a> - Person re-identification dataset containing 4108 person images with  timestamps. (Meng Zheng, Srikrishna Karanam, Richard J. Radke) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/dilipprasad/home/singapore-maritime-dataset">Singapore Maritime Dataset</a> - Visible range videos and Infrared videos. (Dilip K. Prasad) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://web.northeastern.edu/ostadabbas/2019/06/27/multimodal-in-bed-pose-estimation/">SLP (Simultaneously-collected multimodal Lying Pose)</a> -  large scale dataset on in-bed poses includes: 2 Data Collection  Settings: (a) Hospital setting: 7 participants, and (b) Home setting:  102 participants (29 females, age range: 20-40). 4 Imaging Modalities:  RGB (regular webcam), IR (FLIR LWIR camera), DEPTH (Kinect v2) and  Pressure Map (Tekscan Pressure Sensing Map). 3 Cover Conditions:  uncover, bed sheet, and blanket. Fully labeled poses with 14 joints.  (Ostadabbas and Liu) [2/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://www.synthia-dataset.net/">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/">Shinpuhkan 2014</a> - A Person Re-identification dataset containing 22,000 images of 24  people captured by 16 cameras. (Yasutomo Kawanishi et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvgl.stanford.edu/projects/groupdiscovery/">Stanford Structured Group Discovery dataset</a> - Discovering Groups of People in Images (W. Choi et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org">TrackingNet</a> - Large-scale dataset for tracking in the wild: more than 30k annotated sequences for training, more than 500 sequestered sequences for  testing, evaluation server and leaderboard for fair ranking. (Matthias  Muller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi and Bernard  Ghanem) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.idi.ntnu.no/grupper/vis/tbnd/">Transient Biometrics Nails Dataset V01</a> (Igor Barros Barbosa) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.dabi.temple.edu/~hbling/data/TColor-128/TColor-128.html">Temple Color 128 - Color Tracking Benchmark</a> - Encoding Color Information for Visual Tracking (P. Liang, E. Blasch, H. Ling) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.mmk.ei.tum.de/verschiedenes/tum-gaid-database/">TUM Gait from Audio, Image and Depth (GAID) database</a> - containing tracked RGB video, tracked depth video, and audio for 305  subjects (Babaee, Hofmann, Geiger,  Bachmann,  Schuller,  Rigoll)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vrai.dii.univpm.it/re-id-dataset">TVPR (Top View Person Re-identification) dataset</a> - person re-identification using an RGB-D camera in a Top-View  configuration: indoor 23 sessions, 100 people, 8 days (Liciotti,  Paolanti, Frontoni, Mancini and Zingaretti) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html">UCLA Aerial Event Dataset</a> - Human activities in aerial videos with annotations of people,  objects, social groups, activities and roles (Shu, Xie, Rothrock,  Todorovic, and Zhu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/projects/crowd.php">Univ of Central Florida - Crowd Dataset</a> (Saad Ali) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.ucf.edu/~sali/Projects/CrowdSegmentation/index.html">Univ of Central Florida - Crowd Flow Segmentation datasets</a> (Saad Ali) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.soe.ucsc.edu/node/178">VIPeR: Viewpoint Invariant Pedestrian Recognition</a> - 632 pedestrian image pairs taken from arbitrary viewpoints under  varying illumination conditions. (Gray, Brennan, and Tao) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.votchallenge.net/">Visual object tracking challenge datasets</a> - The VOT datasets is a collection of fully annotated visual object  tracking datasets used in the single-target short-term visual object  tracking challenges. (The VOT committee) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-vpu.eps.uam.es/DS/WUds/">WUds: Wheelchair Users Dataset</a> - wheelchair users detection data, to extend people detection,  providing a more general solution to detect people in environments such  as independent and assisted living, hospitals, healthcare centers and  senior residences (Mart??n-Nieto, Garc??a-Mart??n, Mart??nez) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xR-EgoPose">xR-EgoPose</a> - Photorealistic synthetic dataset for 3D human pose estimation from an ego-centric perspective (Tome, Peluse, Agapito and Badino) [4/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://research.google.com/youtube-bb/">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
</ol>
<h2 id="Remote-Sensing"><a href="#Remote-Sensing" class="headerlink" title="Remote Sensing"></a>Remote Sensing</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.airs-dataset.com/">Aerial Imagery for Roof Segmentation (AIRS)</a> - 457 km2 coverage of orthorectified aerial images with over 220,000  buildings for roof segmentation. (Lei Wang, Qi Chen) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.patreo.dcc.ufmg.br/downloads/brazilian-cerrado-savanna-dataset/">Brazilian Cerrado-Savanna Scenes Dataset</a> - Composition of IR-R-G scenes taken by RapidEye sensor for vegetation  classification in Brazilian Cerrado-Savanna. (K. Nogueira, J. A. dos  Santos, T. Fornazari, T. S. Freire, L. P. Morellato, R. da S. Torres)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.patreo.dcc.ufmg.br/downloads/brazilian-coffee-dataset/">Brazilian Coffee Scenes Dataset</a> - Composition of IR-R-G scenes taken by SPOT sensor for identification  of coffee crops in Brazilian mountains. (O. A. B. Penatti, K. Nogueira,  J. A. dos Santos.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://biz.nevsehir.edu.tr/ozgunok/en/duyurular">Building Detection Benchmark</a> -14 images acquired from IKONOS (1 m) and QuickBird (60 cm)(Ali Ozgun Ok and Caglar Senaras) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://wwwp.fc.unesp.br/~papa/recogna/remote_sensing.html">CBERS-2B, Landsat 5 TM, Geoeye, Ikonos-2 MS and ALOS-PALSAR</a> - land-cover classification using optical images(D. Osaku et al. ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/">Data Fusion Contest 2015 (Zeebruges)</a> - This dataset provides a RGB aerial dataset (5cm) and a Lidar point  cloud (65pts/m2) over the harbor of the city of Zeebruges (Belgium). It  also provided a DSM derived from the point cloud and a semantic  segmentation ground truth of five of the seven 10000 x 10000 pixels  tiles. An evaluation server is used to evaluate the results on the two  other tiles. (Image analysis and Data Fusion Technical Committee, IEEE  Geoscience, Remote Sensing Society) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2017-ieee-grss-data-fusion-contest-2/">Data Fusion Contest 2017</a> - This dataset provides satellite (Landsat, Sentinel 2) and vector GIS  layers (e.g. buildings and road footprint) for nine cities worldwide.  The task is to predict land use classes useful for climate models at a  100m prediction grid, given data of different resolution and types of  features. 5 cities come with labels, 4 others are kept hidden for  scoring on an evaluation server. (Image analysis and Data Fusion  Technical Committee, IEEE Geoscience, Remote Sensing Society) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://deepglobe.org/">deepGlobe challenge</a> - This  datasets comprises three challenges, road extraction, buildings  detection and semantic segmentation of land cover. A series of satellite images from Digital Globe (RGB, 50 cm resolution) and labels over  several countries worldwide are provided. The results were presented at  the DeepGlobe workshop at CVPR 2018. (Facebook, Digital Globe) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://deepglobe.org/">DeepGlobe Satellite Image Understanding Challenge</a> - Datasets and evaluation platforms for three deep learning tasks on  satellite images: road extraction, building detection, and land type  classification. (Demir, Ilke and Koperski, Krzysztof and Lindenbaum,  David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://captain-whu.github.io/DOTA">DOTA</a> - 2806 large  aerial images with 188,282 over 15 categories (Xia, Bai, Ding, Zhu,  Belongie, Luo, Datcu, Pelillo, Zhang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://v-sense.scss.tcd.ie/dublincity/">DublinCity: Annotated LiDAR Point Cloud and its Applications</a> - Annotated (13 labels) aerial lidar scan of central Dublin (Zolanvari, Ruano, Rana, Cummins, da Silva, Rahbar, Smolic) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ics.forth.gr/cvrl/msi">FORTH Multispectral Imaging (MSI) datasets</a> - 5 datasets for Multispectral Imaging (MSI), annotated with ground truth data (Polykarpos Karamaoynas) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://wwwp.fc.unesp.br/~papa/recogna/remote_sensing.html">Furnas and Tiete</a> - sediment yield classification( Pisani et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.escience.cn/people/liuzikun/DataSet.html">HSRC</a> - High Resolution Optical Satellite Image Dataset for Ship Recognition. 1061 ships images over 3 subclass levels (Liu, Yuan, Weng, Yang)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html">ISPRS 2D semantic labeling</a> - Height models and true ortho-images with a ground sampling distance  of 5cm have been prepared over the city of Potsdam/Germany (Franz  Rottensteiner, Gunho Sohn, Markus Gerke, Jan D. Wegner) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www2.isprs.org/commissions/comm3/wg4/3d-semantic-labeling.html">ISPRS 3D semantic labeling</a> - nine class airborne laser scanning data (Franz Rottensteiner, Gunho Sohn, Markus Gerke, Jan D. Wegner) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://project.inria.fr/aerialimagelabeling/">Inria Aerial Image Labeling Dataset</a> -  9000 square kilometeres of color aerial imagery over U.S. and  Austrian cities. (Emmanuel Maggiori, Yuliya Tarabalka, Guillaume  Charpiat, Pierre Alliez.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/tomalampert/data-sets">Lampert’s Spectrogram Analysis</a> - Passive sonar spectrogram images derived from time-series  data,??these spectrograms are generated from recordings of acoustic  energy radiated from propeller and engine machinery in underwater sea  recordings. (Thomas Lampert) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvl.isy.liu.se/en/research/datasets/ltir/">Linkoping Thermal InfraRed dataset</a> - The LTIR dataset is a thermal infrared dataset for evaluation of  Short-Term Single-Object (STSO) tracking (Linkoping University) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iuii.ua.es/datasets/masati/index.html">MASATI: MAritime SATellite Imagery dataset</a> - MASATI is a dataset composed of optical aerial imagery with 6212  samples which were obtained from Microsoft Bing Maps. They were labeled  and classified into 7 classes of maritime scenes: land, coast, sea,  coast-ship, sea-ship, sea with multi-ship, sea-ship in detail.  (University of Alicante) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/GatorSense/MUUFLGulfport">MUUFL Gulfport Hyperspectral and LiDAR data set</a> - Co-registered aerial hyperspectral and lidar data over the University of Southern Mississippi Gulfpark campus containing several sub-pixel  targets. (Gader, Zare, Close, Aitken, Tuell) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.escience.cn/people/gongcheng/NWPU-RESISC45.html">NWPU-RESISC45</a> - A large-scale benchmark dataset used for remote sensing image scene  classification containing 31500 images covered by 45 scene classes.  (Cheng, Han, Lu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html">NWPU VHR-10 dataset</a> - 800 high resolution satellite images of 10 classes (airplane, ship,  storage tank, baseballdiamond, tennis court, basketball court, ground  track field, harbor, bridge, and vehicle) (Cheng, Han, Zhou, Guo)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rmkemker/RIT-18">RIT-18</a> - a  high-resolution multispectral dataset for semantic segmentation. (Ronald Kemker, Carl Salvaggio, Christopher Kanan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ieee-dataport.org/documents/sar-ship-dataset-detection-discrimination-and-analysis">SAR SHIP DATASET</a> - 43 Synthetic Aperture Radar images (Schwegmann, Kleynhans, Salmon, Mdakane, Meyer) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://dronedataset.icg.tugraz.at/">Semantic Drone Dataset</a> - 20 houses from nadir (bird’s eye) view acquired at 5 to 30 meters  above ground. 400 public and 200 private high resolution images of  6000x4000px (24Mpx).  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">UC Merced Land Use Dataset</a> 21 class land use image dataset with 100 images per class, largely  urban, 256x256 resolution, 1 foot pixels (Yang and Newsam) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/data/Cross-View/">UCF-CrossView Dataset: Cross-View Image Matching for Geo-localization in Urban Environments</a> - A new dataset of street view and bird’s eye view images for  cross-view image geo-localization. (Center for Research in Computer  Vision, University of Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/michelevolpiresearch/data/zurich-dataset">Zurich Summer dataset</a> - t is intended for semantic segmentation of very high resolution  satellite images of urban scenes, with incomplete ground truth (Michele  Volpi and Vitto Ferrari.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/zurichmavdataset.html">Zurich Urban Micro Aerial Vehicle Dataset</a> - time synchronized aerial high-resolution images of 2 km of Zurich,  with associated other data (Majdik, Till, Scaramuzza) [Before 28/12/19]</li>
</ol>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><ol>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/UTENSILS/">Edinburgh Kitchen Utensil Database</a> - 897 raw and binary images of 20 categories of kitchen utensil, a  resource for training future domestic assistance robots (D. Fullerton,  A. Goel, R. B. Fisher) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/davis_data.html">Event-Camera Dataset</a> - This presents the world’s first collection of datasets with an  event-based camera for high-speed robotics (E. Mueggler, H. Rebecq, G.  Gallego, T. Delbruck, D. Scaramuzza) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://collections.durham.ac.uk/catalog?utf8=%E2%9C%93&q=breckon">Improved 3D Sparse Maps for High-performance Structure from Motion with Low-cost Omnidirectional Robots - Evaluation Dataset</a> - Data set used in research paper doi:10.1109/ICIP.2015.7351744 (Breckon, Toby P., Cavestany, Pedro) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.raghavendersahdev.com/place-recognition.html">Indoor Place Recognition Dataset for localization of Mobile Robots</a> - The dataset contains 17 different places built  from 2 different  robots (virtualMe and pioneer) (Raghavender Sahdev, John K. Tsotsos.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://jtl.lassonde.yorku.ca/2017/05/person-following-cnn/">JTL Stereo Tacking Dataset for Person Following Robots</a> - 11 different indoor and outdoor places for the task of robots  following people under challenging situations (Chen, Sahdev, Tsotsos)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Meta rooms</a> - RGB-D data comprised of 28 aligned depth camera images collected by  having robot go to specific place and do 360 degrees of pan with various tilts. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://hijeffery.github.io/PanoNavi/">PanoNavi dataset</a> - A panoramic dataset for robot navigation, consisted of 5 videos lasting about 1 hour. (Lingyan Ran) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://kos.informatik.uni-osnabrueck.de/3Dscans/">Robotic 3D Scan Repository</a> - 3D point clouds from robotic experiments of scenes (Osnabruck and Jacobs Universities) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.nal.usda.gov/dataset/data-solving-robot-world-hand-eyes-calibration-problem-iterative-methods"> Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods</a> - These datasets were generated for calibrating robot-camera systems. (Amy Tabb) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.rovit.ua.es/dataset/vidrilo/">ViDRILO</a> -  ViDRILO is a dataset containing 5 sequences of annotated RGB-D images  acquired with a mobile robot in two office buildings under challenging  lighting conditions. (Miguel Cazorla, J. Martinez-Gomez, M. Cazorla, I.  Garcia-Varea and V. Morell.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cas.kth.se/data/strands/data.html">Witham Wharf</a> - For RGB-D of eight locations collect by robot every 10 min over ~10  days by the University of Lincoln. (John Folkesson et al.) [Before  28/12/19]</li>
</ol>
<h2 id="Scenes-or-Places-Scene-Segmentation-or-Classification"><a href="#Scenes-or-Places-Scene-Segmentation-or-Classification" class="headerlink" title="Scenes or Places, Scene Segmentation or Classification"></a>Scenes or Places, Scene Segmentation or Classification</h2><ol>
<li><a target="_blank" rel="noopener" href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2017">3DRMS Challenge Dataset 2017</a> - real garden stereo image pairs with camera poses and semantic  annotation captured by a small mobile robot (TrimBot2020 consortium)  [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2018">3DRMS Challenge Dataset 2018</a> - synthetic garden stereo image pairs with depths, camera poses and semantic annotation (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/">Barcelona</a> - 15,150 images, urban views of Barcelona (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://diml.yonsei.ac.kr/~srkim/MIIC/">Cross-modal Landmark Identification Benchmark</a> - Dandmark-identification benchmark taken under varying weather  conditions, which consists of 17 landmark images taken under several  weather conditions, e.g., sunny, cloudy, snowy, and sunset. (Yonsei  University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://3dvis.ri.cmu.edu/data-sets/localization/">CMU Visual Localization Data Set</a> - Dataset collected over the period of a year using the Navlab 11 equipped with IMU, GPS, INS, Lidars and cameras. (Hernan Badino, Daniel Huber and Takeo Kanade) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.csc.kth.se/~pronobis/research/ullah07cold/">COLD (COsy Localization Database) - place localization</a> (Ullah, Pronobis, Caputo, Luo, and Jensfelt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://davischallenge.org/">DAVIS: Video Object Segmentation dataset 2016</a> - A Benchmark Dataset and Evaluation Methodology for Video Object  Segmentation (F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.  Gross, and A. Sorkine-Hornung) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://davischallenge.org/">DAVIS: Video Object Segmentation dataset 2017</a> -  The 2017 DAVIS Challenge on Video Object Segmentation (J.  Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/">EDUB-Seg</a>- Egocentric dataset for event segmentation. (Mariella Dimiccoli, Marc  Bolaños, Estefania Talavera, Maedeh Aghaei, Stavri G. Nikolov, and Petia Radeva.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cvjena/eu-flood-dataset">European Flood 2013</a> - 3,710 images of a flood event in central Europe, annotated with  relevance regarding 3 image retrieval tasks (multi-label) and important  image regions. (Friedrich Schiller University Jena, Deutsches  GeoForschungsZentrum Potsdam) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.eng.au.dk/fieldsafe/">Fieldsafe</a> - A multi-modal dataset for obstacle detection in agriculture.  (Aarhus University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture">Fifteen Scene Categories</a> - A dataset of fifteen natural scene categories. (Fei-Fei Li and Aude Oliva) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://figrim.mit.edu/">FIGRIM (Fine Grained Image Memorability Dataset)</a> - A subset of images from the SUN database used for human memory  experiments, and provided along with memorability scores. (Bylinskii,  Isola, Bainbridge, Torralba, Oliva) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dhoiem.cs.illinois.edu/projects/context/">Geometric Context - scene interpretation images</a> (Derek Hoiem) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/cvdfoundation/google-landmark">GLDv2: Google Landmarks Dataset v2</a> - 4,132,914 training images, 761,757 index images, and 117,577 test  images annotated with labels representing human-made and natural  landmarks (Weyand, Araujo, Cao, Sim) [16/4/20]</li>
<li><a target="_blank" rel="noopener" href="https://wp.uni-koblenz.de/hyko/">HyKo: A Spectral Dataset for Scene Understanding</a> - The HyKo dataset was captured with compact, low-cost, snapshot mosaic (SSM) imaging cameras, which are able to capture a whole spectral cube  in one shot recorded from a moving vehicle enabling hyperspectral scene  analysis for road scene understanding. (Active Vision Group, University  of Koblenz-Landau) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/visipedia/inat_comp">iNaturalist Species Classification and Detection Dataset</a> - The iNaturalist 2017 species classification and detection dataset has been collected and annotated by citizen scientists and contains 859,000 images from over 5,000 different species of plants and animals.  (Caltech) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.raghavendersahdev.com/place-recognition.html">Indoor Place Recognition Dataset for localization of Mobile Robots</a> - The dataset contains 17 different places built  from 2 different  robots (virtualMe and pioneer) (Raghavender Sahdev, John K. Tsotsos.)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://web.mit.edu/torralba/www/indoor.html">Indoor Scene Recognition</a> - 67 Indoor categories, 15620 images (Quattoni and Torralba) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/">Intrinsic Images in the Wild (IIW)</a> -  Intrinsic Images in the Wild, is a large-scale, public dataset for  evaluating intrinsic image decompositions of indoor scenes (Sean Bell,  Kavita Bala, Noah Snavely) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/HKBU-HPML/IRS">IRS: Large Synthetic Indoor Robotics Stereo Dataset</a> - 103,316 samples covering a wide range of indoor scenes, such as home, office, store and restaurant (Wang, Zheng, Yan, Deng, Zhao, Chu)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/">LM+SUN</a> - 45,676 images, mainly urban or human related scenes (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://europe.naverlabs.com/blog/making-maps-evergreen/">Mallscape dataset</a> - a collection of 33K localized and time-stamped images captured in two large shopping malls during two different sessions temporally separated by several months, enabling to evaluate Point-of-Interests (POI) change detection methods in realistic conditions (Revaud, Sampaio De Rezende,  Heo, You, Jeong) [2/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://vcipl-okstate.org/pbvs/bench/Data/12/VAIS.zip">Maritime Imagery in the Visible and Infrared Spectrums</a> - VAIS contains simultaneously acquired unregistered thermal and  visible images of ships acquired from piers (Zhang, Choi, Daniilidis,  Wolf,  &amp; Kanan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.iuii.ua.es/datasets/masati/index.html">MASATI: MAritime SATellite Imagery dataset</a> - MASATI is a dataset composed of optical aerial imagery with 6212  samples which were obtained from Microsoft Bing Maps. They were labeled  and classified into 7 classes of maritime scenes: land, coast, sea,  coast-ship, sea-ship, sea with multi-ship, sea-ship in detail.  (University of Alicante) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/minc/">Materials in Context (MINC)</a> - The Materials in Context Database (MINC) builds on OpenSurfaces, but  includes millions of point annotations of material labels. (Sean Bell,  Paul Upchurch, Noah Snavely, Kavita Bala) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~rgrosse/intrinsic/">MIT Intrinsic Images</a> - 20 objects (Roger Grosse, Micah K. Johnson, Edward H. Adelson, and William T. Freeman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://people.csail.mit.edu/jstraub/_pages/nyu-mmf-dataset/index.html">NYU V2 Mixture of Manhattan Frames Dataset</a> - We provide the Mixture of Manhattan Frames (MMF) segmentation and MF  rotations on the full NYU depth dataset V2 by Silberman et al. (Straub,  Julian and Rosman, Guy and Freifeld, Oren and Leonard, John J. and  Fisher III, John W.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/opensurfaces/">OpenSurfaces</a> - OpenSurfaces consists of tens of thousands of examples of surfaces  segmented from consumer photographs of interiors, and annotated with  material parameters, texture information, and contextual information .  (Kavita Bala et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~aarnab/bmvc_2015.html">Oxford Audiovisual Segmentation Dataset</a> - Oxford Audiovisual Segmentation Dataset with Oxford Audiovisual  Segmentation Dataset including audio recordings of objects being struck  (Arnab, Sapienza, Golodetz, Miksik and Torr) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/drivableregion/">Thermal Road Dataset</a> - Our thermal-road dataset provides around 6000 thermal-infrared images captured in the road scene with manually annotated ground-truth. (3500: general road, 1500: complicated road, 1000: off-road). (Jae Shin Yoon)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://places2.csail.mit.edu/">Places 2 Scene Recognition database</a> -365 scene categories and 8 millions of images (Zhou, Khosla, Lapedriza, Torralba and Oliva) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://places.csail.mit.edu/">Places Scene Recognition database</a> - 205 scene categories and 2.5 millions of images (Zhou, Lapedriza, Xiao, Torralba, and Oliva) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ivrl.epfl.ch/supplementary_material/cvpr11/">RGB-NIR Scene Dataset</a> - 477 images in 9 categories captured in RGB and Near-infrared (NIR) (Brown and Susstrunk) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2017">RMS2017 - Reconstruction Meets Semantics outdoor dataset</a> - 500 semantically annotated images with poses and point cloud from a real garden (Tylecek, Sattler) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2018">RMS2018 - Reconstruction Meets Semantics virtual dataset</a> - 30k semantically annotated images with poses and point cloud from 6 virtual gardens (Le, Tylecek) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://robotvault.bitbucket.io/scenenet-rgbd.html">SceneNet RGB-D</a> - 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth including RGB and depth (McCormac, Handa, Leutenegger, Davison)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://syns.soton.ac.uk/">Southampton-York Natural Scenes Dataset</a> 90 scenes, 25 indoor and outdoor scene categories, with spherical  LiDAR, HDR intensity, stereo intensity panorama. (Adams, Elder, Graf,  Leyland, Lugtigheid, Muryy) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.csail.mit.edu/vision/SUN/">SUN 2012</a> - 16,873 fully annotated scene images for scene categorization (Xiao et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.csail.mit.edu/vision/SUN/">SUN 397</a> - 397 scene categories for scene classification (Xiao et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rgbd.cs.princeton.edu/">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</a> - 10,000 RGB-D images, 146,617 2D polygons and 58,657 3D bounding boxes (Song, Lichtenberg, and Xiao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.synthia-dataset.net/">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/">Sift Flow (also known as LabelMe Outdoor, LMO)</a> - 2688 images, mainly outdoor natural and urban (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://dags.stanford.edu/projects/scenedataset.html">Stanford Background Dataset</a>  - 715 images of outdoor scenes containing at least one foreground object (Gould et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://collections.durham.ac.uk/catalog?utf8=%E2%9C%93&q=breckon">Surface detection</a> - Real-time traversable surface detection by colour space fusion and  temporal analysis - Evaluation Dataset (Breckon, Toby P., Katramados,  Ioannis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://taskonomy.stanford.edu/">Taskonomy</a> - Over 4.5  million real images each with ground truth for 25 semantic, 2D, and 3D  tasks. (Zamir, Sax, Shen, Guibas, Malik, Savarese) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/marialeyva/TB_Places">TB-Places</a> – a  data set of garden images for bechmarking algorithms for image retrieval and visual place recognition (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="https://www.research.ed.ac.uk/portal/en/datasets/trimbot2020-dataset-for-garden-navigation-and-bush-trimming(9f9de786-5e58-4bca-9279-f1d7ffddda41).html">TrimBot2020 Dataset for Garden Navigation</a> – sensor RGBD data recorded from cameras and other sensors mounted on a robotic platform as well as additional external sensors capturing the  garden (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="https://www.pf.bgu.tum.de/pub/testdaten.html">TUM City Campus</a> - Urban point clouds taken by Mobile Laser Scanning (MLS) for  classification, object extraction and change detection (Stilla, Hebel,  Xu, Gehrung) [3/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://ivi.fnwi.uva.nl/cv/intrinseg">UVA Intrinsic Images and Semantic Segmentation Dataset</a> – RGB dataset with ground-truth albedo, shading, and semantic annotations (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="http://www.rovit.ua.es/dataset/vidrilo/">ViDRILO</a> -  ViDRILO is a dataset containing 5 sequences of annotated RGB-D images  acquired with a mobile robot in two office buildings under challenging  lighting conditions. (Miguel Cazorla, J. Martinez-Gomez, M. Cazorla, I.  Garcia-Varea and V. Morell.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/">Virtual Gallery</a> - a synthetic dataset that targets multiple challenges such as varying  lighting conditions and different occlusion levels for various tasks  such as depth estimation, instance segmentation and visual localization  (Weinzaepfel, Csurka, Cabon, Humenberger) [7/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huangkuns/wireframe">Wireframe dataset</a> - A set of RGB images of man-made scenes are annotated with junctions  and lines, which describes the large-scale geometry of the scenes.  (Huang et al.) [Before 28/12/19]</li>
</ol>
<h2 id="Segmentation-General"><a href="#Segmentation-General" class="headerlink" title="Segmentation (General)"></a>Segmentation (General)</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.ime.usp.br/~eduardob/datasets/sky/">A Dataset for Sky Segmentation</a> - sentence describing it: This Sky dataset was used to evaluate the  method IFT-SLIC and other superpixel algorithms, using the  superpixel-based sky segmentation method proposed by Juraj Kostolansky.  It contains a collection of 60 images based on the Caltech Airplanes  Side dataset by R. Fergus  with ground truth for sky segmentation.  (Eduardo B. Alexandre, Paulo A. V. Miranda, R. Fergus) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.168158">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a> - 22+K hierarchically segmented and labeled scene images (900 scene  categories, 3+K classes and subpart classes) (Zhou, Zhao, Puig, Fidler,  Barriuso, Torralba) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html">Alpert et al. Segmentation evaluation database</a> (Sharon Alpert, Meirav Galun, Ronen Basri, Achi Brandt) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://bmc.iut-auvergne.com/">BMC (Background Model Challenge)</a> - A dataset for comparing background subtraction algorithms, composed of real and synthetic videos(Antoine) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">Berkeley Segmentation Dataset and Benchmark</a> (David Martin and Charless Fowlkes) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://zenodo.org/record/495570#.WZGTYl2lilM">CAD 120 affordance dataset</a> - Pixelwise affordance annotation in human context (Sawatzky, Srikantha, Gall) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvteam.net/projects/2018/MM/40Dataset.rar">COLT</a> - The dataset contains 40 imagenet categories with manually annotated per-pixel object masks. (Jia Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jkoteswarrao/Object-Co-skeletonization-with-Co-segmentation">CO-SKEL dataset</a> - This dataset consists of categorized skeleton and segmentation masks  for evaluating co-skeletonization methods. (Koteswar Rao Jerripothula,  Jianfei Cai, Jiangbo Lu, Junsong Yuan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.irit.fr/~Sylvie.Chambon/Crack_Detection_Database.html">Crack detection on 2D pavement images</a> - five sets of pavement images that contain cracks with the manual  ground truth associated and 5 automatic segmentations obtained with  existing approaches (Sylvie Chambon) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://clopema.felk.cvut.cz/color_and_depth_dataset.html">CTU Color and Depth Image Dataset of Spread Garments</a> - Images of spread garments with annotated corners. (Wagner, L., Krejov D., and Smutn V. (Czech Technical University in Prague)) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://clopema.felk.cvut.cz/garment_folding_photo_dataset.html">CTU Garment Folding Photo Dataset</a> - Color and depth images from various stages of garment folding.  (Sushkov R., Melkumov I., Smutn y V. (Czech Technical University in  Prague)) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cs.sfu.ca/~hamarneh/software/DeformIt/index.html">DeformIt 2.0</a> - Image Data Augmentation Tool: Simulate novel images with ground truth segmentations from a single image-segmentation pair (Brian Booth and  Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://better-flow.github.io/evimo/">EVIMO</a> - Dataset  for motion segmentation, egomotion estimation and tracking using an  event camera; the dataset is collected with DAVIS 346C and provides 3D  poses for camera and independently moving objects, and pixelwise motion  segmentation masks. (Mitrokhin, Ye, Fermuller, Aloimonos, Delbruck)  [14/1/20]</li>
<li><a target="_blank" rel="noopener" href="http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/segmentation/grabcut.htm">GrabCut Image database</a> (C. Rother, V. Kolmogorov, A. Blake, M. Brown) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://medisp.bme.teiath.gr/hicl/index.html">Histology Image Collection Library (HICL)</a> - The HICL is a compilation of 3870histopathological images (so far)  from various diseases, such as brain cancer,breast cancer and HPV (Human Papilloma Virus)-Cervical cancer. (Medical Image and Signal Processing  (MEDISP) Lab., Department of BiomedicalEngineering, School of  Engineering, University of West Attica) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://smartdoc.univ-lr.fr/">ICDAR’15 Smartphone document capture and OCR competition - challenge 1</a> - videos of documents filmed by a user with a smartphone to simulate  mobile document capture, and ground truth coordinates of the document  corners to detect. (Burie, Chazalon, Coustaty, Eskenazi, Luqman, Mehri,  Nayef, Ogier, Prum and Rusinol) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/">Intrinsic Images in the Wild (IIW)</a> -  Intrinsic Images in the Wild, is a large-scale, public dataset for  evaluating intrinsic image decompositions of indoor scenes (Sean Bell,  Kavita Bala, Noah Snavely) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://people.csail.mit.edu/brussell/research/LabelMe/intro.html">LabelMe images database and online annotation tool</a> (Bryan Russell, Antonio Torralba, Kevin Murphy, William Freeman) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.lits-challenge.com/">LITS Liver Tumor Segmentation</a> - 130 3D CT scans with segmentations of the liver and liver tumor.  Public benchmark with leaderboard at Codalab.org (Patrick Christ)  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/minc/">Materials in Context (MINC)</a> - The Materials in Context Database (MINC) builds on OpenSurfaces, but  includes millions of point annotations of material labels. (Sean Bell,  Paul Upchurch, Noah Snavely, Kavita Bala) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network"> Multi-species fruit flower detection</a> - This dataset consists of four sets of flower images, from three  different tree species: apple, peach, and pear, and accompanying ground  truth images. (Philipe A. Dias, Amy Tabb, Henry Medeiros) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.vision.ime.usp.br/~lucyacm/thesis/coift.html"> Objects with thin and elongated parts</a> - The three datasets used to evaluate our method Oriented Image  Foresting Transform with Connectivity Constraints, which contain objects with thin and elongated parts. These databases are composed of 280  public images of birds and insects with ground truths. (Lucy A. C.  Mansilla (IME-USP), Paulo A. V. Miranda) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://opensurfaces.cs.cornell.edu/publications/opensurfaces/">OpenSurfaces</a> - OpenSurfaces consists of tens of thousands of examples of surfaces  segmented from consumer photographs of interiors, and annotated with  material parameters, texture information, and contextual information .  (Kavita Bala et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ikw.uos.de/~cv/projects/mm-mkv">Osnabrück gaze tracking data</a> - 318 video sequences from several different gaze tracking data sets  with polygon based object annotation. (Schöning, Faion, Heidemann,  Krumnack, Gert, Açik, Kietzmann, Heidemann &amp; König) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.jifengdai.org/downloads/scribble_sup/">PASCAL-Scribble Dataset</a> - Our  PASCAL-Scribble Dataset provides scribble-annotations on 59 object/stuff categories. (Di Lin) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://lrs.icg.tugraz.at/research/petroglyphsegmentation/">PetroSurf3D</a> - 26 high resolution (sub-millimeter accuracy) 3D scans of rock art  with pixelwise labeling of petroglyphs for segmentation. (Poier, Seidl,  Zeppelzauer, Reinbacher, Schaich, Bellandi, Marretta, Bischof) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://sailvos.web.illinois.edu/">SAIL-VOS</a> - The  Semantic Amodal Instance Level Video Object Segmentation (SAIL-VOS)  dataset provides accurate ground truth annotations to develop methods  for reasoning about occluded parts of objects while enabling to take  temporal information into account (Hu, Chen, Hui, Huang, Schwing)  [29/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://doi.org/10.5281/zenodo.59019">Shadow Detection/Texture Segmentation Computer Vision Dataset</a> - Video based sequences for shadow detection/suppression, with ground  truth (Newey, C., Jones, O., &amp; Dee, H. M.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.synthia-dataset.net/">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip">Stony Brook University Shadow Dataset (SBU-Shadow5k)</a> - Large scale shadow detection dataset from a wide variety of scenes  and photo types, with human annotations (Tomas F.Y. Vicente, Le Hou,  Chen-Ping Yu, Minh Hoai, Dimitris Samaras) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://gitlab.com/nicstrisc/RUSTICO/tree/master/data">TB-roses-v1</a> – data set of rose bush images with ground truth for evaluation of rose stem segmentation (TrimBot2020 consortium) [26/2/20]</li>
<li><a target="_blank" rel="noopener" href="http://www.tromai.icoc.me/">TRoM: Tsinghua Road Markings</a> - This is a dataset which contributes to the area of road marking  segmentation for Automated Driving and ADAS. (Xiaolong Liu, Zhidong  Deng, Lele Cao, Hongchao Lu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://ivi.fnwi.uva.nl/cv/intrinseg">UVA Intrinsic Images and Semantic Segmentation Dataset</a> - RGB dataset with ground-truth albedo, shading, and semantic  annotations (Baslamisli, Groenestege, Das, Le, Karaoglu, Gevers)&gt;  [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvteam.net/projects/TIP18-VOS/VOS-Dataset.zip">VOS</a> - A dataset with 200 Internet videos for video-based salient object  detection and segmentation. (Jia Li, Changqun Xia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz">XPIE</a> - An image dataset with 10000 images containing manually annotated  salient objects and 8596 containing no salient objects. (Jia Li,  Changqun Xia) [Before 28/12/19]</li>
</ol>
<h2 id="Simultaneous-Localization-and-Mapping"><a href="#Simultaneous-Localization-and-Mapping" class="headerlink" title="Simultaneous Localization and Mapping"></a>Simultaneous Localization and Mapping</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/torrvision/CollaborativeSLAMDataset">Collaborative SLAM Dataset (CSD)</a> - The dataset consists of four different subsets - Flat, House, Priory  and Lab - each containing several RGB-D sequences that can be  reconstructed and successfully relocalised against each other to form a  combined 3D model. Each sequence was captured using an Asus ZenFone AR,  and we provide an accurate local 6D pose for each RGB-D frame in the  dataset. We also provide the calibration parameters for the depth and  colour sensors, optimised global poses for the sequences in each subset, and a pre-built mesh of each sequence. (Golodetz, Cavallari, Lord,  Prisacariu, Murray, Torr) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/davis_data.html">Event-Camera Data for Pose Estimation, Visual Odometry, and SLAM</a>The data also include intensity images, inertial measurements, and ground  truth from a motion-capture system. (ETH) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://better-flow.github.io/evimo/">EVIMO</a> - Dataset  for motion segmentation, egomotion estimation and tracking using an  event camera; the dataset is collected with DAVIS 346C and provides 3D  poses for camera and independently moving objects, and pixelwise motion  segmentation masks. (Mitrokhin, Ye, Fermuller, Aloimonos, Delbruck)  [14/1/20]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/House3D">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rpg.ifi.uzh.ch/datasets/mav_circle.tar.gz">Indoor Dataset of Quadrotor with Down-Looking Camera</a> - This dataset contains the recording of the raw images, IMU  measurements as well as the ground truth poses of a quadrotor flying a  circular trajectory in an office size environment. (Scaramuzza, ETH  Zurich, University of Zurich) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ok.sc.e.titech.ac.jp/INLOC/">InLoc</a> -  Benchmark for evaluating the accuracy of 6DoF visual localization  algorithms in challenging indoor scenarios. (Hajime Taira, Masatoshi  Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic,  Tomas Pajdla, Akihiko Torii) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://visuallocalization.net/">Long-term visual localization</a> - TBenchmark for evaluating visual localization and mapping algorithms  under various illumination and seasonal condition. (Torsten Sattler,  Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik  Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, Tomas Pajdla) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://hijeffery.github.io/PanoNavi/">PanoNavi dataset</a> - A panoramic dataset for robot navigation, consisted of 5 videos lasting about 1 hour. (Lingyan Ran) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.rawseeds.org/home/category/benchmarking-toolkit/datasets/">RAWSEEDS SLAM benchmark datasets</a> (Rawseeds Project) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://figshare.com/articles/Rijksmuseum_Challenge_2014/5660617">Rijksmuseum Challenge 2014</a> - It consist of 100K art objects from the rijksmuseum and comes with an extensive xml files describing each object. (Thomas Mensink and Jan van Gemert) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.bici-lab.org/projects/visual-paths-navigation">RSM dataset of Visual Paths</a> - Visual dataset of indoor spaces to benchmark localisation/navigation  methods. It consists of 1.5 km of corridors and indoor spaces with  ground truth for every frame, measured as distance in centimetres from  starting point. Includes a synthetically generated corridor for  benchmark. (Jose Rivera-Rubio, Ioannis Alexiou, Anil A. Bharath) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://daniilidis-group.github.io/mvsec/">The Multi Vehicle Stereo Event Camera Dataset</a> - Multiple sequences containing a stereo pair of DAVIS 346b event  cameras with ground truth poses, depth maps and optical flow. (lex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, Kostas  Daniilidis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/data/datasets/rgbd-dataset">TUM RGB-D Benchmark</a> - Dataset and benchmark for the evaluation of RGB-D visual odometry and SLAM algorithms (BCrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard and Daniel Cremers) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://vision.in.tum.de/data/datasets/visual-inertial-dataset">TUM VI Benchmark</a> - 28 sequences, indoor and outdoor, sensor data from stereo camera and  IMU, accurate ground truth at beginning and end segments. (David  Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, Joerg  Stueckler, Daniel Cremers) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php">Visual Odometry / SLAM Evaluation</a> - The odometry benchmark consists of 22 stereo sequences (Andreas Geiger and Philip Lenz and Raquel Urtasun) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.hs-karlsruhe.de/odometry-data/">Visual Odometry Dataset with Plenoptic and Stereo Data</a> - The dataset contains 11 sequences recorded by a hand-held platform  consisting of a plenoptic camera and a pair of stereo cameras. The  sequences comprising different indoor and outdoor sequences with  trajectory length ranging from 25 meters up to several hundred meters.  The recorded sequences show moving objects as well as changing lighting  conditions. (Niclas Zeller and Franz Quint, Hochschule Karlsruhe,  Karlsruhe University of Applied Sciences) [Before 28/12/19]</li>
</ol>
<h2 id="Surveillance-and-Tracking"><a href="#Surveillance-and-Tracking" class="headerlink" title="Surveillance and Tracking"></a>Surveillance and Tracking</h2><ol>
<li><a target="_blank" rel="noopener" href="http://dixie.udg.edu/udgms/">A collection of challenging motion segmentation benchmark datasets</a> - These datasets enclose real-life long and short sequences, with  increased number of motions and frames per sequence, and also real  distortions with missing data. The ground truth is provided on all the  frames of all the sequences. (Muhammad Habib Mahmood, Yago Diez, Joaquim Salvi, Xavier Llado) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.sethi.org/ricky/datasets/">ATOMIC GROUP ACTIONS dataset</a> - (Ricky J. Sethi et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.aiia.csd.auth.gr/LAB_PROJECTS/MULTIDRONE/AUTH_MULTIDRONE_Dataset.html">AUT MULTIDRONE video dataset for racing bicycle detection/tracking from UAV footage</a> -  7 Youtube videos (resolution: 1920 x 1080) at 25fps (Mademlis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~andrea/avss2007_d.html">AVSS07: Advanced Video and Signal based Surveillance 2007 datasets</a> (Andrea Cavallaro) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.idiap.ch/~odobez/RESSOURCES/DataRelease-TrafficJunction.php">Activity modeling and abnormality detection dataset</a> - The dataset containes a 45 minutes video with annotated anomalies. (Jagan Varadarajan and Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/backgroundsubtraction/test-sequences">Background subtraction</a> - a list of datasets about background subtraction(Thierry BOUWMANS ) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.uow.edu.au/~wanqing/#Datasets">CAMO-UOW Dataset</a> - 10 high resolution videos captured in real scenes for camouflaged  background subtraction (Shuai Li and Wanqing Li) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rose1.ntu.edu.sg/Datasets/cctvFights.asp">CCTV-Fights</a> - 1,000 videos picturing real-world fights, recorded from CCTVs or  mobile cameras, and temporally annotated at the frame level. (Mauricio  Perez, ROSE Lab, NTU) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.consortium.ri.cmu.edu/projSRD.php">CMUSRD: Surveillance Research Dataset</a> - multi-camera video for indoor surveillance scenario (K. Hattori, H. Hattori, et al) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vision.cs.duke.edu/DukeMTMC/">DukeMTMC: Duke Multi-Target Multi-Camera tracking dataset</a> - 8 cameras, 85 min, 2m frames, 2000 people of video (Ergys Ristani,  Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo Tomasi) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/layumi/DukeMTMC-reID_evaluation">DukeMTMC-reID</a> - A subset of the DukeMTMC for image-based person re-identification (8  cameras,16,522 training images of 702 identities, 2,228 query images of  the other 702 identities and 17,661 gallery images.) (Zheng, Zheng, and  Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www-sop.inria.fr/orion/ETISEO/download.htm">ETISEO Video Surveillance Download Datasets</a> (INRIA Orion Team and others) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cmp.felk.cvut.cz/fmo/">FMO dataset</a> - FMO dataset contains annotated video sequences with Fast Moving Objects - objects  which move over a projected distance larger than their size in one  frame. (Denys Rozumnyi, Jan Kotera, Lukas Novotny, Ales Hrabalik, Filip  Sroubek, Jiri Matas) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://vislab.isr.ist.utl.pt/hda-dataset/">HDA+ Multi-camera Surveillance Dataset</a> - video from a network of 18 heterogeneous cameras (different  resolutions and frame rates) distributed over 3 floors of a research  institute with 13 fully labeled sequences, 85 persons, and 64028  bounding boxes of persons. (D. Figueira, M. Taiana, A. Nambiar, J.  Nascimento and A. Bernardino) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://github.com/RichieZh/HIC">Human click data</a> - 20K human clicks on a tracking target (including click errors) (Zhu and Porikli) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html">Immediacy Dataset</a> - This dataset is designed for estimation personal relationships. (Xiao Chu et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mahnob-db.eu/">MAHNOB Databases</a> -including Laughter Database,HCI-tagging Database,MHI-Mimicry Database( M. Pantic. etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://s.fhg.de/mini-rgbd">Moving INfants In RGB-D (MINI-RGBD)</a> - A synthetic, realistic RGB-D data set for infant pose estimation  containing 12 sequences of moving infants with ground truth joint  positions. (N. Hesse, C. Bodensteiner, M. Arens, U. G. Hofmann, R.  Weinberger, A. S. Schroeder) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.pkuvmc.com/publications/msmt17.html">MSMT17</a> - Person re-identification dataset. 180 hours of videos, 12 outdoor  cameras, 3 indoor cameras, and 12 time slots. (Wei Longhui, Zhang  Shiliang, Gao Wen, Tian Qi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://multidrone.eu/">MULTIDRONE boat detection/tracking</a> - 3 HD videos (720p - 1280 x 720) subsampled at 25 fps (Mademlis,) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://community.wvu.edu/~samotiian/datasets.html">MVHAUS-PI</a> - a multi-view human interaction recognition dataset (Saeid et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.openvisor.org/">Openvisor - Video surveillance Online Repository</a> (Univ of Modena and Reggio Emilia) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html">Parking-Lot dataset</a> - Parking-Lot dataset is a car dataset which focus on moderate and  heavily occlusions on cars in the parking lot scenario. (B. Li, T.F. Wu  and S.C. Zhu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/site/pornographydatabase/">Pornography Database</a> - The Pornography database is a pornography detection dataset  containing nearly 80 hours of 400 pornographic and 400 non-pornographic  videos extracted from pornography websites and Youtube. (Avila, Thome,  Cord, Valle, de Araujo) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tracking.cs.princeton.edu/">Princeton Tracking Benchmark</a>  - 100 RGBD tracking datasets (Song and Xiao) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html">QMUL Junction Dataset 1 and 2</a> - Videos of busy road junctions. Supports anomaly detection tasks. (T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~xx302/ProjectPage/TCSVT_15/index.html">Queen Mary Multi-Camera Distributed Traffic Scenes Dataset (QMDTS)</a> - The QMDTS is collected from urban surveillance environment for the  study of  surveillance behaviours in distributed scenes. (Dr. Xun Xu.  Prof. Shaogang Gong and Dr. Timothy Hospedales) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://radprojectbismil.blogspot.co.uk/">Road Anomaly Detection</a> - 22km, 11 vehicles, normal + 4 defect categories (Hameed, Mazhar, Hassan) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org">S-Hock dataset</a> - A new Benchmark for Spectator Crowd Analysis. (Francesco Setti,  Davide Conigliaro, Paolo Rota, Chiara Bassetti, Nicola Conci, Nicu Sebe, Marco Cristani) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://tev.fbk.eu/salsa">SALSA: Synergetic sociAL Scene Analysis</a> - A Novel Dataset for Multimodal Group Behavior Analysis(Xavier Alameda-Pineda etc.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://scenebackgroundmodeling.net/">SBMnet (Scene Background Modeling.NET)</a> - A dataset for testing background estimation algorithms(Jodoin, Maddalena, and Petrosino) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html">SBM-RGBD dataset</a> - 35 Kinect indoor RGBD videos to evaluate and compare scene background modelling methods for moving object detection (Camplani, Maddalena,  Moy?? Alcover, Petrosino, Salgado) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://uti.eu.com/pncd-scouter/rezultate-en.html">SCOUTER</a> - video surveillance ground truthing (shifting perspectives, different  setups/lighting conditions, large variations of subject). 30 videos and  approximately 36,000 manually labeled frames. (Catalin Mitrea) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://best.sjtu.edu.cn/">SJTU-BEST</a>One  surveillance-specified datasets platform with realistic, on-using  camera-captured, diverse set of surveillance images and videos (Shanghai Jiao Tong University) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~andrea/spevi.html">SPEVI: Surveillance Performance EValuation Initiative</a> (Queen Mary University London) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/">Shinpuhkan 2014</a> - A Person Re-identification dataset containing 22,000 images of 24  people captured by 16 cameras. (Yasutomo Kawanishi et al.) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvgl.stanford.edu/projects/uav_data/">Stanford Drone Dataset</a> - 60 images and videos of various types of agents (not just  pedestrians, but also bicyclists, skateboarders, cars, buses, and golf  carts) that navigate in a real world outdoor environment such as a  university campus (Robicquet, Sadeghian, Alahi, Savarese) [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.vis.uni-stuttgart.de/en/research/visual_analytics/visual-analytics-video-steams/stuttgart_artificial_background_subtraction_dataset/index.html">Stuttgart Artificial Background Subtraction Dataset</a> [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="https://www.dropbox.com/s/2rhugw868em465r/DSTdataset.zip?dl=0">Tracking in extremely cluttered scenes</a> - this single object tracking dataset has 28 highly cluttered sequences with per frame annotation(Jingjing Xiao,Linbo Qiao,Rustam Stolkin,Ale  Leonardis) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org">TrackingNet</a> - Large-scale dataset for tracking in the wild: more than 30k annotated sequences for training, more than 500 sequestered sequences for  testing, evaluation server and leaderboard for fair ranking. (Matthias  Muller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi and Bernard  Ghanem) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://crcv.ucf.edu/projects/real-world/">UCF-Crime Dataset: Real-world Anomaly Detection in Surveillance Videos</a> - A large-scale dataset for real-world anomaly detection in  surveillance videos. It consists of 1900 long and untrimmed real-world  surveillance videos (of 128 hours), with 13 realistic anomalies such as  fighting, road accident, burglary, robbery, etc. as well as normal  activities. (Center for Research in Computer Vision, University of  Central Florida) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html">UCLA Aerial Event Dataset</a> - Human activities in aerial videos with annotations of people,  objects, social groups, activities and roles (Shu, Xie, Rothrock,  Todorovic, and Zhu) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm">UCSD Anomaly Detection Dataset</a> - a stationary camera mounted at an elevation, overlooking pedestrian  walkways, with unusual pedestrian or non-pedestrian motion. [Before  28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvrr.ucsd.edu/bmorris/datasets/">UCSD trajectory clustering and analysis datasets</a> - (Morris and Trivedi) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://research.sethi.org/ricky/datasets/">USC Information Sciences Institute’s ATOMIC PAIR ACTIONS dataset</a> - (Ricky J. Sethi et al.) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://avires.dimi.uniud.it/papers/trclust/">Udine Trajectory-based anomalous event detection dataset</a> - synthetic trajectory datasets with outliers (Univ of Udine Artificial Vision and Real Time Systems Laboratory) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html">Visual Tracker Benchmark</a> - 100 object tracking sequences with ground truth with <a target="_blank" rel="noopener" href="http://www.visual-tracking.net/">Visual Tracker Benchmark evaluation</a>, including tracking results from a number of trackers (Wu, Lim, Yang) [Before 28/12/19]</li>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
</ol>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>License： </strong>
          
          <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://charliej107.github.io/Repost/CVonline%20Image%20Databases.html" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CV/" rel="tag">CV</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dataset/" rel="tag">Dataset</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/Experience/%E4%BD%BF%E7%94%A8%20Verilog%20%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0FGPA%E4%B8%8A%E7%9A%84Mealy%E7%8A%B6%E6%80%81%E6%9C%BA.html" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            使用 Verilog 设计实现FGPA上的Mealy状态机
          
        </div>
      </a>
    
    
      <a href="/Tutorial/C++%E8%BF%AD%E4%BB%A3%E5%99%A8.html" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title"></div>
      </a>
    
  </nav>

  
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2019-2020
        <i class="ri-heart-fill heart_icon"></i> Charlie J
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Vankyle"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://data-vankyle-1257862518.cos.ap-shanghai.myqcloud.com/image/Blog/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://data-vankyle-1257862518.cos.ap-shanghai.myqcloud.com/image/Blog/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>