<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>(也许是)全世界第一份在RaspberryPi4上安装ROS-melodic的教程</title>
    <url>/Tutorial/(%20%E4%B9%9F%E8%AE%B8%E6%98%AF%20)%20%E5%85%A8%E4%B8%96%E7%95%8C%E7%AC%AC%E4%B8%80%E4%BB%BD%E5%9C%A8RaspberryPi4%E4%B8%8A%E5%AE%89%E8%A3%85ROS%E7%9A%84%E6%95%99%E7%A8%8B.html</url>
    <content><![CDATA[<h5 id="How-to-install-ROS-Melodic-On-Raspberry-Pi-4-with-Ubuntu-Server-19-10"><a href="#How-to-install-ROS-Melodic-On-Raspberry-Pi-4-with-Ubuntu-Server-19-10" class="headerlink" title="How to install ROS - Melodic On Raspberry Pi 4 with Ubuntu Server 19.10"></a>How to install ROS - Melodic On Raspberry Pi 4 with Ubuntu Server 19.10</h5><a id="more"></a>

<h3 id="失踪人口回归"><a href="#失踪人口回归" class="headerlink" title="失踪人口回归"></a>失踪人口回归</h3><p>长期以来被学校和各种各样其他的事物压榨得奄奄一息, 一度把自己弄到抑郁. 但经过这一番奋斗, 虽然依然压力很大, 但还是希望能够留下点东西, 不然学完了就学完了, 姑且不说久不用了会不会忘, 重要的是, 总需要一点东西逼迫自己写点东西. 于是, 还是希望每周能抽出一点时间来写点东西. </p>
<h3 id="源起"><a href="#源起" class="headerlink" title="源起"></a>源起</h3><p>​    想做一个情绪机器人, 灵感来源于超能陆战队里的大白. 但是大白的这个形象从硬件上涉及很多问题, 根据曾灿辉的说法, 几乎不可能做到. 心血来潮想要参加微软的IC, 所以就想着, 要不做个球吧. 叫做Ba Ba’ll. 名字起得很随便, 总之, 这玩意儿是一个机器人. </p>
<p>​    基本的思想就是树莓派作为大脑, 控制高级功能, 再用一个Arduino来作为小脑, 控制运动. RaspberryPi4刚刚出来, 4GB的RAM真的很爽. 不过官方的Raspberrybian还是一个32位的操作系统, 而且作为一个教育为导向的操作系统, 总觉得太Low了. ROS也并没有为了这个版本的debian设计过. 虽然一样没有为1910的Ubuntu设计, 但好歹也是Ubuntu. </p>
<p>​    不过Ubuntu还是有问题. 首先, 18.04 的LTS是不支持RP4的, 这玩意儿出来的时候RP4还没出来. 如果强行刷进去的话会直接没办法进系统. 第二, 即使是19.10 (eoan) 的Ubuntu server, 还是有个惊天bug导致了整个USB不能用. 因为这个USB的芯片需要一些有特殊要求的内存空间, 但是这个内存空间没有被好好划给芯片, 被系统其他部分占用了. 这个东西已经被一个内核补丁给搞定了, 但是在我的板子上解决这个问题还要重新编译内核, 这个可能可以留给以后去解决这个问题. </p>
<p>​    于是就有了我们这个并不完美的环境: 一块RP4, 一根网线 ( 用来连接SSH访问命令行) , 以及电源. 外带一个预安装版本的Ubuntu server 19.10 的arm64版本. </p>
<h3 id="正片开始"><a href="#正片开始" class="headerlink" title="正片开始"></a>正片开始</h3><p>​    在RP4上安装ROS有以下几个基本问题:</p>
<ol>
<li>ROS是为Ubuntu设计的, 其他的发行版只是顺便支持一下. 所以Raspberrybian是别想了, 乖乖按照Ubuntu官网上的指南安装Ubuntu. 在此之前, 很多人都是在RP上装Ubuntu mate. 这个mate虽然也是基于Ubuntu, 但是难保你这个定制的桌面环境会给你整出什么幺蛾子. 当然作为一个愿意折腾的人虽然我们不怕幺蛾子, 但是第一这玩意儿很浪费时间, 第二, 你想想一旦出了幺蛾子, 你觉不觉得Ubuntu server 的相关资料要比一个小众的mate要多得多? </li>
<li>ROS的预编译版本是为x86的设备编译的, 虽然也有arm64的版本, 但无论是哪个预编译版本, 都只支持bionic(18.04). 这就意味着仅有19.10能支持的树莓派也别想用预编译版本了, 只能从源码编译. </li>
<li>正如之前所说, ROS的依赖关系检查和安装是由一个专门的工具 (<code>rorsdep</code>) 来完成的. 这玩意儿是python写的, 而Cpython的解释器是c写的. 虽然自己从源码编译理论上能够保证任何有GCC的平台就能用上ROS, 然鹅这个愚蠢的<code>rosdep</code>根本不认识刚发布不久的19.10. 你在使用<code>rosdep</code>检查和处理依赖的时候会报一堆说不认识你这个OS版本. 这就意味着你只能自己找依赖. </li>
</ol>
<p>​    首先先记得换一下镜像源. 镜像源的问题已经老生常谈了. 网上有很多资料, 总的来说, 就是换掉<code>/etc/apt/source.list</code>里面的域名. 我原本以为这个应该是一个很简单那的东西, 但是我发现好多人根本不看版本, 直接上CSDN上面照抄, 结果一个1604的版本换了个1804的镜像源, 报了一堆错. </p>
<p>​    哦, 也有一堆人根本不知道镜像源是什么, 我原本以为机器人队质量会高一点, 没想到一堆人还不知道linux是啥…</p>
<p>​    嗯, 中国的高考制度让中国优秀的大学里充斥着90%的精通考试的学生, 但这群学生一旦遇到考试之外的东西, 完全不知道怎么下手. 而中国的大学为了保证这群学生不至于太惨, 只好调整自己的考试和考核模式, 即让考试考得好的人评价好, 而实际的工程能力和学习能力, who cares? 我非常理解这种举动, 因为大家都是这样上来的, 所有人都是既得利益者. 所以只能牺牲那极少数的, 工程OK但考试不强的, 来希图那三分之一考试比较好的人里面, 能够萌生出更极少数的, 工程能力强的人. </p>
<h3 id="官网上的能用的部分"><a href="#官网上的能用的部分" class="headerlink" title="官网上的能用的部分"></a>官网上的能用的部分</h3><ol>
<li><p>安装<code>bootstrap</code>依赖. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-rosdep python-rosinstall-generator python-wstool python-rosinstall build-essential</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化 <code>rosdep</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo rosdep init</span><br><span class="line">rosdep update</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建<code>catkin</code>工作区</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir ~/ros_catkin_ws</span><br><span class="line"><span class="built_in">cd</span> ~/ros_catkin_ws</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后用<code>wstool</code>把源码下载下来</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rosinstall_generator desktop --rosdistro melodic --deps --tar &gt; melodic-desktop.rosinstall</span><br><span class="line">wstool init -j8 src melodic-desktop.rosinstall</span><br></pre></td></tr></table></figure>

<p>这里安装的是带桌面组件的ROS melodic. 可以根据需要选择其他版本, 也可以选择命令行, 不带桌面组件. 因为目前还处在开发阶段, 等到实际跑起来的时候就不需要桌面了, 直接上命令行的 Bare Bones 版本. </p>
</li>
</ol>
<p>​    好, 官网上能用的部分就到这儿了. 接下来按照官网上的东西会出现各种各样的问题, 我们解决问题的漫漫长路也是从这里开始的.. </p>
<h3 id="解决依赖"><a href="#解决依赖" class="headerlink" title="解决依赖"></a>解决依赖</h3><p>​    到目前为止, 按照官网上的东西都能够正常跑通. 接下来就是要解决依赖的问题了. 官网上的命令是这样的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rosdep install --from-paths src --ignore-src --rosdistro melodic -y</span><br></pre></td></tr></table></figure>



<p>到这里会报一堆错, 大概的意思就是劳资不认识你这个系统版本. 因为咱们这个系统是19.10, 它不认识是正常的. </p>
<p>​    这个时候我们可以先行找到一些 (有可能) 需要安装并且可以被安装的依赖, 简单来说, 看看上面告诉你什么包没找到, 就在<code>apt</code>里面安装这个包. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install python-xxx -y</span><br></pre></td></tr></table></figure>

<p>当然, 你就算安装了这些包, 再次跑一边检查依赖的命令还是会报同样的错误, 也许缺的包少一些了, 但问题还没有被解决. </p>
<p>​    我在走这一步的时候, 先后尝试了很多种花式安装, 先是用<code>apt</code>安装了一遍, 又用<code>pip</code>安装了一遍, 但是还是没有解决这个问题, 索性就不去管他, 直接开始编译. </p>
<h3 id="踩完了坑之后的做法"><a href="#踩完了坑之后的做法" class="headerlink" title="踩完了坑之后的做法"></a>踩完了坑之后的做法</h3><p>​    如果你根本不关心我在这个过程当中遇到的艰难险阻和朴实无华却行之有效的调试方法, 你可以直接跑一遍接下来的命令, 然后直接开始编译. </p>
<p>​    首先是可以用<code>apt</code>或者<code>pip</code>直接安装的: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev libeigen3-dev python-empy liblog4cxx-dev tinyxml-dev qt5-default python-pyqt5 python-lz4 python3-lz4 liburdfdom-dev libzip2 libogre-1.9.0-dev libogre-1.9.0v5  libyaml-cpp-dev libyaml-cpp0.6  libassimp-dev assimp-utils libassimp4 python-pyassimp python3-pyassimp  python-netifaces python3-netifaces</span><br></pre></td></tr></table></figure>

<p>然后就是有一个很尴尬的事情, 有些依赖我也不知道是哪个包出了问题, 所以索性就把所有有可能的包装上了. 我猜有一堆人不知道<code>apt</code>还有<code>apt search</code>这个命令的, 你可以试试用<code>apt search</code>一些缺的, 然后把看起来有用的都装上, 最后解决了这两个东西的依赖问题. 鉴于我也不知最后到底是哪个包起作用, 读者可以自己试着做一下. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt search lz4</span><br><span class="line">apt search bzip2</span><br><span class="line">apt search opencv</span><br></pre></td></tr></table></figure>

<p>这里说一下, lz4我主要挑了一些介绍或者名字里面有ros和lz4的. 另外, bzip2应该就是libzip之类的库需要装, 所以可以不用把所有搜索出来的都装上, 有选择性地装上就可以了. </p>
<p>有一些东西需要自己下载源码下来编译. 在此之前, 请确保自己装了git 并且配置了全局用户名. 有些系统镜像(比如docker里面的那个)过于精简, 甚至连wget都没有. 不过考虑到这是树莓派而且这是Ubuntu Server, 所以应该这些都是有的. </p>
<p>​    首先是可以从GitHub上面clone 源码的</p>
<p>console_bridge</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/ros/console_bridge.git</span><br><span class="line"><span class="built_in">cd</span> console_bridge</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>



<p>gtenst</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libgtest-dev</span><br><span class="line">sudo apt-get install cmake <span class="comment"># install cmake</span></span><br><span class="line"><span class="built_in">cd</span> /usr/src/gtest</span><br><span class="line">sudo cmake CMakeLists.txt</span><br><span class="line">sudo make</span><br></pre></td></tr></table></figure>

<p>tinyxml2</p>
<p>这个说一下, 早些年这玩意儿就是tinyxml, 后来更新了以后就是tinyxml2, tinyxml的库就没了. 但是不知道为啥这个ROS里面还有一些包用的还是旧版本的tinyxml, 值得庆幸的是旧版的tinyxml可以用<code>apt</code>安装, 所以也顺利解决了这个问题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/leethomason/tinyxml2.git</span><br><span class="line"><span class="built_in">cd</span> tityxml.git</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>然后就是sip</p>
<p>这个涉及到一个很严肃的问题: python2 今年上半年停止支持了以后, pypi干脆连Python2的sip都不支持了. 所以这个本来可以用<code>pip</code>安装的东西, 到头来却要从源码编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://www.riverbankcomputing.com/static/Downloads/sip/4.19.19/sip-4.19.19tar.gz</span><br><span class="line">tar -xvfz sip-4.19.19.tar.gz</span><br><span class="line"><span class="built_in">cd</span> sip-4.19.19.tar.gz</span><br><span class="line">python configure.py</span><br><span class="line">sudo make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>差不多就是这些了. </p>
<p>这个时候就可以放心大胆地继续跑官网的那个教程里面的make环节了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./src/catkin/bin/catkin_make_isolated --install -DCMAKE_BUILD_TYPE=Release --install-space /opt/ros/melodic</span><br></pre></td></tr></table></figure>

<p>这个命令注意一下, 官网上的是没有最后那个<code>--instal-space</code>这个参数的. 这个参数主要是用来指定安装的地方, 我个人建议最好还是指定一下, 尽可能缩小不同平台之间的差距, 提高代码的可移植性. </p>
<p>然后就是, 为了能够获得与预编译版本相似的体验, 我个人建议不要依赖官网上的编译教程, 在最后设置环境的时候, 还是按照预编译安装的那个教程, 走这个命令: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/melodic/setup.bash"</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>



<p>​    到此为止, 整个ROS就安装好了. </p>
]]></content>
      <categories>
        <category>Tutorial</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Ubuntu</tag>
        <tag>Raspberry Pi</tag>
      </tags>
  </entry>
  <entry>
    <title>2019年各大高校自主招生政策解读</title>
    <url>/Other/2019%E5%B9%B4%E5%90%84%E5%A4%A7%E9%AB%98%E6%A0%A1%E8%87%AA%E4%B8%BB%E6%8B%9B%E7%94%9F%E6%94%BF%E7%AD%96%E8%A7%A3%E8%AF%BB.html</url>
    <content><![CDATA[<p> 2019年各大高校的自主招生工作正在进行, 目前已有半数高校已经发布了自主招生简章. 笔者作为曾经的自主招生的受益者 , 同时作为各位考生忠实的陪伴者, 在此对截止目前公布的各大高校的自主招生简章进行粗浅的解读, 仅供参考.</p>
<a id="more"></a>
<h3 id="进行自招的学校数量没有太大变化"><a href="#进行自招的学校数量没有太大变化" class="headerlink" title="进行自招的学校数量没有太大变化."></a>进行自招的学校数量没有太大变化.</h3><p>目前已经公布自主招生简章的学校包括但不限于北京科技大学, 北京化工大学, 北京邮电大学, 北京林业大学, 北京中医药大学, 北京师范大学, 北京语言大学, 中国传媒大学, 对外经贸大学, 武汉大学, 厦门大学, 中山大学, 中南大学, 中南财经政法大学等.  </p>
<p>​    根据阳光高考网的公示, 今年进行自主招生的学校较往年相比没有太大变化, 厦门大学, 武汉大学等往年较为热门的自招学校都发布了自己的招生简章. </p>
<h3 id="自招政策明显收紧"><a href="#自招政策明显收紧" class="headerlink" title="自招政策明显收紧"></a>自招政策明显收紧</h3><p>目前已经公布自招简章的学校当中, 大多数学校的招生政策都有较大调整, 主要呈现出以下几个特点:</p>
<ol>
<li><p>五大学科竞赛的成为唯一必要条件, 其他各大竞赛不再作为硬性条件. </p>
<p>绝大多数的学校在自招简章中都明确表示, 申请参加自招的考生应当在全国数学\物理\化学\生物\信息竞赛中, 获得省级赛区二等奖以上的奖项, 一些学校甚至要求全国三等甚至二等奖. 并且包括厦门大学, 中山大学, 武汉理工大学在内的多所学校都明确五大学科竞赛的奖项是唯一必要条件. 其他竞赛不再作为申请参加自招的条件. 也就是说, 在许多学校的自招中, 五大学科竞赛的奖项将成为唯一的”敲门砖”, 没有五大学科竞赛的获奖将在自招中被绝大多数学校拒之门外. </p>
</li>
<li><p>专利、论文不再作为申请的参考资料</p>
<p>许多学校都在自招简章中明确注明, 考生所取得的专利和论文发表不再作为申报材料. 也就是说, 专利和论文不再作为自招当中的加分项. 这可以说是杜绝了专利和论文造假的现象. </p>
</li>
<li><p>体测成绩将作为评审的重要参考</p>
<p>多个学校已经明确公布了考核方案, 其中体测作为一个重要考核项逐渐成为一个重要的参考. 多个学校已经明确注明, 体测成绩将作为考核的重要参考, 这也对考生的体质水平提出了更高要求. </p>
</li>
<li><p>降分幅度大幅收紧</p>
<p>多个学校的录取政策中, 基本上都要求考生达到高校在考生所在省份模拟投档线下20-50分, 一些曾经”一本线录取”等政策在今年各大高校的招生简章中消声觅迹. </p>
</li>
</ol>
<h3 id="自招政策的收紧带来的三大影响"><a href="#自招政策的收紧带来的三大影响" class="headerlink" title="自招政策的收紧带来的三大影响"></a>自招政策的收紧带来的三大影响</h3><p>可见, 今年的呃自主招生政策较往年相比大幅收紧, 对硬性指标的要求更加明确, 曾经常见的”其他具有学科特长和创新潜质”等条件都不再作为申请条件. 这将会给今年的自招带来至少以下几个影响. </p>
<ol>
<li><p>一些把自招当作捷径的考生将寸步难行</p>
<p>自招不再是一个用来逃避高考的捷径，自招的硬性指标要求更高，对于一些特定的学科特长考核更加严格。对于硬性指标不过硬，没有相应的学科竞赛获奖，或者体质不达标的考生，在这样的政策下将寸步难行。而希图利用一些无良中介机构进行论文和专利造假的考生，在这样的政策下也将举步维艰。</p>
</li>
<li><p>文科生在自招中的路越来越窄</p>
<p>许多高校的招生政策明确五大学科竞赛成为唯一的申请条件的情况下，除非文科生在这某个学科竞赛上有较强的学科特长的体现，否则将难以在这样明显偏向理科的门槛上取得优势。</p>
</li>
<li><p><strong>各种软性比赛，其他社会实践等软性能力的体现对后期竞争更为重要</strong></p>
<p>硬性指标门槛的提高，使得通过门槛的竞争者的差距急剧缩小，在基本的硬性指标的竞争力之外，各种软性指标的竞争力显得更为重要。在这种背景下，包括商赛、模联、机器人和各种社会实践活动，志愿者、义工活动等软性指标将成为重要的参考依据。当竞争者之间的其他条件相似的情况下，具有相关奖项和证书的竞争者无疑更具优势。</p>
</li>
</ol>
<p>总的来说, 在当今教育公平性愈显重要性的背景下, 自主招生作为高校在高考之外选拔具有特殊的学科特长和创新潜质的优秀学生的手段, 对申请者的基本学科能力等硬性指标和综合素质等软性指标都提出了更高的要求, 而作为申请者, 不仅要注重提高硬性指标以达到门槛, 更重要的是在综合素质和能力等软性指标上为自己积累竞争力, 才能以足够充分的准备面对新形势下的自主招生. </p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>自主招生</tag>
      </tags>
  </entry>
  <entry>
    <title>C++迭代器</title>
    <url>/Tuorial/C++%E8%BF%AD%E4%BB%A3%E5%99%A8.html</url>
    <content><![CDATA[<p>我真的不知道我之前写的那个迭代器的总结去哪里了…实在不行就再写一遍呗…</p>
<a id="more"></a>

<h3 id="什么是迭代器-为什么要有迭代器"><a href="#什么是迭代器-为什么要有迭代器" class="headerlink" title="什么是迭代器, 为什么要有迭代器"></a>什么是迭代器, 为什么要有迭代器</h3><p>​    STL提供了很多种容器, 像什么array, vector之类的, 迭代器是能够提供一个统一的操作接口, 虽然不同容器的迭代器对象本身不同, 但暴露出来的接口是一样的, 可以通过同样的接口对其进行同样(至少是相似)的操作. </p>
<h3 id="迭代器的分类"><a href="#迭代器的分类" class="headerlink" title="迭代器的分类"></a>迭代器的分类</h3><p>迭代器的动作分成读和写两种, 其实针对容器来说就是输入和输出两种, 一种是输入迭代器, 一种是输出迭代器. 输入迭代器的输入是指针对容器来说的, 往容器里写数据叫输入, 从容器往程序里输出数据叫输出. </p>
<p>此外, 根据迭代器的迭代方向, 可以分为正向迭代器, 反向迭代器, 随机访问迭代器, 双向迭代器几种不同的类型. 其中, 输入迭代器是典型的单项迭代器, 它只能递增但是不能倒退. 输入出迭代器也是单通行的. 双向迭代器则是可以具有正向迭代器所有特性的同时, 附加支持了两种递减运算, 即 <code>++i</code> 和 <code>i++</code> . </p>
<p>通常来说, 比较常用的是随机访问迭代器, 它具有双向迭代器所有的特性, 同时支持了随机访问的操作, 比如指针增加的运算, 以及对于元素进行排序的关系运算符. </p>
<p>到这儿你会发现, 迭代器其实是分三六九等的, 最高级的是随机访问迭代器, 它具备前述所有迭代器的所有功能的同时, 支持了一些其他类型迭代器不支持的功能. 当然, 这样做的代价自然是开销比较大. 如果你的程序对性能过于敏感, 则需要考虑这些区别, 否则可以直接使用随机访问迭代器. </p>
<p>各种迭代器的类型并不是确定的, 只是一种概念性的描述. 在具体的实现上, 是在每个容器类都定义了一个类级 <code>typedef</code> 的名称 <code>iterator</code> . 比如 <code>vector&lt;int&gt;</code> 类的迭代器类型就是 <code>vector&lt;int&gt;::iterator</code>. 然而, 这个类的文档也指出, <code>vector</code> 的迭代器是随机访问迭代器, 它允许使用基于任何迭代器类型的算法. 同样, <code>list&lt;int&gt;</code> 的迭代器就是 <code>list&lt;int&gt;::iterator</code>.  再比如说, STL写了一个双向链表, 这个双向链表具备一个双向迭代器, 但是这个迭代器不能使用基于随机访问的算法, 但是可以使用双向迭代器支持的算法. </p>
<h3 id="常见的迭代器模型"><a href="#常见的迭代器模型" class="headerlink" title="常见的迭代器模型"></a>常见的迭代器模型</h3><h4 id="1-将指针用作迭代器"><a href="#1-将指针用作迭代器" class="headerlink" title="1. 将指针用作迭代器"></a>1. 将指针用作迭代器</h4><p>迭代器其实就是广义的指针, 而且指针也满足了迭代器的所有要求. 因为迭代器是STL算法的接口(interface), 而指针是迭代器, 因此, STL算法可以使用指着来对基于指针的非STL容器进行操作. 这样做可以把STL写好的算法用在自己的一些数据结构上, 而不仅仅是STL容器. 比如STL算法可以用于数组, 用 <code>sort()</code> 进行排序. </p>
<p><code>sort()</code> 接受指向容器第一个元素的迭代器和指向超尾的迭代器作为参数. 我们现在假设有一个 <code>double</code> 型的数组叫 <code>Receipts</code> , 我们拿它作为例子, 进行升序排序: </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> SIZE = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">double</span> Recipts[SIZE];</span><br></pre></td></tr></table></figure>

<p>现在找出你想传进去的参数, 分别是 <code>&amp;Reciptes[0]</code> 和 <code>&amp;Reciptes[SIZE]</code> , 因此你可以按照这个函数的调用来对它进行排序: </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">sort(Reciptes, Reciptes[SIZE]);</span><br></pre></td></tr></table></figure>

<p>当然, 你也可以用其他的STL函数, 比如 <code>copy()</code> , 甚至你可以把你的数组<code>copy()</code>到屏幕上, 只需要把它copy给<code>ostream_iterator</code> 就行. </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span></span></span><br><span class="line">ostream_iterator&lt;int, char&gt; out_iter(cout, " ");</span><br><span class="line">copy(dice.<span class="built_in">begin</span>(), dice.<span class="built_in">end</span>(), out_iter);</span><br></pre></td></tr></table></figure>



<h4 id="2-其他有用的迭代器"><a href="#2-其他有用的迭代器" class="headerlink" title="2. 其他有用的迭代器"></a>2. 其他有用的迭代器</h4><p>除了 <code>ostream_iterator</code>, 头文件 <code>&lt;iterator&gt;</code>还提供了其他的一些预定义迭代器类型, 比如<code>reverse_iterator</code>, <code>back_insert_iterator</code>, <code>front_insert_iterator</code>和<code>insert_iterator</code>. 这些各有千秋, 你可以找官方文档查看这些内容. </p>
]]></content>
      <categories>
        <category>Tuorial</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>libstd</tag>
      </tags>
  </entry>
  <entry>
    <title>Crazy Install  ROS melodic on Raspberry Pi 4 with Ubuntu Server 1910</title>
    <url>/Experience/Crazy%20Install%20%20ROS%20melodic%20on%20Raspberry%20Pi%204%20with%20Ubuntu%20Server%201910.html</url>
    <content><![CDATA[<p>Install 1910</p>
<a id="more"></a>

<p>However, the mouse and keyboard , actually the whole USB port are useless.</p>
<p>Install ROS melodic from source as the tutorial on the wiki.ros.org/melodic/installation/ubuntu. </p>
<p>When resolving Dependencies:</p>
<p>Some dependencies not found in the repository for eoan. </p>
<p>That’s ok to set the official repository ros-latest.list to bionic. </p>
<p>Install the dependences manually one by one.</p>
<p>Build the catkin workspace. </p>
<p>“console bridge” problem:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/ros/console_bridge.git</span><br><span class="line"><span class="built_in">cd</span> console_bridge</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>“boots lib” problem:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev</span><br></pre></td></tr></table></figure>

<p>“gtenst”problems:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">udo apt-get install libgtest-dev</span><br><span class="line">sudo apt-get install cmake <span class="comment"># install cmake</span></span><br><span class="line"><span class="built_in">cd</span> /usr/src/gtest</span><br><span class="line">sudo cmake CMakeLists.txt</span><br><span class="line">sudo make</span><br></pre></td></tr></table></figure>





<p>eigen3 probrems</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libeigen3-dev</span><br></pre></td></tr></table></figure>



<p>no module named sigconfig </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://www.riverbankcomputing.com/static/Downloads/sip/4.19/sip-4.19.tar.gz</span><br><span class="line">tar -xvfz sip-4.19.tar.gz</span><br><span class="line"><span class="built_in">cd</span> sip-4.19.tar.gz</span><br><span class="line">python configure.py</span><br><span class="line">sudo make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<p>empy:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install python-empy</span><br></pre></td></tr></table></figure>

<p>tinyxml2</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/leethomason/tinyxml2.git</span><br><span class="line"><span class="built_in">cd</span> tityxml.git</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>



<p>not found sudo LOG4CXX</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install liblog4cxx-dev</span><br></pre></td></tr></table></figure>

<p>tinyxml1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install tinyxml-dev</span><br></pre></td></tr></table></figure>



<p>tested: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">sudo apt-get install libfontconfig1-dev libdbus-1-dev libfreetype6-dev libudev-dev libicu-dev libsqlite3-dev libxslt1-dev libssl-dev libasound2-dev libavcodec-dev libavformat-dev libswscale-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev gstreamer-tools gstreamer0.10-plugins-good gstreamer0.10-plugins-bad libraspberrypi-dev libpulse-dev libx11-dev libglib2.0-dev libcups2-dev freetds-dev libsqlite0-dev libpq-dev libiodbc2-dev libmysqlclient-dev firebird-dev libpng12-dev libjpeg9-dev libgst-dev libxext-dev libxcb1 libxcb1-dev</span><br><span class="line">libx11-xcb1 libx11-xcb-dev libxcb-keysyms1 libxcb-keysyms1-dev libxcb-image0 libxcb-image0-dev libxcb-shm0 libxcb-shm0-dev libxcb-icccm4 libxcb-icccm4-dev libxcb-sync1 libxcb-sync-dev libxcb-render-util0 libxcb-render-util0-dev libxcb-xfixes0-dev libxrender-dev libxcb-shape0-dev libxcb-randr0-dev libxcb-glx0-dev libxi-dev libdrm-dev libssl-dev libxcb-xinerama0 libxcb-xinerama0-dev</span><br></pre></td></tr></table></figure>





<p>qt5-widgets</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install qt5-default</span><br></pre></td></tr></table></figure>



<p>pyqt5</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install python-pyqt5</span><br></pre></td></tr></table></figure>





<p>‘SIP_NULLPTR’</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;www.riverbankcomputing.com&#x2F;static&#x2F;Downloads&#x2F;sip&#x2F;4.19&#x2F;sip-4.49.tar.gz</span><br><span class="line">tar -xvfz sip-4.49.tar.gz</span><br><span class="line">cd sip-4.49.tar.gz</span><br><span class="line">python configure.py</span><br><span class="line">sudo make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>



<p>lz4</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install python-lz4 python3-lz4</span><br></pre></td></tr></table></figure>

<p>it not use</p>
<p>try everything you can search in apt about lz4</p>
<p>seems lz4 and ros about would be work?</p>
<p>urdfdom_header:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt insatll liburdfdom-dev</span><br></pre></td></tr></table></figure>



<p>bzip2</p>
<p>try everything you can fine in <code>apt search</code> about bzip2</p>
<p>opencv</p>
<p>install everything about opencv, actually you will use it later them all. </p>
<p>“OGRE”</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libogre-1.9.0-dev libogre-1.9.0v5</span><br></pre></td></tr></table></figure>

<p>“yaml-cpp”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install libyaml-cpp-dev libyaml-cpp0.6</span><br></pre></td></tr></table></figure>

<p>asssmp:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libassimp-dev assimp-utils libassimp4 python-pyassimp python3-pyassimp</span><br></pre></td></tr></table></figure>





<p>After installation:</p>
<p>roscore:</p>
<p>netiface</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install netifaces</span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">sudo apt install python-netifaces python3-netifaces</span><br></pre></td></tr></table></figure>



<p>The main idea of debug the dependences is apt search the package name. </p>
]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Ubuntu</tag>
        <tag>Raspberry Pi</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World!</title>
    <url>/Other/Hello,%20World!.html</url>
    <content><![CDATA[<p>​    大家好, 我是Charlie, 是这个博客的创作人和维护人。</p>
<a id="more"></a>

<p>​    这个博客创建并于2019年6月8日发布第一篇文章《Hello，world！》，这是这个博客的第一篇文章，用于致敬支持我搭建起这个博客的所有的知识的发现者和创造者。</p>
<p>​    这是我的个人博客，纵然到了今天，已经不再有多少人拥有和使用博客，但我仍然坚信，在信息高度快餐化、碎片化的今天，一篇篇经过了深思熟虑的，或技术，或思考的文章，更能够承载起属于人类智慧的力量。</p>
<p>​    在此之前，我曾在我的云服务器上运行了我的第一个网站，用于服务于我高中阶段所致力于创建和维护的社团。如今，我会从此开始，走向更远的地方。</p>
<p>​    感谢每一个陪伴和关注的人。</p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Hello</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS安装时的raw.gihubusercontent.com连不上咋办# 标题</title>
    <url>/Experience/ROS%E5%AE%89%E8%A3%85%E6%97%B6%E7%9A%84raw.gihubusercontent.com%E8%BF%9E%E4%B8%8D%E4%B8%8A%E5%92%8B%E5%8A%9E.html</url>
    <content><![CDATA[<ol>
<li><p>连不上是因为一些众所周知的原因…</p>
</li>
<li><p>解决思路:</p>
<ol>
<li>科学上网</li>
<li>换一个能连上的</li>
</ol>
</li>
</ol>
<a id="more"></a>

<ol start="3">
<li><p>我自己搭建了一个镜像, 简单来说, 你可以直接吧域名<code>raw.githubusercontent.com</code>改成<code>mirrors.vankyle.cn</code>, 其他的可以照原样. </p>
</li>
<li><p>修改的地方: 有以下几个</p>
<ol>
<li>rosdistro下的<code>__init__.py</code>里定义了一个常量叫<code>DEFAULT_INDEX_URL</code></li>
<li>rosdistro下<code>github.py</code>下有两个函数, 一个<code>package_xml_in_parents</code>里面的<code>url</code>这个变量</li>
<li>还是这个文件下有个<code>_get_url_contents(url)</code>里面也是有个叫<code>url</code>的变量</li>
<li>rosdep下有三个<ol>
<li>gbpdistro_support.py的<code>FURTER_GBPDISTRO_URL</code></li>
<li>rep3.py的REP3_TARGETS_URL</li>
<li>sources_list.py的<code>DEFAULT_SOURCES_LIST_URL</code></li>
</ol>
</li>
</ol>
<p>以上这几个地方的<code>raw.githubusercontent.com</code>全部改成mirrors.vankyle.cn就可以了</p>
</li>
</ol>
<p>报这个错是因为<a href="https://raw.githubusercontent.com/ros/rosdistro/这个的库里的raw文件没办法直接下载下来" target="_blank" rel="noopener">https://raw.githubusercontent.com/ros/rosdistro/这个的库里的raw文件没办法直接下载下来</a>, 那我们就直接把这个库clone到一个我们可以下载的地方就好了.<br>所以我自己搭了一个镜像, 用起来有点麻烦, 但是可以解决问题. 简单来说, 就是把那个域名raw.githubusercontent.com改成我的镜像网址mirrors.vankyle.cn<br>具体的修改方法如下:<br>找到/usr/lib/python2.7/dist-packages/这个目录, 这个目录下有两个地方需要改:, 一个rosdep2, 一个rosdistro.<br>这两个地方下面有几个.py文件里定义了Url, 把那个Url里面的域名修改一下就可以了. 这几个URL的位置如下: </p>
<ol>
<li>rosdistro下的<code>__init__.py</code>里定义了一个常量叫<code>DEFAULT_INDEX_URL</code></li>
<li>rosdistro下<code>github.py</code>下有两个函数, 一个<code>package_xml_in_parents</code>里面的<code>url</code>这个变量</li>
<li>还是这个文件下有个<code>_get_url_contents(url)</code>里面也是有个叫<code>url</code>的变量</li>
<li>rosdep下有三个<ol>
<li>gbpdistro_support.py的<code>FURTER_GBPDISTRO_URL</code></li>
<li>rep3.py的<code>REP3_TARGETS_URL</code></li>
<li>sources_list.py的<code>DEFAULT_SOURCES_LIST_URL</code></li>
</ol>
</li>
</ol>
<p>如果你找到这几个文件, 就可以看见这几个url都是<a href="https://raw.githubusercontent.com/xxxxxx的" target="_blank" rel="noopener">https://raw.githubusercontent.com/xxxxxx的</a>, 把里面的raw.guthubusercontent.com改成我的镜像mirrors.vankyle.cn就可以了</p>
<p>提示: vim里面可以用:%s/被替换的内容/替换成的内容/g (g表示见到就替换, 全文都这样)来实现替换</p>
<p>毕竟是个人的土办法, 当然, 也可以科学x网, 或者换一个其他的靠谱的源. mirrors.vankyle.cn这个源是我自己搭建的, 可能不是很快, 但能用了. </p>
<p>仅供参考, 如果有帮助, 不胜荣幸. 如果有更好的办法, 欢迎讨论</p>
]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>在多台机器上运行ROS-理论篇</title>
    <url>/Tutorial/Run-ROS-on-Multiple-Machines.html</url>
    <content><![CDATA[<p>​    由于仿真需要, 准备在同一个局域网内开多个容器进行多机协同演练. 这需要在一个局域网内把多个机器的节点链接起来. 查阅官网的教程, 记录如下, 之后可能会补一个实操篇, 记录自己的操作流程.</p>
<a id="more"></a>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>ROS设计的灵魂就在于其分布式计算。一个优秀的节点不需要考虑在哪台机器上运行，它允许实时分配计算量以最大化的利用系统资源。(有一个特例——驱动节点必须运行在跟硬件设备有物理连接的机器上）。在多个机器人上使用ROS是一件很简单的事，你只需要记住一下几点：  </p>
<ul>
<li>你只需要一个master，只要在一个机器上运行它就可以了。 </li>
<li>所有节点都必须通过配置 <code>ROS_MASTER_URI</code>连接到同一个master。 </li>
<li>任意两台机器间任意两端口都必须要有完整的、双向连接的网络。(参考<a href="http://wiki.ros.org/ROS/NetworkSetup" target="_blank" rel="noopener">ROS/NetworkSetup</a>). </li>
<li>每台机器都必须向其他机器广播其能够解析的名字。(参考 <a href="http://wiki.ros.org/ROS/NetworkSetup" target="_blank" rel="noopener">ROS/NetworkSetup</a>). </li>
</ul>
<p>###　跨机器运行的 Talker / listener</p>
<p>假如说我们希望在两台机器上分别运行talker / listener， 主机名分别为 <strong>marvin</strong> 和 <strong>hal</strong>.登陆主机名为｀<code>marvin</code>的机器,你只要: </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh marvin@192.168.0.1</span><br></pre></td></tr></table></figure>

<p>同样的方法可以登陆<code>hal</code>. </p>
<p>你还需配置ROS_IP为当前的局域网ip地址。(利用ifconfig指令可以查看你当前的ip地址）。其次，很有可能你的主机名不能够被其他机器解析，所以保险的方法是利用 ssh hostname@local_ip的方式进行登陆(如<em>ssh <a href="mailto:turtlebot@192.168.1.100">turtlebot@192.168.1.100</a></em>)。再者，ROS_MASTER_URI最好也用运行master的那台机器的ip地址来替换主机名（如：<em>export ROS_MASTER_URI=<a href="http://192.168.1.100:11311" target="_blank" rel="noopener">http://192.168.1.100:11311</a></em>) </p>
<h3 id="启动-Listener"><a href="#启动-Listener" class="headerlink" title="启动 Listener"></a>启动 Listener</h3><p>在<code>hal</code>机器上启用Listener, 并且配置ROS_MASTER_URL来使用<code>hal</code>机器上的Master. 你也可以在其他机器上运行master。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh marvin@192.168.0.1</span><br><span class="line"><span class="built_in">export</span> ROS_MASTER_URL=http://hal:11311</span><br><span class="line">rosrun rospy_tutorial talker.py</span><br></pre></td></tr></table></figure>

<p>小惊喜: 现在你可以看到机器<strong>hal</strong>上的listener正在接收来自<strong>marvin</strong>机器上talker发布的消息。 </p>
<p>请注意，talker / listener启动的顺序是没有要求的， 唯一的要求就是master必须先于节点启动。 </p>
<h3 id="反向测试"><a href="#反向测试" class="headerlink" title="反向测试"></a>反向测试</h3><p>现在我们来尝试一下反向测试。终止talker和listener的运行，但仍然保留master在机器 <strong>hal</strong>上，然后让talker和listerner交换机器运行。 </p>
<p>首先，在机器<strong>marvin</strong>启动listerner: </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh marvin@192.168.0.1</span><br><span class="line"><span class="built_in">export</span> ROS_MASTER_URI=http://hal:11311</span><br><span class="line">rosrun rospy_tutorials listener.py</span><br></pre></td></tr></table></figure>

<p>然后在机器<strong>hal</strong>上启动talker: </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh hal@192.168.0.2</span><br><span class="line"><span class="built_in">export</span> ROS_MASTER_URI=http://hal:11311</span><br><span class="line">rosrun rospy_tutorials talker.py</span><br></pre></td></tr></table></figure>

<h3 id="运行出错"><a href="#运行出错" class="headerlink" title="运行出错"></a>运行出错</h3><p>如果没有取得如上预期的效果，那么很有可能是你的网络配置出错了。参考<a href="http://wiki.ros.org/ROS/NetworkSetup" target="_blank" rel="noopener">ROS/NetworkSetup</a>重新配置你的网络。 </p>
]]></content>
      <categories>
        <category>Tutorial</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>std::vector</title>
    <url>/Other/std-vector.html</url>
    <content><![CDATA[<p>今天做题的时候用到了<code>std::vector</code>, 这里记录一下，内容主要来自《C++标准库》这本书。</p>
<a id="more"></a>

<p>根据C++standard，vector是以dynamic array实现的。在使用之前需要包含头文件<code>&lt;vector&gt;</code></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br></pre></td></tr></table></figure>

<h3 id="大小和容量"><a href="#大小和容量" class="headerlink" title="大小和容量"></a>大小和容量</h3><p>Vector本身效率很高，但代价是需要分配出更大的空间。所以相比起一些手动算法，在空间控制上可能没那么理想。</p>
<p>想要获得当前vector内的元素个数，可以使用<code>size()</code>方法。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//std::vector&lt;int&gt; coll</span></span><br><span class="line">coll.<span class="built_in">size</span>()</span><br></pre></td></tr></table></figure>

<p>可以直接返回一个整数，其大小是这个vector内的元素个数。</p>
<p>如果这个vector是空的，则<code>coll.empty()</code>会返回<code>true</code>。</p>
<p>除此之外，还涉及一个问题就是一个vector的容量（Capacity）。特别是如果你需要使用频繁、大规模地使用Vector可以改变长度这个特性的话，尤其需要注意。因为vector在运行过程中可能会重新分配空间，这将导致地址发生变化，如果不做任何措施，原有的引用、指针和迭代器等都可能会失效。而且重新分配内存很耗时间。序偶一你需要好好考虑容量问题。</p>
<p>​    你可以使用<code>reserve()</code>来保留适当的容量以避免因容量不够而重新分配内存：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v;</span><br><span class="line">v.reserve(<span class="number">80</span>);<span class="comment">//reserve memroy for 80 elements</span></span><br></pre></td></tr></table></figure>

<p>但vector不能像string那样用<code>reserve()</code>来减小容量。vector的容量不能缩减(但长度可以缩减)。如果给<code>reserve()</code>的参数小于当前容量，什么也不会发生。</p>
<h3 id="几种比较常见的操作"><a href="#几种比较常见的操作" class="headerlink" title="几种比较常见的操作"></a>几种比较常见的操作</h3><p><code>c.empty()</code>: 返回是否为空</p>
<p><code>c.size()</code>：返回元素个数</p>
<p><code>c.max_size()</code>发挥元素个数之最大可能量</p>
<p><code>c.assign(n, elem)</code>: 复制n个elem赋给C</p>
<p><code>c.assign(begin, end)</code>: 将区间[begin, end)内的元素赋给c</p>
<p><code>c.assign(initlist)</code>: 用初始值列表给c赋值</p>
<p><code>c1.swap()</code>或<code>swap(c1, c2)</code>: 置换c1和c2的数据</p>
<h4 id="访问操作"><a href="#访问操作" class="headerlink" title="访问操作"></a>访问操作</h4><p><code>c[index]</code>(不检查范围)或<code>c.at(index)</code>（检查范围）</p>
<p><code>c.front()</code>返回首元素</p>
<p><code>c.back()</code>返回末尾元素</p>
<h3 id="迭代器相关"><a href="#迭代器相关" class="headerlink" title="迭代器相关"></a>迭代器相关</h3><p><code>c.begin()</code>, <code>c.end()</code>返回一个random-access iterator指向首、尾元素</p>
<p><code>c.cbegin()</code>, <code>c.cend()</code> 返回一个const ramdom-access iterator指向首、尾元素</p>
<p><code>c.rbegin()</code>, <code>c.rend()</code>, <code>c.crbegin()</code>, <code>c.crend()</code>返回反向迭代的首尾元素迭代器</p>
<h4 id="利用迭代器的一些操作"><a href="#利用迭代器的一些操作" class="headerlink" title="利用迭代器的一些操作"></a>利用迭代器的一些操作</h4><p><code>c.push_back(elem)</code>: 附加一个elem拷贝于末尾</p>
<p><code>c.pop_back()</code>: …上面的逆操作</p>
<p><code>c.insert()</code>支持以下参数列表：</p>
<p><code>pos, elem</code></p>
<p><code>pos, n, elem</code></p>
<p><code>pos, begin, end</code></p>
<p><code>pos, initlist</code></p>
<p><code>c.emplace(pos, args...)</code></p>
<p><code>e.emplace_back(args...)</code></p>
<p><code>c.erase(pos)</code>: 移除pos这个iterator上的元素</p>
<p><code>e.erase(begin, end)</code>： 移除从begin到end中间所有的元素</p>
<p>所以如果你想移除“与某个值相等”的元素，虽然vector没有直接提供，但是可以通过其他一些辅助函数来实现。比如，你可以通过下面这个语句将所有值为val的元素移除</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;//提供remove函数的声明</span></span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;elem&gt; coll;</span><br><span class="line">...</span><br><span class="line"><span class="comment">//remove al elements with value val</span></span><br><span class="line">coll.erase(<span class="built_in">remove</span>(coll.<span class="built_in">begin</span>(), coll.<span class="built_in">end</span>(), val), coll.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure>

<p>如果你想只删除值一样的第一个元素：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;elem&gt; coll;</span><br><span class="line">...</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;elem&gt;::iterator pos;</span><br><span class="line">pos=<span class="built_in">find</span>(coll.<span class="built_in">begin</span>(), coll.<span class="built_in">end</span>(), val);</span><br><span class="line"><span class="keyword">if</span>(pos!=coll.<span class="built_in">end</span>())</span><br><span class="line">&#123;</span><br><span class="line">    coll.erase(pos);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>###　关于Vector的异常处理</p>
<p>除非用户自定义了新的异常，或者标准异常（比如<code>bad_alloc</code>), <code>c.at()</code>是唯一一个标准认可的可以抛出异常的函数. 其他的函数，基本都不会抛出异常，甚至标准还保证类似<code>push_back()</code>这样的方法绝对不会抛出异常。</p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>libstd</tag>
      </tags>
  </entry>
  <entry>
    <title>关于我们这个机器人队</title>
    <url>/Publicity/%E5%85%B3%E4%BA%8E%E6%88%91%E4%BB%AC%E8%BF%99%E4%B8%AA%E6%9C%BA%E5%99%A8%E4%BA%BA%E9%98%9F.html</url>
    <content><![CDATA[<p>TUF到底是什么样的一群人</p>
<a id="more"></a>
<h3 id="关于我们这个机器人队"><a href="#关于我们这个机器人队" class="headerlink" title="关于我们这个机器人队"></a>关于我们这个机器人队</h3><h4 id="一"><a href="#一" class="headerlink" title="一"></a>一</h4><p>​    首先希望大家明白一点, 机器人并不是某个单一领域的事情, 而是一个高度综合, 学科交叉, 涉猎广泛的一个项目, 以至于独立成为了一个学科叫做<strong>机器人学</strong>(Robotics). 这个领域涉猎如此之广, 以至于单一的计算机, 或智能, 或自动化, 或软件工程, 或电子, 都不能独当一面地完成一个机器人比赛的全部工作. </p>
<p>​    其次, 至少以我个人的经历来看, 我对这个领域了解得越多, 学习得越深入, 越觉得这个领域知识树之广袤高深, 难以遍历.</p>
<p>​    而我们这样一个队伍, 在全国范围内, 实在是十分年轻的一个队伍, 以至于我们即将开展的很多工作, 既是在开创开创我们的道路, 也是在追赶其他队伍的步伐. 因此, 无论未来我们这个队伍的命运如何, 我都希望大家能够对我们即将开展的工作, 以及即将面对和学习的知识保有一颗敬畏之心, 正是那一番谦逊和求知的热情, 才是我们这个团队能够前进和进步的动力. </p>
<p>​    但值得庆幸的是, 我们这样一个队伍生来就带有工程师的光环, 在这样一个集体中我们不仅仅是一个”职业考生”, 更是用我们手握的各种知识和工具去解决实际问题的工程师. 我们在这样一个环境里真正地会将大家所学应用到实际中, 并在此基础上, 接触某些领域内比较高深的知识, 那些你们当前阶段, 乃至整个本科阶段都不会讲的知识, 那些老师们上课漏掉了的知识. </p>
<p>​    正如我所说, 我们所面对的是一整棵知识树, 我们也许不能将其遍历, 但我们可以在这个过程中, 尽可能多摘取一些果实. </p>
<h4 id="二"><a href="#二" class="headerlink" title="二"></a>二</h4><p>​    中国的学生从小都非常擅长一件事——考试。</p>
<p>​    老师和家长一直都告诉我们要好好学习，却从没有告诉我们什么是学习，学习和记忆的区别是什么，如何将学习学来的东西应用于我们的实践中。这也不奇怪，在过去的十多年里，同学们的生活是被一场一场考试分割开来的，你如果现在回想起你过去十数年的人生，无非就是这么几个阶段：上小学以前，考中学以前，中考以前，高考以前……让我们的这群孩子去用自己少有人生经历去面对人生中的诸多选择，真的是一件可恨又可悲的事情。</p>
<p>​    我们如今将大家招募过来并聚集在一起, 不仅是与大家分享工程师的光环, 抑或是跟大家共同完成一个机器人队的使命, 更是通过我们的方式, 帮助大家从考试这件事本身挣脱出来, 开始思考我们学到的东西会如何应用在我们的实际工程中, 并且通过理论去指导实践, 帮助我们在实践中能够以更高的效率实现我们的工作. </p>
<p>​    你们的老师也许告诉过你们C语言需要经过编译器编译才能运行, 但老师们未必告诉过你们常用的Code::Blocks使用的是GCC的Windows版本Min-GW, 也未必会告诉你们GCC-7默认使用的是C++14而GCC-5使用的是C++11. 老师们也许教过你们在Code::Blocks里点一下绿色的三角外加一个齿轮就可以让你的程序跑起来, 但未必告诉过你们要在Repository里面建立一个<code>Build</code>文件夹并在里面使用<code>cmake</code>来编译大型工程. 老师们也许告诉过你们如何求一个矩阵的特征值和特征向量, 但你们也许还不知道我们的机器人在进行状态估计的时候就是借用特征值和特征向量来解决各种各样的问题. </p>
<p>​    你们的老师也许会在大一的第二学期教你们如何用贝叶斯公式求一个事件的条件概率, 而我们就是要用同样的方式去估计机器人所处的状态和下个时刻机器人的决策有多大可能实现目的. </p>
<p>​    因此, 我需要提前跟大家说明白的是, 机器人的这些事情不会给你的考试成绩有立竿见影的提升, 也不会为你争取到更多的自习, 刷题的时间. 但我们希望能够做到把一群”不奢望, 不绝望, 永远怀着热望”的人聚集在一起, 并以不同的角度去理解我们的考试知识. </p>
<p>​    </p>
<h3 id="三"><a href="#三" class="headerlink" title="三"></a>三</h3><p>​    如果你问我, 来这个机器人队能收获什么, 我真的没办法像很多学生会部门那样跟大家保证会有什么样的收获, 我们不是一个官僚部门, 我们只是一支创新实验团队. </p>
<p>​    我们真的没有什么学霸带你飞, 最多也就是带你在大一大二就学完一大半的本科课程而已; </p>
<p>​    我们也没有什么保研出国秘籍, 顶天了就是给你一个世界冠军的荣耀而已;</p>
<p>​    我们也没有什么期末考试通关宝典, 也就是一群人一起相互讲题组队学习而已;</p>
<p>​    我们也没有什么团队福利, 也就是大家一起出去比个赛, 嗨一顿, 再回来接着干活儿而已;</p>
<p>​    我们也没有什么可以装逼的东西, 也就是别人一辈子都未必会接触到的知识, 和工程师的头衔而已. </p>
<p>​    作为一个年轻的团队, 我们几乎什么都没有, 我们所拥有的和即将拥有的, 都是我们一同创造的. </p>
<h3 id="四"><a href="#四" class="headerlink" title="四"></a>四</h3><p>​    说到底, 我们还是一个团队. </p>
<p>​    团队就意味着需要合作, 需要实现全局最优解, 而非局部最优解, 意味着需要共同面对困难, 共同承担风险, 也意味着共同分享荣誉, 共同分享收获. </p>
<p>​    我们不仅希望你足够牛X, 还希望你能够融入我们这个团队中, 为团队贡献力量的同时, 也在团队中成长. 我们即希望你能够听从指挥, 也希望你能够坚持自己的想法, 在必要的时候敢于提出问题, 敢于挑战权威, 敢于<strong>据理</strong>力争. </p>
<p>​    我们希望你能够按时按量完成任务, 同时能够”知识改变命运”, 用自己的聪明才智找到高效轻松地解决问题的方法. 我们希望你能够对不合理的要求说不, 并给出更好的办法. 我们还希望你能够帮助团队中的伙伴, 共同成长和进步.</p>
<p>​    我们希望你能够承担起作为一个团队成员的责任, 不求你挑大梁, 但求你千万别来划水. </p>
]]></content>
      <categories>
        <category>Publicity</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Robotics</tag>
        <tag>xmu.edu.cn</tag>
        <tag>xmu-tuf</tag>
      </tags>
  </entry>
  <entry>
    <title>小米路由器不安装客户端的访问方法</title>
    <url>/Experience/%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8%E4%B8%8D%E5%AE%89%E8%A3%85%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E8%AE%BF%E9%97%AE%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<ol>
<li>win10禁用了一个安全策略, 导致小米路由器的路由盘没办法访问. </li>
<li>调整这个策略的得放在<strong>本地组策略编辑器</strong>, 打开的方式是在<strong>运行</strong>里输入<code>gpedit.msc</code>.</li>
<li>找到<strong>计算机配置→管理模板→网络→Lanman工作站</strong>，把里面的“<strong>启用不安全的来宾登陆</strong>”项状态改为已启用, 即可访问</li>
<li>访问结束后, 记得关闭这个选项. 确保安全</li>
</ol>
]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>System</tag>
        <tag>Win10</tag>
        <tag>Config</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈ASP.Net Core 依赖关系注入</title>
    <url>/Tuorial/%E6%B5%85%E8%B0%88ASP.Net%20Core%20%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E6%B3%A8%E5%85%A5.html</url>
    <content><![CDATA[<p>事由是想写一个教材征订管理系统, 里面涉及到自行设计的一个购物车的数据模型类<code>Cart</code>和一个管理类<code>CartManager</code>, 遇到了相关问题, 所以仔细去读了官方的文档, 现在记录如下:</p>
<a id="more"></a>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h4 id="1-依赖项是什么"><a href="#1-依赖项是什么" class="headerlink" title="1. 依赖项是什么"></a>1. 依赖项是什么</h4><p>依赖项 , 是另一个对象所需的任何对象。</p>
<p>比如我的<code>CartManager</code>对象, 就需要依赖Cart对象, <code>ICollection</code>对象和<code>BookBill</code>对象等等.</p>
<h4 id="2-为什么要做依赖关系注入"><a href="#2-为什么要做依赖关系注入" class="headerlink" title="2. 为什么要做依赖关系注入"></a>2. 为什么要做依赖关系注入</h4><p>在这里, 官方文档举了一个更加简单的例子, 假设有一个被其他类依赖的<code>MyDependency</code>类, 其他类需要调用这个类的<code>WriteMessage</code>方法:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class MyDependency</span><br><span class="line">&#123;</span><br><span class="line">    public MyDependency()</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Task WriteMessage(string message)</span><br><span class="line">    &#123;</span><br><span class="line">        Console.WriteLine(</span><br><span class="line">            $&quot;MyDependency.WriteMessage called. Message: &#123;message&#125;&quot;);</span><br><span class="line"></span><br><span class="line">        return Task.FromResult(0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>稍微分析一下这个类, 除了一个什么都不做的构造函数之外, 还有一个打印消息的函数, 这个函数返回Task. </p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class IndexModel : PageModel</span><br><span class="line">&#123;</span><br><span class="line">    MyDependency _dependency &#x3D; new MyDependency();</span><br><span class="line"></span><br><span class="line">    public async Task OnGetAsync()</span><br><span class="line">    &#123;</span><br><span class="line">        await _dependency.WriteMessage(</span><br><span class="line">            &quot;IndexModel.OnGetAsync created this message.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接下来, 我们开始在一个页面的<code>PageModel</code>实例化一个<code>Mydependency</code>对象, 这个是一个常规操作. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class IndexModel : PageModel</span><br><span class="line">&#123;</span><br><span class="line">    MyDependency _dependency &#x3D; new MyDependency();</span><br><span class="line"></span><br><span class="line">    public async Task OnGetAsync()</span><br><span class="line">    &#123;</span><br><span class="line">        await _dependency.WriteMessage(</span><br><span class="line">            &quot;IndexModel.OnGetAsync created this message.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是这样做是有问题的: </p>
<ol>
<li><p>如果你需要修改<code>MyDependency</code>, 就必须在类的源代码里卖弄修改, 这个很可怕, 万一你还有别的地方用到了这个类, 那你修改就全改了, 会很麻烦. </p>
<pre><code>2. 如果`Mydependency`具有其他依赖关系, 就必须在`Mydependency`里面进行配置. 然后又有很多类需要依赖于`MyDpendency`,  这就导致这种代码到处都是, 很分散. 
   3. 如果你想单独测试这个部分, 你会发现现在这个架构很艰难. </code></pre></li>
</ol>
<p>为了解决这些问题, 我们通过依赖关系注入来实现这个类. 它可以做到以下几点:</p>
<ol>
<li>使用接口或者基类抽象化依赖关系的实现, 你的依赖关系是通过类继承来实现的, 你想换一个依赖, 继承另一个类就可以了.</li>
<li>可以同时注册服务容器中的依赖关系. ASP.NET Core 提供了一个内置服务容器<code>IServiceProvider</code>, 这个服务已经在应用的<code>Startup.ConfigureService</code>方法中注册. </li>
<li>将服务注入到使用它的类的构造函数中. 框架负责创建依赖的对象. 如果不再需要这个对象了, 就可以直接由框架进行处理. </li>
</ol>
<h3 id="如何依赖关系注入"><a href="#如何依赖关系注入" class="headerlink" title="如何依赖关系注入"></a>如何依赖关系注入</h3><p>按照依赖关系注入的思想, 当你需要自己做一个服务的时候 (比如我要做的这个<code>CartManager</code>), 你需要先写一个这个服务的接口(<code>interface</code>), 然后根据这个接口写一个类来实现这个服务, 之后再注册</p>
<p>所以我们可以这样来实现这个框架:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public interface IMyDependency</span><br><span class="line">&#123;</span><br><span class="line">    Task WriteMessage(string message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这是个<code>interface</code>, 这个<code>interface</code>. </p>
<p>这个接口里面的<code>WriteMessage</code>由一个<code>MyDependency</code>这个类的实现. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class MyDependency : IMyDependency</span><br><span class="line">&#123;</span><br><span class="line">    private readonly ILogger&lt;MyDependency&gt; _logger;</span><br><span class="line"></span><br><span class="line">    public MyDependency(ILogger&lt;MyDependency&gt; logger)</span><br><span class="line">    &#123;</span><br><span class="line">        _logger &#x3D; logger;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Task WriteMessage(string message)</span><br><span class="line">    &#123;</span><br><span class="line">        _logger.LogInformation(</span><br><span class="line">            &quot;MyDependency.WriteMessage called. Message: &#123;MESSAGE&#125;&quot;, </span><br><span class="line">            message);</span><br><span class="line"></span><br><span class="line">        return Task.FromResult(0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接下来, 你需要在<code>Startup.cs</code>中的服务注册里把你的服务注册进去: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void ConfigureServices(IServiceCollection services)</span><br><span class="line">&#123;</span><br><span class="line">    services.AddRazorPages();</span><br><span class="line"></span><br><span class="line">    services.AddScoped&lt;IMyDependency, MyDependency&gt;();&#x2F;&#x2F;这里是你注册的服务</span><br><span class="line">    </span><br><span class="line">    services.AddTransient&lt;IOperationTransient, Operation&gt;();</span><br><span class="line">    services.AddScoped&lt;IOperationScoped, Operation&gt;();</span><br><span class="line">    services.AddSingleton&lt;IOperationSingleton, Operation&gt;();</span><br><span class="line">    services.AddSingleton&lt;IOperationSingletonInstance&gt;(new Operation(Guid.Empty));</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; OperationService depends on each of the other Operation types.</span><br><span class="line">    services.AddTransient&lt;OperationService, OperationService&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用的注册函数根据你这个服务的生存期不同, 可以选择以下几种生存期</p>
<ul>
<li><p>暂时</p>
<p>暂时生存期服务 (<a href="https://docs.microsoft.com/zh-cn/dotnet/api/microsoft.extensions.dependencyinjection.servicecollectionserviceextensions.addtransient" target="_blank" rel="noopener">AddTransient</a>) 是每次从服务容器进行请求时创建的。 这种生存期适合轻量级、 无状态的服务。</p>
</li>
<li><p>范围内</p>
<p>作用域生存期服务 (<a href="https://docs.microsoft.com/zh-cn/dotnet/api/microsoft.extensions.dependencyinjection.servicecollectionserviceextensions.addscoped" target="_blank" rel="noopener">AddScoped</a>) 以每个客户端请求（连接）一次的方式创建。</p>
<p>注意: 在中间件内使用有作用域的服务时，请将该服务注入至 <code>Invoke</code> 或 <code>InvokeAsync</code> 方法。 请不要通过构造函数注入进行注入，因为它会强制服务的行为与单一实例类似。 这个在自定义中间件中会有专门的说明. </p>
</li>
<li><p>单例</p>
<p>单一实例生存期服务 (<a href="https://docs.microsoft.com/zh-cn/dotnet/api/microsoft.extensions.dependencyinjection.servicecollectionserviceextensions.addsingleton" target="_blank" rel="noopener">AddSingleton</a>) 是在第一次请求时（或者在运行 <code>Startup.ConfigureServices</code> 并且使用服务注册指定实例时）创建的。 每个后续请求都使用相同的实例。 如果应用需要单一实例行为，建议允许服务容器管理服务的生存期。 不要实现单一实例设计模式并提供用户代码来管理对象在类中的生存期。</p>
<p>注意: 从单一实例解析有作用域的服务很危险。 当处理后续请求时，它可能会导致服务处于不正确的状态。</p>
</li>
</ul>
<h3 id="依赖实现的配置"><a href="#依赖实现的配置" class="headerlink" title="依赖实现的配置"></a>依赖实现的配置</h3><p>当你的依赖(比如<code>MyDependency</code>) 需要ASP.Net内置的其他服务时, 可以直接用传参的方式传进来 因为其他的服务已经被注册过了. 但是如果你的构造函数需要内置类型(比如<code>string</code>), 你就没办法给<code>string</code>做服务注册. 这个时候你可以通过<code>Configuration</code>或者<code>Options</code>来注入这些类型: </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class MyDependency : IMyDependency</span><br><span class="line">&#123;</span><br><span class="line">    public MyDependency(IConfiguration config)</span><br><span class="line">    &#123;</span><br><span class="line">        var myStringValue &#x3D; config[&quot;MyStringKey&quot;];</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; Use myStringValue</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="大功告成"><a href="#大功告成" class="headerlink" title="大功告成"></a>大功告成</h3><p>你终于可以在页面中像使用其他框架提供的服务一样使用自己的服务了. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class IndexModel : PageModel</span><br><span class="line">&#123;</span><br><span class="line">    private readonly IMyDependency _myDependency;&#x2F;&#x2F;定义一个服务的私有变量</span><br><span class="line"></span><br><span class="line">    public IndexModel(</span><br><span class="line">        IMyDependency myDependency, &#x2F;&#x2F;构造函数中接受这个变量</span><br><span class="line">        OperationService operationService,</span><br><span class="line">        IOperationTransient transientOperation,</span><br><span class="line">        IOperationScoped scopedOperation,</span><br><span class="line">        IOperationSingleton singletonOperation,</span><br><span class="line">        IOperationSingletonInstance singletonInstanceOperation)</span><br><span class="line">    &#123;</span><br><span class="line">        _myDependency &#x3D; myDependency;&#x2F;&#x2F;赋值</span><br><span class="line">        OperationService &#x3D; operationService;</span><br><span class="line">        TransientOperation &#x3D; transientOperation;</span><br><span class="line">        ScopedOperation &#x3D; scopedOperation;</span><br><span class="line">        SingletonOperation &#x3D; singletonOperation;</span><br><span class="line">        SingletonInstanceOperation &#x3D; singletonInstanceOperation;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public OperationService OperationService &#123; get; &#125;</span><br><span class="line">    public IOperationTransient TransientOperation &#123; get; &#125;</span><br><span class="line">    public IOperationScoped ScopedOperation &#123; get; &#125;</span><br><span class="line">    public IOperationSingleton SingletonOperation &#123; get; &#125;</span><br><span class="line">    public IOperationSingletonInstance SingletonInstanceOperation &#123; get; &#125;</span><br><span class="line"></span><br><span class="line">    public async Task OnGetAsync()</span><br><span class="line">    &#123;</span><br><span class="line">        &#x2F;&#x2F;愉快地使用这个服务提供的方法</span><br><span class="line">        await _myDependency.WriteMessage(</span><br><span class="line">            &quot;IndexModel.OnGetAsync created this message.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="一些建议"><a href="#一些建议" class="headerlink" title="一些建议"></a>一些建议</h3><h4 id="如何从创建一个适合依赖关系注入的服务"><a href="#如何从创建一个适合依赖关系注入的服务" class="headerlink" title="如何从创建一个适合依赖关系注入的服务:"></a>如何从创建一个适合依赖关系注入的服务:</h4><p>最佳做法是：</p>
<ul>
<li>设计服务以使用依赖关系注入来获取其依赖关系。</li>
</ul>
<ul>
<li>避免有状态的、静态类和成员。 将应用设计为改用单一实例服务，可避免创建全局状态。</li>
<li>避免在服务中直接实例化依赖类。 直接实例化将代码耦合到特定实现。</li>
<li>不在应用类中包含过多内容，确保设计规范，并易于测试。</li>
</ul>
<p>如果一个类似乎有过多的注入依赖关系，这通常表明该类拥有过多的责任并且违反了<a href="https://docs.microsoft.com/zh-cn/dotnet/standard/modern-web-apps-azure-architecture/architectural-principles#single-responsibility" target="_blank" rel="noopener">单一责任原则 (SRP)</a>。 尝试通过将某些职责移动到一个新类来重构类。 请记住，Razor Pages 页模型类和 MVC 控制器类应关注用户界面问题。 业务规则和数据访问实现细节应保留在适用于这些<a href="https://docs.microsoft.com/zh-cn/dotnet/standard/modern-web-apps-azure-architecture/architectural-principles#separation-of-concerns" target="_blank" rel="noopener">分离的关注点</a>的类中。</p>
]]></content>
      <categories>
        <category>Tuorial</category>
      </categories>
      <tags>
        <tag>C#</tag>
        <tag>ASP.Net Core</tag>
        <tag>Micorsoft</tag>
      </tags>
  </entry>
  <entry>
    <title>Hyper-V 的增强会话能不能自己开?</title>
    <url>/Experience/Hyper-V%20%E7%9A%84%E5%A2%9E%E5%BC%BA%E4%BC%9A%E8%AF%9D%E8%83%BD%E4%B8%8D%E8%83%BD%E8%87%AA%E5%B7%B1%E5%BC%80.html</url>
    <content><![CDATA[<h1 id="Hyper-V-的增强会话能不能自己开"><a href="#Hyper-V-的增强会话能不能自己开" class="headerlink" title="Hyper-V 的增强会话能不能自己开?"></a>Hyper-V 的增强会话能不能自己开?</h1><p>可以</p>
<a id="more"></a>

<h3 id="啥"><a href="#啥" class="headerlink" title="啥?"></a>啥?</h3><p>微软买系统送了一个虚拟机平台叫Hyper-V, 其实这个东西跑起来性能比VirtualBox 啥的快, 还有docker支持. 但是问题在于, 很多必要的功能, 比如共享剪贴板, USB设备, 都需要Hyper-V的增强回话模式才能用. </p>
<p>其实用过之后你会发现, 这个增强会话模式其实就是用远程桌面的方式连接到虚拟机. 也就是要用到RDP协议来连接. 这对于Windows虚拟机当然是很香的, 但是对于Linux就很痛苦. </p>
<p>微软在Hyper-V管理器的”快速创建”这个栏目中是给了一些常用的镜像, 比如Ubuntu 18.04的镜像, 帮你配好了xrdp, 可以用来跟Linux进行RDP协议的链接. 人家给你配好了, 你直接用就可以了. </p>
<p>当然也不是没有别的问题. 下载太慢了. 而且是预配好的, 你自己可以定制的东西不多. 所以我自己研究了一下, 其实你自己装的ubuntu系统, 也是可以配置的, 而且人家微软也帮你写好了脚本, 就是有些地方跑起来不太灵光, 得自己弄一下就好了. </p>
<h3 id="搞"><a href="#搞" class="headerlink" title="搞!"></a>搞!</h3><p>首先找到微软这个VM-tool的git仓库, 里面有我们想要的配置脚本. 在<a href="https://github.com/microsoft/linux-vm-tools" target="_blank" rel="noopener">这儿</a>.</p>
<p>我们现在假定你装好了Ubuntu, 1804版本, 注意不管是18.04.2, 18.04.3, 18.04.4他们都不一样, 只不过大概可以通用. 其实后面的版本也差不多, 因为桌面环境没什么太大变化. 唯独就是16.04和18.04的桌面环境换了, 所以会有所区别. </p>
<p>现在你可以用你熟悉的git把它的脚本clone下来</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/microsoft/linux-vm-tools</span><br></pre></td></tr></table></figure>

<p>然后就是跑一遍<code>linux-vm-tools/18.04/instal.sh</code>这个脚本, 跑之前记得先给它加个运行的权限: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo chmod +x ./install.sh</span><br><span class="line">sudo ./install.sh</span><br></pre></td></tr></table></figure>

<p>这时候微软已经觉得OK了, 但其实还不行, 你看到它让你先重启一轮, 你就先重启</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>

<p>然后再跑一次</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ./install.sh</span><br></pre></td></tr></table></figure>

<p>然后再关机</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shutdown now</span><br></pre></td></tr></table></figure>

<p>现在你可以用powershell设定一下给这个虚拟机用上增强会话的RDP协议了: </p>
<p>用Powershell(管理员)跑一下:</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Set-VM</span> <span class="literal">-VMName</span> <span class="string">'你的虚拟机名称'</span> <span class="literal">-EnhancedSessionTransportType</span> HvSocket</span><br><span class="line"><span class="comment">#设置完成之后可以通过以下命令查看是否设置成功</span></span><br><span class="line">(<span class="built_in">Get-VM</span> <span class="literal">-VMName</span> <span class="string">'你的虚拟机名称'</span>).EnhancedSessionTransportType</span><br></pre></td></tr></table></figure>

<p>这个时候, 按道理你可以用上增强会话了, 但其实还不行, 根据<a href="http://c-nergy.be/blog/?p=13390" target="_blank" rel="noopener">这篇文章</a>, 你还是会遇到登录不了的问题. </p>
<p>这个时候你需要这样: </p>
<p>先装一个依赖: </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xserver-xorg-core</span><br></pre></td></tr></table></figure>

<p>再补上这个依赖挤掉的一些包, 因为据说这个依赖安装上去的时候会导致有些包被挤掉, 然后会导致鼠标用不了, 然鹅我并没有遇到这个问题, 还是装上吧. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get -y install xserver-xorg-input-all</span><br></pre></td></tr></table></figure>

<p>安装缺少的依赖项之后，需要手动安装xorgxrdp软件包以恢复xRDP功能 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get install xorgxrdp</span><br></pre></td></tr></table></figure>

<p>现在你终于可以愉快地用增强会话来连接了. </p>
]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Windows</tag>
        <tag>Hyper-V</tag>
        <tag>Virtualizations</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>[Repost] Ubuntu 安装 VirtualBox 启用安全启动的方案</title>
    <url>/Repost/Ubuntu%20%E5%AE%89%E8%A3%85%20VirtualBox%20%E5%90%AF%E7%94%A8%E5%AE%89%E5%85%A8%E5%90%AF%E5%8A%A8%E7%9A%84%E6%96%B9%E6%A1%88.html</url>
    <content><![CDATA[<h1 id="Ubuntu-安装-VirtualBox-启用安全启动的方案"><a href="#Ubuntu-安装-VirtualBox-启用安全启动的方案" class="headerlink" title="Ubuntu 安装 VirtualBox 启用安全启动的方案"></a>Ubuntu 安装 VirtualBox 启用安全启动的方案</h1><a id="more"></a>

<h1 id="VirtualBox-Secure-Boot-Ubuntu-fail"><a href="#VirtualBox-Secure-Boot-Ubuntu-fail" class="headerlink" title="VirtualBox + Secure Boot + Ubuntu = fail"></a>VirtualBox + Secure Boot + Ubuntu = fail</h1><p>​    </p>
<p>Posted by<a href="https://stegard.net/author/oyvind/" target="_blank" rel="noopener">Øyvind Stegard</a><a href="https://stegard.net/2016/10/virtualbox-secure-boot-ubuntu-fail/" target="_blank" rel="noopener">14. October 2016</a>             </p>
<p><a href="https://stegard.net/2016/10/virtualbox-secure-boot-ubuntu-fail/#respond" target="_blank" rel="noopener">Leave a comment on VirtualBox + Secure Boot + Ubuntu = fail</a>     </p>
<p>Here are the steps I did to enable VirtualBox to work properly in  Ubuntu with UEFI Secure Boot fully enabled<em>. The problem is the  requirement that all kernel modules must be signed by a key trusted by  the UEFI system, otherwise loading will fail. Ubuntu does *not sign</em> the third party vbox* kernel modules, but rather gives the user the  option to disable Secure Boot upon installation of the virtualbox  package. I could do that, but then I would see an annoying “Booting in  insecure mode” message every time the machine starts, and also the dual  boot Windows 10 installation I have would not function.</p>
<p><em>*Ubuntu 16.04 on a Dell Latitude E7440 with BIOS A18, and with a dual boot Windows 10 installation.</em></p>
<p>Credit goes to the primary source of information I used to resolve this problem, which applies specifically to Fedora/Redhat:<br> <a href="http://gorka.eguileor.com/vbox-vmware-in-secureboot-linux-2016-update/" target="_blank" rel="noopener">http://gorka.eguileor.com/vbox-vmware-in-secureboot-linux-2016-update/</a></p>
<p>And a relevant Ask Ubuntu question:<br> <a href="http://askubuntu.com/questions/760671/could-not-load-vboxdrv-after-upgrade-to-ubuntu-16-04-and-i-want-to-keep-secur" target="_blank" rel="noopener">http://askubuntu.com/questions/760671/could-not-load-vboxdrv-after-upgrade-to-ubuntu-16-04-and-i-want-to-keep-secur</a></p>
<h3 id="Steps-to-make-it-work-specifically-for-Ubuntu-Debian"><a href="#Steps-to-make-it-work-specifically-for-Ubuntu-Debian" class="headerlink" title="Steps to make it work, specifically for Ubuntu/Debian"></a>Steps to make it work, specifically for Ubuntu/Debian</h3><ol>
<li><p>Install the virtualbox package. If the installation detects that  Secure Boot is enabled, you will be presented with the issue at hand and given the option to disable Secure Boot. Choose <em>“No”</em>.</p>
</li>
<li><p>Create a personal public/private RSA key pair which will be used to  sign kernel modules. I chose to use the root account and the directory </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;root&#x2F;module-signing&#x2F;</span><br></pre></td></tr></table></figure>

<p> to store all things related to signing kernel modules.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo -i</span><br><span class="line"># mkdir &#x2F;root&#x2F;module-signing</span><br><span class="line"># cd &#x2F;root&#x2F;module-signing</span><br><span class="line"># openssl req -new -x509 -newkey rsa:2048 -keyout MOK.priv -outform DER -out MOK.der -nodes -days 36500 -subj &quot;&#x2F;CN&#x3D;YOUR_NAME&#x2F;&quot;</span><br><span class="line">[...]</span><br><span class="line"># chmod 600 MOK.priv</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use the MOK (“Machine Owner Key”) utility to import the public key  so that it can be trusted by the system. This is a two step process  where the key is first imported, and then later must be enrolled when  the machine is booted the next time. A simple password is good enough,  as it is only for temporary use.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mokutil --import &#x2F;root&#x2F;module-signing&#x2F;MOK.der</span><br><span class="line">input password:</span><br><span class="line">input password again:</span><br></pre></td></tr></table></figure>
</li>
<li><p>Reboot the machine. When the bootloader starts, the MOK manager EFI  utility should automatically start. It will ask for parts of the  password supplied in step 3. Choose to <em>“Enroll MOK”</em>, then you  should see the key imported in step 3. Complete the enrollment steps,  then continue with the boot. The Linux kernel will log the keys that are loaded, and you should be able to see your own key with the command: <code>dmesg|grep &#39;EFI: Loaded cert&#39;</code></p>
</li>
<li><p>Using a signing utility shippped with the kernel build files, sign  all the VirtualBox modules using the private MOK key generated in step  2. I put this in a small script </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;root&#x2F;module-signing&#x2F;sign-vbox-modules</span><br></pre></td></tr></table></figure>

<p>, so it can be easily run when new kernels are installed as part of regular updates:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">for modfile in $(dirname $(modinfo -n vboxdrv))&#x2F;*.ko; do</span><br><span class="line">  echo &quot;Signing $modfile&quot;</span><br><span class="line">  &#x2F;usr&#x2F;src&#x2F;linux-headers-$(uname -r)&#x2F;scripts&#x2F;sign-file sha256 \</span><br><span class="line">                                &#x2F;root&#x2F;module-signing&#x2F;MOK.priv \</span><br><span class="line">                                &#x2F;root&#x2F;module-signing&#x2F;MOK.der &quot;$modfile&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># chmod 700 &#x2F;root&#x2F;module-signing&#x2F;sign-vbox-modules</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run the script from step 5 as root. You will need to run the signing script every time a new kernel update is installed, since this will  cause a rebuild of the third party VirtualBox modules. Use the script  only after the new kernel has been booted, since it relies on <code>modinfo -n</code> and <code>uname -r</code> to tell which kernel version to sign for.</p>
</li>
<li><p>Load vboxdrv module and fire up VirtualBox:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># modprobe vboxdrv</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>The procedure can also be used to sign other third party kernel  modules, like the nvidia graphics drivers, if so is required. (I have  not tested that myself.)</p>
]]></content>
      <categories>
        <category>Repost</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
        <tag>VirtualBox</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Verilog 设计实现FGPA上的Mealy状态机</title>
    <url>/Experience/%E4%BD%BF%E7%94%A8%20Verilog%20%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0FGPA%E4%B8%8A%E7%9A%84Mealy%E7%8A%B6%E6%80%81%E6%9C%BA.html</url>
    <content><![CDATA[<h3 id="Mealy状态机的设计方法"><a href="#Mealy状态机的设计方法" class="headerlink" title="Mealy状态机的设计方法"></a>Mealy状态机的设计方法</h3><a id="more"></a>

<p>​    用Mealy状态机设计1101序列检测器的状态转换图如图所示:</p>
<p>![Mealy状态机状态转换图]</p>
<p>Mealy状态机设计1101序列检测的Verilog代码:</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> seqdetb(</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clk,</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clr,</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> din,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> dout</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">1</span>:<span class="number">0</span>] present_state, next_state;</span><br><span class="line">    <span class="keyword">parameter</span> S0=<span class="number">3'b00</span>, S1=<span class="number">3'b01</span>,S2=<span class="number">3'b10</span>,S3=<span class="number">3'b11</span>;</span><br><span class="line">    <span class="comment">//状态寄存器</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> clk <span class="keyword">or</span> <span class="keyword">posedge</span> clr)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span> (clr==<span class="number">1</span>)</span><br><span class="line">                present_state &lt;= S0;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                present_state &lt;= next_state;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="comment">//C1模块</span></span><br><span class="line">        <span class="keyword">always</span>@ (*)</span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                <span class="keyword">case</span>(present_state)</span><br><span class="line">                    S0: <span class="keyword">if</span>(din == <span class="number">1</span>)</span><br><span class="line">                            next_state &lt;= S1;</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            next_state &lt;= S0;</span><br><span class="line">                    S1: <span class="keyword">if</span>(din == <span class="number">1</span>)</span><br><span class="line">                            next_state &lt;= S2;</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            next_state &lt;= S0;</span><br><span class="line">                    S2: <span class="keyword">if</span>(din == <span class="number">0</span>)</span><br><span class="line">                            next_state &lt;= S3;</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            next_state &lt;= S2;</span><br><span class="line">                    S3: <span class="keyword">if</span>(din == <span class="number">1</span>)</span><br><span class="line">                            next_state &lt;= S1;</span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            next_state &lt;= S0;</span><br><span class="line">                    <span class="keyword">default</span></span><br><span class="line">                        next_state &lt;= S0;</span><br><span class="line">                <span class="keyword">endcase</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//C2模块</span></span><br><span class="line">        <span class="keyword">always</span> @ (<span class="keyword">posedge</span> clk <span class="keyword">or</span> <span class="keyword">posedge</span> clr)</span><br><span class="line">            <span class="keyword">begin</span></span><br><span class="line">                <span class="keyword">if</span>(clr==<span class="number">1</span>)</span><br><span class="line">                    dout &lt;= <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>( (present_state == S3) &amp;&amp; (din == <span class="number">1</span>))</span><br><span class="line">                    dout &lt;=<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    dout &lt;= <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">end</span>    </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p>针对这个设计代码, 可以编写如下的测试文件用于行为仿真:</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> seqdetb_test(</span><br><span class="line"></span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">reg</span> clk;</span><br><span class="line">    <span class="keyword">reg</span> clr;</span><br><span class="line">    <span class="keyword">reg</span> din;</span><br><span class="line">    <span class="keyword">wire</span> dout;</span><br><span class="line">    <span class="keyword">parameter</span> period = <span class="number">100</span>;</span><br><span class="line">    seqdetb seqdetb1(</span><br><span class="line">    <span class="variable">.clk</span>(clk),</span><br><span class="line">    <span class="variable">.clr</span>(clr),</span><br><span class="line">    <span class="variable">.din</span>(din),</span><br><span class="line">    <span class="variable">.dout</span>(dout)</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">wire</span> present_state;</span><br><span class="line">    <span class="keyword">wire</span> next_state;</span><br><span class="line">    <span class="keyword">initial</span> </span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            clk = <span class="number">0</span>;</span><br><span class="line">            clr = <span class="number">1</span>;</span><br><span class="line">            din = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">always</span> <span class="variable">#(period/2)</span> </span><br><span class="line">        <span class="keyword">begin</span> </span><br><span class="line">            clk = ~clk;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">always</span> <span class="variable">#(period*2)</span></span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            din=~din;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">always</span> #period</span><br><span class="line">        <span class="keyword">begin</span> </span><br><span class="line">            clr=~clr;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p>仿真获得的测试波形如下: </p>
<p><img src="https://image-vankyle-1257862518.cos.ap-chongqing.myqcloud.com/github/seqdetb_test.png" alt="seqdetb_test"></p>
<p>对工程进行综合后, 可以成功生成BitStream文件.</p>
<h3 id="结合状态机实现交通信号灯"><a href="#结合状态机实现交通信号灯" class="headerlink" title="结合状态机实现交通信号灯"></a>结合状态机实现交通信号灯</h3><p>通过状态机来实现一个交通信号灯, 状态转换表如下: </p>
<table>
<thead>
<tr>
<th>状态</th>
<th>南北方向</th>
<th>东西方向</th>
<th>延迟/s</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>绿</td>
<td>红</td>
<td>5</td>
</tr>
<tr>
<td>1</td>
<td>黄</td>
<td>红</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>红</td>
<td>红</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>红</td>
<td>绿</td>
<td>5</td>
</tr>
<tr>
<td>4</td>
<td>红</td>
<td>黄</td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>红</td>
<td>红</td>
<td>1</td>
</tr>
</tbody></table>
<p>信号灯的实现程序如下: </p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> traffic(</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clk_3Hz,</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clr,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> [<span class="number">5</span>:<span class="number">0</span>] lights</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">2</span>:<span class="number">0</span>] state;</span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">3</span>:<span class="number">0</span>] count;</span><br><span class="line">    <span class="keyword">parameter</span> S0=<span class="number">3'b000</span>, S1=<span class="number">3'b001</span>, S2=<span class="number">3'b010</span>,<span class="comment">//states</span></span><br><span class="line">                S3=<span class="number">3'b011</span>, S4=<span class="number">3'b100</span>, S5=<span class="number">3'b101</span>;</span><br><span class="line">    <span class="keyword">parameter</span> SEC5=<span class="number">4'b1110</span>, SEC1=<span class="number">4'b0010</span>;</span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> clk_3Hz <span class="keyword">or</span> <span class="keyword">posedge</span> clr)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(clr==<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    state &lt;= S0;</span><br><span class="line">                    count &lt;= <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">case</span>(state)</span><br><span class="line">                    S0:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC5)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S0;</span><br><span class="line">                                count &lt;= count +<span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S1;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    S1:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC1)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;=S1;</span><br><span class="line">                                count &lt;= count +<span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S2;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    S2:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC1)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S2;</span><br><span class="line">                                count &lt;= count +<span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;=  S3;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    S3:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC1)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S3;</span><br><span class="line">                                count &lt;= count +<span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;=  S4;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    S4:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC1)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S4;</span><br><span class="line">                                count &lt;= count +<span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;=  S5;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    S5:</span><br><span class="line">                        <span class="keyword">if</span>(count&lt;SEC1)</span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;= S5;</span><br><span class="line">                                count &lt;= count + <span class="number">1</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="keyword">begin</span></span><br><span class="line">                                state &lt;=  S0;</span><br><span class="line">                                count &lt;= <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">end</span></span><br><span class="line">                    <span class="keyword">default</span> state &lt;= S0;</span><br><span class="line">                    <span class="keyword">endcase</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">always</span> @(*)</span><br><span class="line">                <span class="keyword">begin</span></span><br><span class="line">                    <span class="keyword">case</span>(state)</span><br><span class="line">                        S0: lights=<span class="number">6'b100001</span>;</span><br><span class="line">                        S1: lights=<span class="number">6'b100010</span>;</span><br><span class="line">                        S2: lights=<span class="number">6'b100100</span>;</span><br><span class="line">                        S3: lights=<span class="number">6'b001100</span>;</span><br><span class="line">                        S4: lights=<span class="number">6'b010100</span>;</span><br><span class="line">                        S5: lights=<span class="number">6'b100100</span>;</span><br><span class="line">                        <span class="keyword">default</span> lights=<span class="number">6'b100001</span>;</span><br><span class="line">                    <span class="keyword">endcase</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p>对信号灯这个部分进行仿真, 编写的仿真测试文件如下: </p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> traffic_test(</span><br><span class="line"></span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">reg</span> clk_3Hz;</span><br><span class="line">    <span class="keyword">reg</span> clr;</span><br><span class="line">    <span class="keyword">wire</span> [<span class="number">5</span>:<span class="number">0</span>] lights;</span><br><span class="line">    traffic theTraffic(</span><br><span class="line">        <span class="variable">.clk_3Hz</span>(clk_3Hz),</span><br><span class="line">        <span class="variable">.clr</span>(clr),</span><br><span class="line">        <span class="variable">.lights</span>(lights)</span><br><span class="line">        );</span><br><span class="line">    <span class="keyword">parameter</span> period = <span class="number">20</span>;</span><br><span class="line">    <span class="keyword">initial</span> </span><br><span class="line">        <span class="keyword">begin</span> </span><br><span class="line">        clk_3Hz=<span class="number">0</span>;</span><br><span class="line">        clr=<span class="number">1</span>;</span><br><span class="line">        #<span class="number">10</span>;</span><br><span class="line">        clr=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">always</span> # (period/<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            clk_3Hz=~clk_3Hz;</span><br><span class="line">        <span class="keyword">end</span> </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p><img src="https://image-vankyle-1257862518.cos.ap-chongqing.myqcloud.com/github/traffic_test.png" alt="traffic_test"></p>
<p>为了制造3Hz的时钟信号, 需要实现一个分频器</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> clkdiv(</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clk_100MHz,</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clr,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> clk_3Hz</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">reg</span> [<span class="number">24</span>:<span class="number">0</span>] q;</span><br><span class="line">    <span class="comment">//25bit counter</span></span><br><span class="line">    <span class="keyword">always</span> @ (<span class="keyword">posedge</span> clk_100MHz <span class="keyword">or</span> <span class="keyword">posedge</span> clr)</span><br><span class="line">        <span class="keyword">begin</span></span><br><span class="line">            <span class="keyword">if</span>(clr==<span class="number">1</span>)</span><br><span class="line">                q &lt;= <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                q &lt;= q + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">assign</span> clk_3Hz=q[<span class="number">24</span>] ;<span class="comment">//3Hz</span></span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p>最后用一个顶层模块将分频器和交通灯两个模块封装起来</p>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line"><span class="meta">`<span class="meta-keyword">timescale</span> 1ns / 1ps</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> traffic_lights_top(</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> clk_100MHz,</span><br><span class="line">    <span class="keyword">input</span> <span class="keyword">wire</span> [<span class="number">4</span>:<span class="number">4</span>] s,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">wire</span> [<span class="number">5</span>:<span class="number">0</span>] ld</span><br><span class="line">    );</span><br><span class="line">    <span class="keyword">wire</span> clr, clk_3Hz;</span><br><span class="line">    <span class="keyword">assign</span> clr=s;</span><br><span class="line">    clkdiv U1(<span class="variable">.clk_100MHz</span>(clk_100MHz),</span><br><span class="line">                <span class="variable">.clr</span>(clr),</span><br><span class="line">                <span class="variable">.clk_3Hz</span>(clk_3Hz)</span><br><span class="line">              );</span><br><span class="line">    traffic U2(<span class="variable">.clk_3Hz</span>(clk_3Hz),</span><br><span class="line">                <span class="variable">.clr</span>(clr),</span><br><span class="line">                <span class="variable">.lights</span>(ld)</span><br><span class="line">              );    </span><br><span class="line"><span class="keyword">endmodule</span></span><br></pre></td></tr></table></figure>

<p>经过综合, 可以生成BitStream文件</p>
]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>FGPA</tag>
        <tag>Verilog</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title>[Repost] CVonline: Image Databases</title>
    <url>/Repost/CVonline%20Image%20Databases.html</url>
    <content><![CDATA[<p><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/" target="_blank" rel="noopener"><img src="http://homepages.inf.ed.ac.uk/rbf/CVonline/cvlogo.gif" alt="img"></a></p>
<h1 id="CVonline-Image-Databases"><a href="#CVonline-Image-Databases" class="headerlink" title="CVonline: Image Databases"></a>CVonline: Image Databases</h1><a id="more"></a>

<hr>
<p>Source page: <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline" target="_blank" rel="noopener">http://homepages.inf.ed.ac.uk/rbf/CVonline</a></p>
<p>This is a collated list of image and video databases that people    have found useful for computer vision research and algorithm evaluation.</p>
<p>An important article    <a href="https://link.springer.com/article/10.1007/s11263-017-1020-z" target="_blank" rel="noopener">How Good Is My Test Data? Introducing Safety Analysis for Computer Vision</a>    (by Zendel, Murschitz, Humenberger, and Herzner)    introduces a methodology for ensuring that your dataset has sufficient    variety that algorithm results on the dataset are representative of    the results that one could expect in a real setting.    In particular, the team have produced a    <a href="https://vitro-testing.com/cv-hazop/" target="_blank" rel="noopener">Checklist</a> of potential    hazards (imaging situations) that may cause algorithms to have problems.    Ideally, test datasets should have examples of the relevant hazards.</p>
<h2 id="Index-by-Topic"><a href="#Index-by-Topic" class="headerlink" title="Index by Topic"></a>Index by Topic</h2><ol>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#action" target="_blank" rel="noopener">Action Databases</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#agriculture" target="_blank" rel="noopener">Agriculture</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#atre" target="_blank" rel="noopener">Attribute recognition</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#autodriving" target="_blank" rel="noopener">Autonomous Driving</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#biomed" target="_blank" rel="noopener">Biological/Medical</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#Caca" target="_blank" rel="noopener">Camera calibration</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#face" target="_blank" rel="noopener">Face and Eye/Iris Databases</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#fingerprints" target="_blank" rel="noopener">Fingerprints</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#images" target="_blank" rel="noopener">General Images</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#rgbd" target="_blank" rel="noopener">General RGBD and depth datasets</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#videos" target="_blank" rel="noopener">General Videos</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#gesture" target="_blank" rel="noopener">Hand, Hand Grasp, Hand Action and Gesture Databases</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#retrieve" target="_blank" rel="noopener">Image, Video and Shape Database Retrieval</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#object" target="_blank" rel="noopener">Object Databases</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#human" target="_blank" rel="noopener">People (static and dynamic), human body pose</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#people" target="_blank" rel="noopener">People Detection and Tracking Databases</a> (See also <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#surveillance" target="_blank" rel="noopener">Surveillance</a>)</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#remote" target="_blank" rel="noopener">Remote Sensing</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#robotics" target="_blank" rel="noopener">Robotics</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#scene" target="_blank" rel="noopener">Scenes or Places, Scene Segmentation or Classification</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation" target="_blank" rel="noopener">Segmentation</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#SLAM" target="_blank" rel="noopener">Simultaneous Localization and Mapping</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#surveillance" target="_blank" rel="noopener">Surveillance and Tracking</a> (See also <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#people" target="_blank" rel="noopener">People</a>)</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#texture" target="_blank" rel="noopener">Textures</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#urban" target="_blank" rel="noopener">Urban Datasets</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#language" target="_blank" rel="noopener">Vision and Natural Language</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#collect" target="_blank" rel="noopener">Other Collection Pages</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#misc" target="_blank" rel="noopener">Miscellaneous Topics</a></li>
</ol>
<p>Other helpful sites are:</p>
<ol>
<li><a href="http://academictorrents.com/collection/computer-vision" target="_blank" rel="noopener">Academic Torrents - computer vision</a> - a set of 30+ large datasets available in BitTorrent form</li>
<li><a href="https://www.datasetlist.com/" target="_blank" rel="noopener">Machine learning datasets</a> - see CV tab</li>
<li><a href="http://riemenschneider.hayko.at/vision/dataset/index.php" target="_blank" rel="noopener">YACVID</a> - a tagged index to some computer vision datasets</li>
</ol>
<hr>
<h2 id="Action-Databases"><a href="#Action-Databases" class="headerlink" title="Action Databases"></a>Action Databases</h2><p>See also:  <a href="http://www.actionrecognition.net" target="_blank" rel="noopener">Action Recognition’s</a> dataset summary with league tables (Gall, Kuehne, Bhattarai).</p>
<ol>
<li><a href="https://www.twentybn.com/datasets/something-something" target="_blank" rel="noopener">20bn-Something-Something</a> - densely-labeled video clips that show humans performing predefined  basic actions with everyday objects (Twenty Billion Neurons GmbH)  [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/skicyyu/rgbd_recognition" target="_blank" rel="noopener">3D online action dataset</a> - There are seven action categories (Microsoft and Nanyang Technological University) [Before 28/12/19]</li>
<li><a href="http://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/" target="_blank" rel="noopener">50 Salads</a> - fully annotated 4.5 hour dataset of RGB-D video + accelerometer data, capturing 25 people preparing two mixed salads each (Dundee University, Sebastian Stein) [Before 28/12/19]</li>
<li><a href="http://www.eecs.qmul.ac.uk/~andrea/fpvo" target="_blank" rel="noopener">A first-person vision dataset of office activities (FPVO)</a> - FPVO contains first-person video segments of office activities  collected using 12 participants. (G. Abebe, A. Catala, A. Cavallaro)  [Before 28/12/19]</li>
<li><a href="http://activity-net.org/" target="_blank" rel="noopener">ActivityNet</a> - A Large-Scale  Video Benchmark for Human Activity Understanding (200 classes, 100  videos per class, 648 video hours) (Heilbron, Escorcia, Ghanem and  Niebles) [Before 28/12/19]</li>
<li><a href="http://www.merl.com/demos/merl-shopping-dataset" target="_blank" rel="noopener">Action Detection in Videos</a> - MERL Shopping Dataset consists of 106 videos, each of which is a  sequence about 2 minutes long (Michael Jones, Tim Marks) [Before  28/12/19]</li>
<li><a href="http://web.eecs.umich.edu/~jjcorso/r/a2d/" target="_blank" rel="noopener">Actor and Action Dataset</a> - 3782 videos, seven classes of actors performing eight different actions (Xu, Hsieh, Xiong, Corso) [Before 28/12/19]</li>
<li><a href="http://www.cs.ubc.ca/~murphyk/videodata.html" target="_blank" rel="noopener">An analyzed collation of various labeled video datasets for action recognition</a> (Kevin Murphy) [Before 28/12/19]</li>
<li><a href="http://rtis.oit.unlv.edu/datasets.html" target="_blank" rel="noopener">AQA-7</a> - Dataset for assessing the quality of 7 different actions. It contains 1106  action samples and AQA scores. (Parmar, Morris) [29/12/19]</li>
<li><a href="http://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html" target="_blank" rel="noopener">ASLAN Action similarity labeling challenge</a> database (Orit Kliper-Gross) [Before 28/12/19]</li>
<li><a href="http://yanweifu.github.io/USAA/download/" target="_blank" rel="noopener">Attribute Learning for Understanding Unstructured Social Activity</a> - Database of videos containing 10 categories of unstructured social  events to recognise, also annotated with 69 attributes. (Y. Fu  Fudan/QMUL, T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/audiovisualresearch" target="_blank" rel="noopener">Audio-Visual Event (AVE) dataset</a>- AVE dataset contains 4143 YouTube videos covering 28 event categories  and videos in AVE dataset are temporally labeled with audio-visual event boundaries. (Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and  Chenliang Xu) [Before 28/12/19]</li>
<li><a href="https://research.google.com/ava/" target="_blank" rel="noopener">AVA: A Video Dataset of Atomic Visual Action</a>- 80 atomic visual actions in 430 15-minute movie clips. (Google Machine Perception Research Group) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/eccv2018bbdb/" target="_blank" rel="noopener">BBDB</a> -  Baseball Database (BBDB) is a large-scale baseball video dataset that  contains 4200 hours of full baseball game videos with 400,000 temporally annotated activity segments. (Shim, Minho, Young Hwi, Kyungmin, Kim,  Seon Joo) [Before 28/12/19]</li>
<li><a href="http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/index.html" target="_blank" rel="noopener">BEHAVE Interacting Person Video Data with markup</a> (Scott Blunsden, Bob Fisher, Aroosha Laghaee) [Before 28/12/19]</li>
<li><a href="http://cs-people.bu.edu/sbargal/BU-action/" target="_blank" rel="noopener">BU-action Datasets</a> - Three image action datasets (BU101, BU101-unfiltered,  BU203-unfiltered) that have 1:1 correspondence with classes of the video datasets UCF101 and ActivityNet. (S. Ma, S. A. Bargal, J. Zhang, L.  Sigal, S. Sclaroff.) [Before 28/12/19]</li>
<li><a href="http://tele-immersion.citris-uc.org/berkeley_mhad" target="_blank" rel="noopener">Berkeley MHAD: A Comprehensive Multimodal Human Action Database</a> (Ferda Ofli) [Before 28/12/19]</li>
<li><a href="http://tele-immersion.citris-uc.org/berkeley_mhad" target="_blank" rel="noopener">Berkeley Multimodal Human Action Database</a> - five different modalities to expand the fields of application  (University of California at Berkeley and Johns Hopkins University)  [Before 28/12/19]</li>
<li><a href="http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/" target="_blank" rel="noopener">Breakfast dataset</a> - It’s a dataset with 1712 video clips showing 10 kitchen activities,  which are hand segmented into 48 atomic action classes . (H. Kuehne, A.  B. Arslan and T. Serre ) [Before 28/12/19]</li>
<li><a href="https://www.cs.bris.ac.uk/~damen/BEOID/" target="_blank" rel="noopener">Bristol Egocentric Object Interactions Dataset</a> - Contains videos shot from a first-person (egocentric) point of view  of 3-5 users performing tasks in six different locations (Dima Damen,  Teesid Leelaswassuk and Walterio Mayol-Cuevas, Bristol University)  [Before 28/12/19]</li>
<li><a href="http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/" target="_blank" rel="noopener">Brown Breakfast Actions Dataset</a> - 70 hours, 4 million frames of 10 different breakfast preparation activities (Kuehne, Arslan and Serre) [Before 28/12/19]</li>
<li><a href="http://pr.cs.cornell.edu/humanactivities/data.php" target="_blank" rel="noopener">CAD-120 dataset</a> - focuses on high level activities and object interactions (Cornell University) [Before 28/12/19]</li>
<li><a href="http://pr.cs.cornell.edu/humanactivities/data.php" target="_blank" rel="noopener">CAD-60 dataset</a> - The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities (Cornell University) [Before 28/12/19]</li>
<li><a href="http://rohitgirdhar.github.io/CATER" target="_blank" rel="noopener">CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning</a> - A synthetic video understanding benchmark, with tasks that by-design  require temporal reasoning to be solved (Girdhar, Ramanan) [29/12/19]</li>
<li><a href="http://vision.fe.uni-lj.si/cvbase06/downloads.html" target="_blank" rel="noopener">CVBASE06: annotated sports videos</a> (Janez Pers) [Before 28/12/19]</li>
<li><a href="http://allenai.org/plato/charades/" target="_blank" rel="noopener">Charades Dataset</a> -  10,000 videos from 267 volunteers, each annotated with multiple  activities, captions, objects, and temporal localizations. (Sigurdsson,  Varol, Wang, Laptev, Farhadi, Gupta) [Before 28/12/19]</li>
<li><a href="http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/" target="_blank" rel="noopener">Composable activities dataset</a> - Different combinations of 26 atomic actions formed 16 activity  classes which were performed by 14 subjects and annotations were  provided (Pontificia Universidad Catolica de Chile and Universidad del  Norte) [Before 28/12/19]</li>
<li><a href="https://www.mica.edu.vn/perso/Tran-Thi-Thanh-Hai/CMDFALL.html" target="_blank" rel="noopener">Continuous Multimodal Multi-view Dataset of Human Fall</a> - The dataset consists of both normal daily activities and simulated  falls for evaluating human fall detection. (Thanh-Hai Tran) [Before  28/12/19]</li>
<li><a href="http://pr.cs.cornell.edu/humanactivities/data.php" target="_blank" rel="noopener">Cornell Activity Datasets CAD 60, CAD 120</a> (Cornell Robot Learning Lab) [Before 28/12/19]</li>
<li><a href="https://sites.uclouvain.be/ispgroup/Softwares/DeepSport" target="_blank" rel="noopener">DeepSport</a> - pairs of successive images captured in different basketball arenas  during professional games, with ground ruth annotations of the ball  position. (UC Louvain ISPGroup)</li>
<li><a href="http://www.demcare.eu/results/datasets" target="_blank" rel="noopener">DemCare dataset</a> - DemCare dataset consists of a set of diverse data collection from  different sensors and is useful for human activity recognition from  wearable/depth and static IP camera, speech recognition for Alzheimmer’s disease detection and physiological data for gait analysis and  abnormality detection. (K. Avgerinakis, A.Karakostas, S.Vrochidis, I.  Kompatsiaris) [Before 28/12/19]</li>
<li><a href="http://mclab.citi.sinica.edu.tw/dataset/dha/dha.html" target="_blank" rel="noopener">Depth-included Human Action video dataset</a> - It contains 23 different actions (CITI in Academia Sinica) [Before 28/12/19]</li>
<li><a href="http://ece.ubc.ca/~mohsena/dmlsmartaction.html" target="_blank" rel="noopener">DMLSmartActions dataset</a> - Sixteen subjects performed 12 different actions in a natural manner. (University of British Columbia) [Before 28/12/19]</li>
<li><a href="http://robotics.ait.kyushu-u.ac.jp/~yumi/db/first_dog.html" target="_blank" rel="noopener">DogCentric Activity Dataset</a> - first-person videos taken from a camera mounted on top of a <em>dog</em> (Michael Ryoo) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CEILIDHDATA/" target="_blank" rel="noopener">Edinburgh ceilidh overhead video data</a> - 16 ground-truthed dances viewed from overhead, where the 10 dancers  follow a structured dance pattern (2 different dances). The dataset is  useful for highly structured behavior understanding (Aizeboje, Fisher)  [Before 28/12/19]</li>
<li><a href="http://epic-kitchens.github.io/" target="_blank" rel="noopener">EPIC-KITCHENS</a> -  egocentric video recorded by 32 participants in their native kitchen  environments, non-scripted daily activities,  11.5M frames, 39.6K  frame-level action segments and 454.2K object bounding boxes (Damen,  Doughty, Fidler, et al) [Before 28/12/19]</li>
<li><a href="https://osf.io/d5k38/wiki/home/" target="_blank" rel="noopener">EPFL crepe cooking videos</a> - 6 types of structured cooking activity (12) videos in 1920x1080  resolution (Lee, Ognibene, Chang, Kim and Demiris) [Before 28/12/19]</li>
<li><a href="https://www.etsmtl.ca/Professeurs/ggagnon/Projects/ai-sports" target="_blank" rel="noopener">ETS Hockey Game Event Data Set</a> - This data set contains footage of two hockey games captured using  fixed cameras. (M.-A. Carbonneau, A. J. Raymond, E. Granger, and G.  Gagnon) [Before 28/12/19]</li>
<li><a href="http://vlm1.uta.edu/~zhangzhong/fall_detection/" target="_blank" rel="noopener">The Falling Detection dataset</a> - Six subjects in two sceneries performed a series of actions continuously (University of Texas) [Before 28/12/19]</li>
<li><a href="http://bigvid.fudan.edu.cn/FCVID/" target="_blank" rel="noopener">FCVID: Fudan-Columbia Video Dataset</a> - 91,223 Web videos annotated manually according to 239 categories (Jiang, Wu, Wang, Xue, Chang) [Before 28/12/19]</li>
<li><a href="http://dipersec.king.ac.uk/G3D/" target="_blank" rel="noopener">G3D</a> - synchronised  video, depth and skeleton data for 20 gaming actions captured with  Microsoft Kinect (Victoria Bloom) [Before 28/12/19]</li>
<li><a href="http://dipersec.king.ac.uk/G3D/" target="_blank" rel="noopener">G3Di</a> - This dataset contains 12 subjects split into 6 pairs (Kingston University) [Before 28/12/19]</li>
<li><a href="http://dipersec.king.ac.uk/G3D/" target="_blank" rel="noopener">Gaming 3D dataset</a> - real-time action recognition in gaming scenario (Kingston University) [Before 28/12/19]</li>
<li><a href="http://ai.stanford.edu/~alireza/GTEA_Gaze_Website/" target="_blank" rel="noopener">Georgia Tech Egocentric Activities - Gaze(+)</a> - videos of where people look at and their gaze location (Fathi, Li, Rehg) [Before 28/12/19]</li>
<li><a href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/" target="_blank" rel="noopener">HMDB: A Large Human Motion Database</a> (Serre Lab) [Before 28/12/19]</li>
<li><a href="http://cvssp.org/Hollywood3D/" target="_blank" rel="noopener">Hollywood 3D dataset</a> - 650 3D video clips, across 14 action classes (Hadfield and Bowden) [Before 28/12/19]</li>
<li><a href="http://www.irisa.fr/vista/actions/hollywood2/" target="_blank" rel="noopener">Human Actions and Scenes Dataset</a> (Marcin Marszalek, Ivan Laptev, Cordelia Schmid) [Before 28/12/19]</li>
<li><a href="http://humamalwassel.com/publication/action-search/" target="_blank" rel="noopener">Human Searches</a> Search sequences of human annotators that were tasked to spot actions  in AVA and THUMOS14 datasets. (Alwassel, H., Caba Heilbron, F., Ghanem,  B.) [Before 28/12/19]</li>
<li><a href="http://www.di.ens.fr/willow/research/actionordering/" target="_blank" rel="noopener">Hollywood Extended</a> - 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies. (Bojanowski, Lajugie,  Bach, Laptev, Ponce, Schmid, and Sivic) [Before 28/12/19]</li>
<li><a href="http://humaneva.is.tue.mpg.de/datasets_human_1" target="_blank" rel="noopener">HumanEva</a>: Synchronized Video and Motion Capture Dataset for Evaluation of Articulated Human Motion (Brown University) [Before 28/12/19]</li>
<li><a href="http://tna.europarchive.org/20100413151426/scienceandresearch.homeoffice.gov.uk/hosdb/cctv-imaging-technology/i-lids/index.html" target="_blank" rel="noopener">I-LIDS video event image dataset (Imagery library for intelligent detection systems)</a> (Paul Hosner) [Before 28/12/19]</li>
<li><a href="http://kahlan.eps.surrey.ac.uk/i3dpost_action/" target="_blank" rel="noopener">I3DPost Multi-View Human Action Datasets</a> (Hansung Kim) [Before 28/12/19]</li>
<li><a href="http://robotics.dei.unipd.it/actions/index.php/overview" target="_blank" rel="noopener">IAS-lab Action dataset</a> - contain sufficient variety of actions and number of people performing the actions (IAS Lab at the University of Padua) [Before 28/12/19]</li>
<li><a href="http://www.ics.forth.gr/cvrl/evaco/" target="_blank" rel="noopener">ICS-FORTH MHAD101 Action Co-segmentation</a> - 101 pairs of long-term action sequences that share one or multiple  common actions to be co-segmented, contains both 3d skeletal and video  related frame-based features (Universtiy of Crete and FORTH-ICS, K.  Papoutsakis) [Before 28/12/19]</li>
<li><a href="https://github.com/suriyasingh/IIIT-Extreme-Sports" target="_blank" rel="noopener">IIIT Extreme Sports</a> - 160 first person (egocentric) sport videos from YouTube with frame  level annotations of 18 action classes. (Suriya Singh, Chetan Arora, and C. V. Jawahar. Trajectory Aligned) [Before 28/12/19]</li>
<li><a href="http://4drepository.inrialpes.fr/public/viewgroup/6" target="_blank" rel="noopener">INRIA Xmas Motion Acquisition Sequences (IXMAS)</a> (INRIA) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/gaochenqiang/publication/infrared-action-dataset" target="_blank" rel="noopener">InfAR Dataset</a> -Infrared Action Recognition at Different Times  Neurocomputing(Chenqiang Gao, Yinhe Du, Jiang Liu, Jing Lv, Luyu Yang,  Deyu Meng, Alexander G. Hauptmann) [Before 28/12/19]</li>
<li><a href="http://jhmdb.is.tue.mpg.de/" target="_blank" rel="noopener">JHMDB: Joints for the HMDB dataset</a> (J-HMDB) based on 928 clips from HMDB51 comprising 21 action categories (Jhuang, Gall, Zuffi, Schmid and Black) [Before 28/12/19]</li>
<li><a href="http://cvrc.ece.utexas.edu/mryoo/jpl-interaction.html" target="_blank" rel="noopener">JPL First-Person Interaction dataset</a> - 7 types of human activity videos taken from a first-person viewpoint (Michael S. Ryoo, JPL) [Before 28/12/19]</li>
<li><a href="http://www.inf-cv.uni-jena.de/JAR_Aibo.html" target="_blank" rel="noopener">Jena Action Recognition Dataset</a> - Aibo dog actions (Korner and Denzler) [Before 28/12/19]</li>
<li><a href="https://www.leightley.com/k3da/" target="_blank" rel="noopener">K3Da - Kinect 3D Active dataset</a> - K3Da (Kinect 3D active) is a realistic clinically relevant human  action dataset containing skeleton, depth data and associated  participant information (D. Leightley, M. H. Yap, J. Coulson, Y.  Barnouin and J. S. McPhee) [Before 28/12/19]</li>
<li><a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" target="_blank" rel="noopener">Kinetics Human Action Video Dataset</a> - 300,000 video clips, 400 human action classe, 10 second clips, single action per clip (Kay, Carreira, et al) [Before 28/12/19]</li>
<li><a href="https://cvhci.anthropomatik.kit.edu/projects/act/kitchen/" target="_blank" rel="noopener">KIT Robo-Kitchen Activity Data Set</a> - 540 clips of 17 people performing 12 complex   kitchen activities.  (L. Rybok, S. Friedberger, U. D. Hanebeck, R. Stiefelhagen) [Before  28/12/19]</li>
<li><a href="http://www.nada.kth.se/cvap/actions/" target="_blank" rel="noopener">KTH human action recognition database</a> (KTH CVAP lab) [Before 28/12/19]</li>
<li><a href="https://cvhci.anthropomatik.kit.edu/projects/act/minta" target="_blank" rel="noopener">Karlsruhe Motion, Intention, and Activity Data set (MINTA)</a> - 7 types of activities of daily living including   fully motion  primitive segments. (D. Gehrig, P. Krauthausen, L. Rybok, H. Kuehne, U.  D.  Hanebeck, T. Schultz, R. Stiefelhagen) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Leeds Activity Dataset–Breakfast (LAD–Breakfast)</a> - It is composed of 15 annotated videos, representing five different  people having breakfast or other simple meal; (John Folkesson et al.)  [Before 28/12/19]</li>
<li><a href="http://liris.cnrs.fr/voir/activities-dataset/" target="_blank" rel="noopener">LIRIS Human Activities Dataset</a> - contains (gray/rgb/depth) videos showing people performing various  activities (Christian Wolf, et al, French National Center for Scientific Research) [Before 28/12/19]</li>
<li><a href="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset" target="_blank" rel="noopener">MEXaction2 action detection and localization dataset</a> - To support the development and evaluation of methods for ‘spotting’  instances of short actions in a relatively large video database: 77  hours, 117 videos (Michel Crucianu and Jenny Benois-Pineau) [Before  28/12/19]</li>
<li><a href="https://github.com/piergiaj/mlb-youtube" target="_blank" rel="noopener">MLB-YouTube</a> - Dataset for activity recognition in baseball videos (AJ Piergiovanni, Michael Ryoo) [Before 28/12/19]</li>
<li><a href="http://moments.csail.mit.edu/" target="_blank" rel="noopener">Moments in Time Dataset</a> - Moments in Time Dataset 1M 3-second videos annotated with action type,  the largest dataset of its kind for action recognition and understanding in video. (Monfort, Oliva, et al.) [Before 28/12/19]</li>
<li><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/" target="_blank" rel="noopener">MPII Cooking Activities Dataset</a> for fine-grained cooking activity recognition, which also includes the  continuous pose estimation challenge (Rohrbach, Amin, Andriluka and  Schiele) [Before 28/12/19]</li>
<li><a href="http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-2-dataset/" target="_blank" rel="noopener">MPII Cooking 2 Dataset</a> - A large dataset of fine-grained cooking activities, an extension of  the MPII Cooking Activities Dataset. (Rohrbach, Rohrbach, Regneri, Amin, Andriluka, Pinkal, Schiele) [Before 28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/" target="_blank" rel="noopener">MSR-Action3D</a> - benchmark RGB-D action dataset (Microsoft Research Redmond and University of Wollongong) [Before 28/12/19]</li>
<li><a href="http://www.cs.ucf.edu/~oreifej/HON4D.html" target="_blank" rel="noopener">MSRActionPair dataset</a> - : Histogram of Oriented 4D Normals for Activity Recognition from  Depth Sequences (University of Central Florida and Microsoft) [Before  28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/" target="_blank" rel="noopener">MSRC-12 Kinect gesture data set</a> - 594 sequences and 719,359 frames from people performing 12 gestures (Microsoft Research Cambridge) [Before 28/12/19]</li>
<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52283" target="_blank" rel="noopener">MSRC-12 dataset</a> - sequences of human movements, represented as body-part locations, and the associated gesture (Microsoft Research Cambridge and University of  Cambridge) [Before 28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/um/people/zliu/ActionRecoRsrc/" target="_blank" rel="noopener">MSRDailyActivity3D Dataset</a> - There are 16 activities (Microsoft and the Northwestern University) [Before 28/12/19]</li>
<li><a href="http://www.dpi.physik.uni-goettingen.de/~eaksoye/dataset.html" target="_blank" rel="noopener">ManiAc</a> RGB-D action dataset:  different manipulation actions, 15 different  versions, 30 different objects manipulated, 20 long and complex chained  manipulation sequences (Eren Aksoy) [Before 28/12/19]</li>
<li><a href="http://mivia.unisa.it/datasets/video-analysis-datasets/mivia-action-dataset/" target="_blank" rel="noopener">Mivia dataset</a> - It consists of 7 high-level actions performed by 14 subjects. (Mivia Lab at the University of Salemo) [Before 28/12/19]</li>
<li><a href="https://github.com/ParitoshParmar/MTL-AQA" target="_blank" rel="noopener">MTL-AQA</a> -  Multitask learning dataset for assessing quality of Olympic Diving. More than 1500 samples. It contains videos of action samples, fine-grained  action class, expert commentary (AQA-oriented captions), AQA scores from judges. Videos from multiple views included wherever available. Can be  used for captioning, and fine-grained action recognition, apart from  AQA. (Parmar, Morris) [29/12/19]</li>
<li><a href="http://dipersec.king.ac.uk/MuHAVi-MAS/" target="_blank" rel="noopener">MuHAVi</a> - Multicamera Human Action Video Data (Hossein Ragheb) [Before 28/12/19]</li>
<li><a href="http://humansensing.cs.cmu.edu/mad/download.html" target="_blank" rel="noopener">Multi-modal action detection (MAD) Dataset</a> - It contains 35 sequential actions performed by 20 subjects. (CarnegieMellon University) [Before 28/12/19]</li>
<li><a href="http://users.eecs.northwestern.edu/~jwa368/my_data.html" target="_blank" rel="noopener">Multiview 3D Event dataset</a> - This dataset includes 8 categories of events performed by 8 subjects  (University of California at Los Angles) [Before 28/12/19]</li>
<li><a href="https://github.com/muralab/Low-Resolution-FIR-Action-Dataset" target="_blank" rel="noopener">Nagoya University Extremely Low-resolution FIR Image Action Dataset</a> - Action recognition dataset captured by a 16x16 low-resolution FIR sensor. (Nagoya University) [Before 28/12/19]</li>
<li><a href="https://github.com/shahroudy/NTURGB-D" target="_blank" rel="noopener">NTU RGB+D Action Recognition Dataset</a> - NTU RGB+D is a large scale dataset for human action recognition(Amir Shahroudy) [Before 28/12/19]</li>
<li><a href="http://users.eecs.northwestern.edu/~jwa368/my_data.html" target="_blank" rel="noopener">Northwestern-UCLA Multiview Action 3D</a> - There are 10 action categories:(Northwestern University and University of California at Los Angles) [Before 28/12/19]</li>
<li><a href="http://smartcity.csr.unibo.it/activity-recognition/" target="_blank" rel="noopener">Office Activity Dataset</a> - It consists of skeleton data acquired by Kinect 2.0 from different  subjects performing common office activities. (A. Franco, A. Magnani, D. Maiop) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/index.html" target="_blank" rel="noopener">Oxford TV based human interactions</a> (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a href="https://github.com/htwang14/PA-HMDB51" target="_blank" rel="noopener">PA-HMDB51</a> -  human action video (592) dataset with potential privacy leak attributes  annotated: skin color, gender, face, nudity, and relationship (Wang, Wu, Wang, Wang, Jin) [Before 28/12/19]</li>
<li><a href="http://www.cse.uoi.gr/~mvrigkas/#downloads" target="_blank" rel="noopener">Parliament</a> - The Parliament dataset is a collection of 228 video sequences,  depicting political speeches in the Greek parliament. (Michalis Vrigkas, Christophoros Nikou, Ioannins A. kakadiaris) [Before 28/12/19]</li>
<li><a href="http://adas.cvc.uab.es/phav/" target="_blank" rel="noopener"> Procedural Human Action Videos</a> - This dataset contains about 40,000 videos for human action  recognition that had been generated using a 3D game engine. The dataset  contains about 6 million frames which can be used to train and evaluate  models not only action recognition but also models for depth map  estimation, optical flow, instance segmentation, semantic segmentation,  3D and 2D pose estimation, and attribute learning. (Cesar Roberto de  Souza) [Before 28/12/19]</li>
<li><a href="http://watchnpatch.cs.cornell.edu/" target="_blank" rel="noopener">RGB-D activity dataset</a> - Each video in the dataset contains 2-7 actions involving interaction  with different objects. (Cornell University and Stanford University)  [Before 28/12/19]</li>
<li><a href="https://data.bris.ac.uk/data/dataset/66qry08cv1fj1eunwxwob3fjz" target="_blank" rel="noopener">RGBD-Action-Completion-2016</a> - This dataset includes 414 complete/incomplete object interaction  sequences, spanning six actions and presenting RGB, depth and skeleton  data. (Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen) [Before  28/12/19]</li>
<li><a href="https://arxiv.org/abs/1601.05511" target="_blank" rel="noopener">RGB-D-based Action Recognition Datasets</a> - Paper that includes the list and links of different rgb-d action  recognition datasets. (Jing Zhang, Wanqing Li, Philip O. Ogunbona,  Pichao Wang, Chang Tang) [Before 28/12/19]</li>
<li><a href="http://www.uestcrobot.net/en/?q=download" target="_blank" rel="noopener">RGBD-SAR Dataset</a> - RGBD-SAR Dataset (University of Electronic Science and Technology of China and Microsoft) [Before 28/12/19]</li>
<li><a href="http://www.cs.rochester.edu/~rmessing/uradl/" target="_blank" rel="noopener">Rochester Activities of Daily Living Dataset</a> (Ross Messing) [Before 28/12/19]</li>
<li><a href="https://saras-esad.grand-challenge.org/Dataset/" target="_blank" rel="noopener">SARAS endoscopic vision challenge for surgeon action detection</a> - 22,601 annotated training frames with 28,055 action instances from 21 different action classes (Cuzzolin, Singh Bawa, Skarga-Bandurova,  Singh) [16/4/20]</li>
<li><a href="http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html" target="_blank" rel="noopener">SBU Kinect Interaction Dataset</a> - It contains eight types of interactions (Stony Brook University) [Before 28/12/19]</li>
<li><a href="http://vision.cs.stonybrook.edu/~kiwon/Datasets/SBU_Kinect_Interactions/README.txt" target="_blank" rel="noopener">SBU-Kinect-Interaction dataset v2.0</a> - It comprises of RGB-D video sequences of humans performing interaction activities (Kiwon Yun etc.) [Before 28/12/19]</li>
<li><a href="http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html" target="_blank" rel="noopener">SDHA Semantic Description of Human Activities 2010 contest - Human Interactions</a> (Michael S. Ryoo, J. K. Aggarwal, Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a href="http://cvrc.ece.utexas.edu/SDHA2010/Aerial_View_Activity.html" target="_blank" rel="noopener">SDHA Semantic Description of Human Activities 2010 contest - aerial views</a> (Michael S. Ryoo, J. K. Aggarwal, Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a href="https://github.com/mostafa-saad/deep-activity-rec" target="_blank" rel="noopener">SFU Volleyball Group Activity Recognition</a> - 2 levels annotations dataset (9 players’ actions and 8 scene’s  activity) for volleyball videos. (M. Ibrahim, S. Muralidharan, Z. Deng,  A. Vahdat, and G. Mori / Simon Fraser University) [Before 28/12/19]</li>
<li><a href="http://isee.sysu.edu.cn/resource" target="_blank" rel="noopener">SYSU 3D Human-Object Interaction Dataset</a> - Forty subjects perform 12 distinct activities (Sun Yat-sen University) [Before 28/12/19]</li>
<li><a href="http://www.projects.science.uu.nl/shakefive/" target="_blank" rel="noopener">ShakeFive Dataset</a> - contains only two actions, namely hand shake and high five. (Universiteit Utrecht) [Before 28/12/19]</li>
<li><a href="http://www.projects.science.uu.nl/shakefive/" target="_blank" rel="noopener">ShakeFive2</a> - A dyadic human interaction dataset with limb level  annotations on 8  classes in 153 HD videos (Coert van Gemeren, Ronald Poppe, Remco  Veltkamp) [Before 28/12/19]</li>
<li><a href="https://silviogiancola.github.io/SoccerNet/" target="_blank" rel="noopener">SoccerNet</a> - Scalable dataset for action spotting in soccer videos: 500 soccer games fully annotated with main actions (goal, cards, subs) and more than 13K soccer games annotated with 500K commentaries for event captioning and  game summarization. (Silvio Giancola, Mohieddine Amine, Tarek Dghaily,  Bernard Ghanem) [Before 28/12/19]</li>
<li><a href="http://cvlab.cse.msu.edu/project-svw.html" target="_blank" rel="noopener">Sports Videos in the Wild (SVW)</a> - SVW is comprised of 4200 videos captured solely with smartphones by  users of Coach Eye smartphone app, a leading app for sports training  developed by TechSmith corporation. (Seyed Morteza Safdarnejad, Xiaoming Liu) [Before 28/12/19]</li>
<li><a href="http://vision.stanford.edu/lijiali/event_dataset/" target="_blank" rel="noopener">Stanford Sport Events dataset</a> (Jia Li) [Before 28/12/19]</li>
<li><a href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php" target="_blank" rel="noopener">THU-READ(Tsinghua University RGB-D Egocentric Action Dataset)</a> - THU-READ is a large-scale dataset for action recognition in RGBD  videos with pixel-layer hand annotation. (Yansong Tang, Yi Tian, Jiwen  Lu, Jianjiang Feng, Jie Zhou) [Before 28/12/19]</li>
<li><a href="http://www.thumos.info/home.html" target="_blank" rel="noopener">THUMOS</a> - Action  Recognition in Temporally Untrimmed Videos! -  430 hours of video data  and 45 million frames (Gorban, Idrees, Jiang, Zamir, Laptev Shah,  Sukthanka) [Before 28/12/19]</li>
<li><a href="https://project.inria.fr/toyotasmarthome/" target="_blank" rel="noopener">Toyota Smarthome dataset</a> - Dataset for Real-world activities of Daily Living (Toyota Motors Europe &amp; INRIA Sophia Antipolis) [30/12/19]</li>
<li><a href="http://ias.in.tum.de/software/kitchen-activity-data" target="_blank" rel="noopener">TUM Kitchen Data Set of Everyday Manipulation Activities</a> (Moritz Tenorth, Jan Bandouch) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~alonso/tv_human_interactions.html" target="_blank" rel="noopener">TV Human Interaction Dataset</a> (Alonso Patron-Perez) [Before 28/12/19]</li>
<li><a href="http://media.tju.edu.cn/tju_dataset.html" target="_blank" rel="noopener">The TJU dataset</a> - contains 22 actions performed by 20 subjects in two different  environments; a total of 1760 sequences. (Tianjin University) [Before  28/12/19]</li>
<li><a href="http://vision.eecs.ucf.edu/data/SmartPhone/" target="_blank" rel="noopener">UCF-iPhone Data Set</a> - 9 Aerobic actions were recorded from (6-9) subjects using the  Inertial Measurement Unit (IMU) on an Apple iPhone 4 smartphone. (Corey  McCall, Kishore Reddy and Mubarak Shah) [Before 28/12/19]</li>
<li><a href="https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones" target="_blank" rel="noopener">UCI Human Activity Recognition Using Smartphones Data Set</a> - recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial  sensors (Anguita, Ghio, Oneto, Parra, Reyes-Ortiz) [Before 28/12/19]</li>
<li><a href="http://rtis.oit.unlv.edu/datasets.html" target="_blank" rel="noopener">UNLV Dive &amp; Gymvault</a> - Dataset for assessing quality of Olympic Diving and Olympic Gymnastic Vault. It consists of videos of action samples and corresponding action quality scores. (Parmar, Morris) [29/12/19]</li>
<li><a href="http://www.upcv.upatras.gr/personal/kastaniotis/datasets.html" target="_blank" rel="noopener">The UPCV action dataset</a> - The dataset consists of 10 actions performed by 20 subjects twice. (University of Patras) [Before 28/12/19]</li>
<li><a href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/" target="_blank" rel="noopener">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a href="http://crcv.ucf.edu/data/UCF101.php" target="_blank" rel="noopener">UCF 101 action dataset</a> 101 action classes, over 13k clips and 27 hours of video data (Univ of Central Florida) [Before 28/12/19]</li>
<li><a href="http://crcv.ucf.edu/projects/real-world/" target="_blank" rel="noopener">UCF-Crime Dataset: Real-world Anomaly Detection in Surveillance Videos</a> - A large-scale dataset for real-world anomaly detection in  surveillance videos. It consists of 1900 long and untrimmed real-world  surveillance videos (of 128 hours), with 13 realistic anomalies such as  fighting, road accident, burglary, robbery, etc. as well as normal  activities. (Center for Research in Computer Vision, University of  Central Florida) [Before 28/12/19]</li>
<li><a href="http://www.cs.ucf.edu/~smasood/datasets/UCFKinect.zip" target="_blank" rel="noopener">UCFKinect</a> - The dataset is composed of 16 actions (University of Central Florida Orlando) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~tianmin.shu/SocialAffordance/index.html" target="_blank" rel="noopener">UCLA Human-Human-Object Interaction (HHOI) Dataset Vn1</a> - Human interactions in RGB-D videos (Shu, Ryoo, and Zhu) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~tianmin.shu/SocialAffordanceGrammar/" target="_blank" rel="noopener">UCLA Human-Human-Object Interaction (HHOI) Dataset Vn2</a> - Human interactions in RGB-D videos (version 2) (Shu, Gao, Ryoo, and Zhu) [Before 28/12/19]</li>
<li><a href="http://www.ee.ucr.edu/~amitrc/datasets.php" target="_blank" rel="noopener">UCR Videoweb Multi-camera Wide-Area Activities Dataset</a> (Amit K. Roy-Chowdhury) [Before 28/12/19]</li>
<li><a href="http://www.utdallas.edu/~kehtar/UTD-MHAD.html" target="_blank" rel="noopener">UTD-MHAD</a> - Eight subjects performed 27 actions four times. (University of Texas at Dallas) [Before 28/12/19]</li>
<li><a href="http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html" target="_blank" rel="noopener">UTKinect dataset</a> - Ten types of human actions were performed twice by 10 subjects (University of Texas) [Before 28/12/19]</li>
<li><a href="http://staffhome.ecm.uwa.edu.au/~00053650/databases.html" target="_blank" rel="noopener">UWA3D Multiview Activity Dataset</a> - Thirty activities were performed by 10 individuals (University of Western Australia) [Before 28/12/19]</li>
<li><a href="http://server.cs.ucf.edu/~vision/data/UCF50.rar" target="_blank" rel="noopener">Univ of Central Florida - 50 Action Category Recognition in Realistic Videos (3 GB)</a> (Kishore Reddy) [Before 28/12/19]</li>
<li><a href="http://server.cs.ucf.edu/~vision/data/UCF-ARG.html" target="_blank" rel="noopener">Univ of Central Florida - ARG</a> Aerial camera, Rooftop camera and Ground camera (UCF Computer Vision Lab) [Before 28/12/19]</li>
<li>[Univ of Central Florida - Feature Films Action Dataset](<a href="http://server.cs.ucf.edu/~vision/projects/action_mach/Slaps" target="_blank" rel="noopener">http://server.cs.ucf.edu/~vision/projects/action_mach/Slaps</a> and kisses.rar) (Univ of Central Florida) [Before 28/12/19]</li>
<li><a href="http://crcv.ucf.edu/data/UCF_Sports_Action.php" target="_blank" rel="noopener">Univ of Central Florida - Sports Action Dataset</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a href="http://server.cs.ucf.edu/~vision/projects/liujg/YouTube_Action_dataset.html" target="_blank" rel="noopener">Univ of Central Florida - YouTube Action Dataset (sports)</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html" target="_blank" rel="noopener">Unsegmented Sports News Videos</a> - Database of 74 sports news videos tagged with 10 categories of  sports. Designed to test multi-label video tagging. (T. Hospedales,  Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="https://www.projects.science.uu.nl/umpm/" target="_blank" rel="noopener">Utrecht Multi-Person Motion Benchmark (UMPM).</a> - a collection of video recordings of people together with a ground  truth based on motion capture data. (N.P. van der Aa, X. Luo, G.J.  Giezeman, R.T. Tan, R.C. Veltkamp.) [Before 28/12/19]</li>
<li><a href="http://www.viratdata.org/" target="_blank" rel="noopener">VIRAT Video Dataset</a> - event  recognition from two broad categories of activities (single-object and  two-objects) which involve both human and vehicles. (Sangmin Oh et al)  [Before 28/12/19]</li>
<li><a href="http://profs.sci.univr.it/~cristanm/datasets.html" target="_blank" rel="noopener">Verona Social interaction dataset</a> (Marco Cristani) [Before 28/12/19]</li>
<li><a href="http://dipersec.king.ac.uk/VIHASI/" target="_blank" rel="noopener">ViHASi: Virtual Human Action Silhouette Data</a> (userID: VIHASI password: virtual$virtual) (Hossein Ragheb, Kingston University) [Before 28/12/19]</li>
<li><a href="http://www.ee.ucr.edu/~amitrc/datasets.php" target="_blank" rel="noopener">Videoweb (multicamera) Activities Dataset</a> (B. Bhanu, G. Denina, C. Ding, A. Ivers, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, B. Varda) [Before 28/12/19]</li>
<li><a href="http://csee.wvu.edu/~vkkulathumani/wvu-action.html" target="_blank" rel="noopener">WVU Multi-view action recognition dataset</a> (Univ. of West Virginia) [Before 28/12/19]</li>
<li><a href="http://visionscience.com/pipermail/visionlist/2013/006219.html" target="_blank" rel="noopener">WorkoutSU-10</a> Kinect dataset for exercise actions (Ceyhun Akgul) [Before 28/12/19]</li>
<li><a href="http://vpa.sabanciuniv.edu.tr/phpBB2/vpa_views.php?s=31&serial=36" target="_blank" rel="noopener">WorkoutSU-10 dataset</a> - contains exercise actions selected by professional trainers for therapeutic purposes. (Sabanci University) [Before 28/12/19]</li>
<li><a href="http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/" target="_blank" rel="noopener">Wrist-mounted camera video dataset</a> - object manipulation (Ohnishi, Kanehira, Kanezaki, Harada) [Before 28/12/19]</li>
<li><a href="http://bit.ly/Zy2ZR1" target="_blank" rel="noopener">YouCook</a> - 88 open-source YouTube cooking videos with annotations (Jason Corso) [Before 28/12/19]</li>
<li><a href="https://research.google.com/youtube8m/" target="_blank" rel="noopener">YouTube-8M Dataset</a> -A Large and Diverse Labeled Video Dataset for Video Understanding Research(Google Inc.) [Before 28/12/19]</li>
</ol>
<h2 id="Agriculture"><a href="#Agriculture" class="headerlink" title="Agriculture"></a>Agriculture</h2><ol>
<li><a href="https://doi.org/10.5281/zenodo.168158" target="_blank" rel="noopener">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a href="https://vision.eng.au.dk/fieldsafe/" target="_blank" rel="noopener">Fieldsafe</a> - A multi-modal dataset for obstacle detection in agriculture.  (Aarhus University) [Before 28/12/19]</li>
<li><a href="http://limu.ait.kyushu-u.ac.jp/~agri/komatsuna/" target="_blank" rel="noopener">KOMATSUNA dataset</a> - The datasets is designed for instance segmentation, tracking and  reconstruction for leaves using both sequential multi-view RGB images  and depth images. (Hideaki Uchiyama, Kyushu University) [Before  28/12/19]</li>
<li><a href="https://vision.eng.au.dk/leaf-counting-dataset/" target="_blank" rel="noopener">Leaf counting dataset</a> - Dataset for estimating the growth stage of small plants. (Aarhus University) [Before 28/12/19]</li>
<li><a href="http://www.plant-phenotyping.org/CVPPP2014-dataset" target="_blank" rel="noopener">Leaf Segmentation Challenge</a>Tobacco and arabidopsis plant images (Hanno Scharr, Massimo Minervini, Andreas  Fischbach, Sotirios A. Tsaftaris) [Before 28/12/19]</li>
<li><a href="https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network" target="_blank" rel="noopener"> Multi-species fruit flower detection</a> - This dataset consists of four sets of flower images, from three  different tree species: apple, peach, and pear, and accompanying ground  truth images. (Philipe A. Dias, Amy Tabb, Henry Medeiros) [Before  28/12/19]</li>
<li><a href="http://www.plant-phenotyping.org/datasets-home" target="_blank" rel="noopener">Plant Phenotyping Datasets</a> - plant data suitable for plant and leaf detection, segmentation,  tracking, and species recognition (M. Minervini, A. Fischbach, H.  Scharr, S. A. Tsaftaris) [Before 28/12/19]</li>
<li><a href="https://vision.eng.au.dk/plant-seedlings-dataset/" target="_blank" rel="noopener">Plant seedlings dataset</a> - High-resolution images of 12 weed species.  (Aarhus University) [Before 28/12/19]</li>
</ol>
<h2 id="Attribute-recognition"><a href="#Attribute-recognition" class="headerlink" title="Attribute recognition"></a>Attribute recognition</h2><ol>
<li><a href="http://yanweifu.github.io/USAA/download/" target="_blank" rel="noopener">Attribute Learning for Understanding Unstructured Social Activity</a> - Database of videos containing 10 categories of unstructured social  events to recognise, also annotated with 69 attributes. (Y. Fu  Fudan/QMUL, T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="https://cvml.ist.ac.at/AwA2/" target="_blank" rel="noopener">Animals with Attributes 2</a> - 37322 (freely licensed) images of 50 animal classes with 85 per-class binary attributes. (Christoph H. Lampert, IST Austria) [Before  28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Birds</a>This database contains 600 images (100 samples each) of six different  classes of birds. (Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce)  [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Butterflies</a>This database contains 619 images of seven different classes of butterflies. (Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce) [Before 28/12/19]</li>
<li><a href="https://caer-dataset.github.io/" target="_blank" rel="noopener">CAER (Context-Aware Emotion Recognition)</a> - Large scale image and video dataset for emotion recognition, and  facial expression recognition (Lee, Kim, Kim, Park, and Sohn) [29/12/19]</li>
<li><a href="http://calvin.inf.ed.ac.uk/datasets/" target="_blank" rel="noopener">CALVIN research group datasets</a> - object detection with eye tracking, imagenet bounding boxes,  synchronised activities, stickman and body poses, youtube objects,  faces, horses, toys, visual attributes, shape classes (CALVIN ggroup)  [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebA</a> - Large-scale CelebFaces Attributes Dataset(Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a href="https://github.com/vana77/DukeMTMC-attribute" target="_blank" rel="noopener">DukeMTMC-attribute</a> - 23 pedestrian attributes for DukeMTMC-reID (Lin, Zheng, Zheng, Wu and Yang) [Before 28/12/19]</li>
<li><a href="http://sunai.uoc.edu/emotic/" target="_blank" rel="noopener">EMOTIC (EMOTIons in Context)</a> - Images of people (34357) embedded in their natural environments,  annotated with 2 distinct emotion representation. (Ronak kosti, Agata  Lapedriza, Jose Alvarez, Adria Recasens) [Before 28/12/19]</li>
<li><a href="https://jurie.users.greyc.fr/datasets/hat.html" target="_blank" rel="noopener">HAT</a> database of 27 human attributes (Gaurav Sharma, Frederic Jurie) [Before 28/12/19]</li>
<li><a href="http://cvit.iiit.ac.in/projects/relativeParts/" target="_blank" rel="noopener">LFW-10 dataset for learning relative attributes</a> - A dataset of 10,000 pairs of face images with instance-level  annotations for 10 attributes. (CVIT, IIIT Hyderabad. ) [Before  28/12/19]</li>
<li><a href="https://github.com/vana77/Market-1501_Attribute" target="_blank" rel="noopener">Market-1501-attribute</a> - 27 visual attributes for 1501 shoppers. (Lin, Zheng, Zheng, Wu and Yang) [Before 28/12/19]</li>
<li><a href="http://vcc.szu.edu.cn/research/2017/RSCM.html" target="_blank" rel="noopener">Multi-Class Weather Dataset</a> - Our multi-class benchmark dataset contains 65,000 images from 6  common categories for sunny, cloudy, rainy, snowy, haze and thunder  weather. This dataset benefits weather classification and attribute  recognition. (Di Lin) [Before 28/12/19]</li>
<li><a href="https://goo.gl/DKuhlY" target="_blank" rel="noopener">Person Recognition in Personal Photo Collections</a> - we introduced three harder splits for evaluation and long-term  attribute annotations and per-photo timestamp metadata. (Oh, Seong Joon  and Benenson, Rodrigo and Fritz, Mario and Schiele, Bernt) [Before  28/12/19]</li>
<li><a href="http://vision.cs.utexas.edu/projects/ego_snappoints/" target="_blank" rel="noopener">UT-Zappos50K Shoes</a> - Large scale shoe dataset consisting of 50,000 catalog images and over 50,000 pairwise relative attribute labels on 11 fine-grained attributes (Aron Yu, Mark Stephenson, Kristen Grauman, UT Austin) [Before  28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/s1151656/resources.html" target="_blank" rel="noopener">Visual Attributes Dataset</a> visual attribute annotations for over 500 object classes (animate and  inanimate) which are all represented in ImageNet. Each object class is  annotated with visual attributes based on a taxonomy of 636 attributes  (e.g., has fur, made of metal, is round). (Before 30/12/19) [Before  28/12/19]</li>
<li><a href="https://tribhuvanesh.github.io/vpa/" target="_blank" rel="noopener">The Visual Privacy (VISPR) Dataset</a> - Privacy Multilabel Dataset (22k images, 68 privacy attributes) (Orekondy, Schiele, Fritz) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html" target="_blank" rel="noopener">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
</ol>
<h2 id="Autonomous-Driving"><a href="#Autonomous-Driving" class="headerlink" title="Autonomous Driving"></a>Autonomous Driving</h2><ol>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/amuse/" target="_blank" rel="noopener">AMUSE</a> -The automotive multi-sensor (AMUSE) dataset taken in real traffic  scenes during multiple test drives. (Philipp Koschorrek etc.) [Before  28/12/19]</li>
<li><a href="http://apolloscape.auto/car_instance.html" target="_blank" rel="noopener">ApolloCar3D</a> - 5000 labelled images with 60K car instances (Song, Wang, Zhou, Zhu, Guan, Dai, Su, Li, Yang) [26/1/20]</li>
<li><a href="http://apolloscape.auto/" target="_blank" rel="noopener">ApolloScape</a> - high resolution cameras and a Riegl acquisition system. Our dataset is collected in  different cities under various traffic conditions. 74555 video frames  and their pixel-level and instance-level annotations (Peking University / Baido) [18/1/20]</li>
<li><a href="https://www.argoverse.org/" target="_blank" rel="noopener">Argoverse</a> - Two public  datasets supported by highly detailed maps to test, experiment, and  teach self-driving vehicles how to understand the world around them;  more than 300,000 curated scenarios, 3D tracking annotations for 113  scenes and 324,557 interesting vehicle trajectories for motion  forecasting (Chang, Lambert, Sangkloy, Singh, Bak, Hartnett, Wang, Carr, Lucey, Ramanan, Hays) [18/1/20]</li>
<li><a href="http://adas.cvc.uab.es/site/elektra" target="_blank" rel="noopener">Autonomous Driving</a> - Semantic segmentation, pedestrian detection, virtual-world data, far  infrared, stereo,driver monitoring. (CVC research center and the UAB and UPC universities) [Before 28/12/19]</li>
<li><a href="https://hci.iwr.uni-heidelberg.de/node/6132" target="_blank" rel="noopener">Bosch Small Traffic Lights Dataset (BSTLD)</a> - A dataset for traffic light detection, tracking, and classification. [Before 28/12/19]</li>
<li><a href="https://drivingstereo-dataset.github.io" target="_blank" rel="noopener">DrivingStereo</a> - A Large-Scale Dataset for Stereo Matching in Autonomous Driving  Scenarios. 180k stereo images covering a diverse set of driving  scenarios (Yang, Song, Huang, Deng, Shi, Zhou) [Before 28/12/19]</li>
<li><a href="https://boxy-dataset.com" target="_blank" rel="noopener">Boxy vehicle detection dataset</a> - A vehicle detection dataset with 1.99 million annotated vehicles in  200,000 images. It contains AABB and keypoint labels. [Before 28/12/19]</li>
<li><a href="https://github.com/VRU-intention/casr/blob/master/readme.txt" target="_blank" rel="noopener">CASR: Cyclist Arm Sign Recognition</a> - Small clips of ~10 seconds showing cyclists performing arm signs. The videos are acquired with a consumer-graded  camera. There are  219 arm  sign  actions annotated. (Zhijie Fang, Antonio M. Lopez) [13/1/20]</li>
<li><a href="http://rpg.ifi.uzh.ch/event_driving_datasets.html" target="_blank" rel="noopener">Driving Event Camera Datasets</a> - sequences that were recorded with a VGA (640x480) event camera  (Samsung DVS Gen3) and a conventional RGB camera (Huawei P20 Pro) placed on the windshield of a car driving through Zurich. (Davide Scaramuzza,  Henri Rebecq) [23/1/20]</li>
<li><a href="http://robots.engin.umich.edu/SoftwareData/Ford" target="_blank" rel="noopener">Ford Campus Vision and Lidar Data Set</a> - time-registered data from professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3  omnidirectional camera system (Pandey, McBride, Eustice) [Before  28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/perso.lcpc.fr/tarel.jean-philippe/bdd/frida.html" target="_blank" rel="noopener">FRIDA (Foggy Road Image DAtabase) Image Database</a> - images for performance evaluation of visibility and contrast  restoration algorithms. FRIDA: 90 synthetic images of 18 urban road  scenes. FRIDA2: 330 synthetic images of 66 diverse road scenes, with  viewpoint closed to that of the vehicle’s driver. (Tarel, Cord,  Halmaoui, Gruyer, Hautiere) [Before 28/12/19]</li>
<li><a href="https://usa.honda-ri.com/H3D" target="_blank" rel="noopener">H3D - Honda Research 3D dataset</a> - 360 degree LiDAR dataset (dense pointcloud from Velodyne-64), 160  crowded and highly interactive traffic scenes, 1,071,302 3D bounding box labels, 8 common classes of traffic participants (Patil, Malla, Gang,  Chen) [18/1/20]</li>
<li><a href="https://github.com/facebookresearch/House3D" target="_blank" rel="noopener">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a href="http://idd.insaan.iiit.ac.in/" target="_blank" rel="noopener">India Driving Dataset (IDD)</a> - unstructured driving conditions from India with 50,000 frames (10,000 semantic, and 40,000 coarse annotations) for training autonomous cars  to see using object detection, scene-level and instance-level semantic  segmentation (CVIT, IIIT Hyderabad and Intel) [Before 28/12/19]</li>
<li><a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/" target="_blank" rel="noopener">Joint Attention in Autonomous Driving (JAAD)</a> -  The dataset includes instances of pedestrians and cars  intended  primarily for the purpose of behavioural studies and  detection in the  context of autonomous driving. (Iuliia Kotseruba, Amir Rasouli and John  K. Tsotsos) [Before 28/12/19]</li>
<li><a href="http://cvrr.ucsd.edu/LISA/vehicledetection.html" target="_blank" rel="noopener">LISA Vehicle Detection Dataset</a> - colour first person driving video under various lighting and traffic conditions (Sivaraman, Trivedi) [Before 28/12/19]</li>
<li><a href="https://unsupervised-llamas.com" target="_blank" rel="noopener">LLAMAS Unsupervised dataset</a> - A lane marker detection and segmentation dataset of 100,000 images  with 3d lines, pixel level dashed markers, and curves for individual  lines. [Before 28/12/19]</li>
<li><a href="http://www.6d-vision.com/lostandfounddataset" target="_blank" rel="noopener">Lost and Found Dataset</a> - The Lost and Found Dataset addresses the problem of detecting  unexpected small road hazards (often caused by lost cargo) for  autonomous driving applications. (Sebastian Ramos, Peter Pinggera,  Stefan Gehrig, Uwe Franke, Rudolf Mester, Carsten Rother) [Before  28/12/19]</li>
<li><a href="https://daniilidis-group.github.io/mvsec/" target="_blank" rel="noopener">Multi Vehicle Stereo Event Camera Dataset</a> - Multiple sequences containing a stereo pair of DAVIS 346b event  cameras with ground truth poses, depth maps and optical flow. (lex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, Kostas  Daniilidis) [Before 28/12/19]</li>
<li><a href="https://www.nuscenes.org/" target="_blank" rel="noopener">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=3D0" target="_blank" rel="noopener">RESIDE (Realistic Single Image DEhazing)</a> - The current largest-scale benchmark consisting of both synthetic and  real-world hazy images, for image dehazing research.  RESIDE highlights  diverse data sources and image contents, and serves various training or  evaluation purposes. (Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan  Feng, Wenjun Zeng, Zhangyang Wang) [Before 28/12/19]</li>
<li><a href="http://semantic-kitti.org/" target="_blank" rel="noopener">semanticKITTI</a> - A Dataset  for Semantic Scene Understanding using LiDAR Sequences (Behley, Garbade, Milioto, Quenzel, Behnke, Stachniss, Gall) [18/1/20]</li>
<li><a href="http://synthia-dataset.net/dataset/" target="_blank" rel="noopener">SYNTHetic collection of Imagery and Annotations</a> - The purpose of aiding semantic segmentation and related scene  understanding problems in the context of driving scenarios. (Computer  vision center,UAB) [Before 28/12/19]</li>
<li><a href="http://synthia-dataset.net/" target="_blank" rel="noopener">SYNTHIA</a> - Large set (~half million) of virtual-world images for training autonomous cars to see.  (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a href="http://www.tromai.icoc.me/" target="_blank" rel="noopener">TRoM: Tsinghua Road Markings</a> - This is a dataset which contributes to the area of road marking  segmentation for Automated Driving and ADAS. (Xiaolong Liu, Zhidong  Deng, Lele Cao, Hongchao Lu) [Before 28/12/19]</li>
<li><a href="https://www.pf.bgu.tum.de/pub/testdaten.html" target="_blank" rel="noopener">TUM City Campus</a> - Urban point clouds taken by Mobile Laser Scanning (MLS) for  classification, object extraction and change detection (Stilla, Hebel,  Xu, Gehrung) [3/1/20]</li>
<li><a href="http://robots.engin.umich.edu/SoftwareData/NCLT" target="_blank" rel="noopener">University of Michigan North Campus Long-Term Vision and LIDAR Dataset</a> - 27 sessions spaced approximately biweekly over the course of 15  months, indoors and outdoors,  varying trajectories, different times of  the day across all four seasons. Includes: moving obstacles (e.g.,  pedestrians, bicyclists, and cars), changing lighting, varying  viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction. Includes  ground-truth pose. (Carlevaris-Bianco, Ushani, Eustice) [Before  28/12/19]</li>
<li><a href="http://rpg.ifi.uzh.ch/uzh-fpv.html" target="_blank" rel="noopener">UZH-FPV Drone Racing Dataset</a> - for visual inertial odometry and SLAM. 28 real-world first-person  view sequences both indoors and outdoors, cintaining images, IMU, and  events and ground truth (Delmerico, Cieslewski, Rebecq, Faessler,  Scaramuzza) [Before 28/12/19]</li>
</ol>
<h2 id="Biological-Medical"><a href="#Biological-Medical" class="headerlink" title="Biological/Medical"></a>Biological/Medical</h2><ol>
<li><a href="https://www.nitrc.org/projects/msseg" target="_blank" rel="noopener">2008 MICCAI MS Lesion Segmentation Challenge</a> (National Institutes of Health Blueprint for Neuroscience Research) [Before 28/12/19]</li>
<li><a href="https://github.com/ragavvenkatesan/np-mil/blob/master/data/DR_data.mat" target="_blank" rel="noopener">ASU DR-AutoCC Data</a> -  a Multiple-Instance Learning feature space for a diabetic  retinopathy classification dataset (Ragav Venkatesan, Parag Chandakkar,  Baoxin Li - Arizona State University) [Before 28/12/19]</li>
<li><a href="https://doi.org/10.5281/zenodo.168158" target="_blank" rel="noopener">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a href="http://www.dsp.utoronto.ca/projects/ADP/ADP_Database/" target="_blank" rel="noopener">ADP: Atlas of Digital Pathology</a> - 17,668 histological patch images extracted from 100 slides annotated  with up to 57 hierarchical tissue types (HTTs) from different organs -  the aim is to provide training data for supervised multi-label learning  of tissue types in a digitized whole slide image (Hosseini, Chan, Tse,  Tang, Deng, Norouzi, Rowsell, Plataniotis, Damaskinos) [14/1/20]</li>
<li><a href="http://spineweb.digitalimaginggroup.ca/spineweb/index.php?n=Main.Datasets" target="_blank" rel="noopener">Annotated Spine CT Database</a> for Benchmarking of Vertebrae Localization,  125 patients, 242 scans (Ben Glockern) [Before 28/12/19]</li>
<li><a href="http://braintumorsegmentation.org/" target="_blank" rel="noopener">BRATS</a> - the  identification and segmentation of tumor structures in multiparametric  magnetic resonance images of the brain (TU Munchen etc.) [Before  28/12/19]</li>
<li><a href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php" target="_blank" rel="noopener">Breast Ultrasound Dataset B</a> - 2D Breast Ultrasound Images with 53 malignant lesions and 110 benign  lesions. (UDIAT Diagnostic Centre, M.H. Yap, R. Marti) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/calgary-campinas-dataset/home" target="_blank" rel="noopener">Calgary-Campinas Public Brain MR Dataset</a>: T1-weighted brain MRI volumes acquired in 359 subjects on scanners from three different vendors (GE, Philips, and Siemens) and at two magnetic  field strengths (1.5 T and 3 T). The scans correspond to older adult  subjects. (Souza, Roberto, Oeslle Lucena, Julia Garrafa, David Gobbi,  Marina Saluzzi, Simone Appenzeller, Leticia Rittner, Richard Frayne, and Roberto Lotufo) [Before 28/12/19]</li>
<li><a href="https://github.com/ThoroughImages/CAMEL" target="_blank" rel="noopener">CAMEL colorectal adenoma dataset</a> - image-level labels for weakly supervised learning containing 177  whole slide images (156 contain adenoma) gathered and labeled by  pathologists (Song and Wang) [29/12/19]</li>
<li><a href="https://stanfordmlgroup.github.io/competitions/chexpert/" target="_blank" rel="noopener">CheXpert</a> - a large dataset of chest X-rays and competition for automated chest  x-ray interpretation, which features uncertainty labels and  radiologist-labeled reference standard evaluation sets (Irvin, Rajpurkar et al) [Before 28/12/19]</li>
<li><a href="http://camma.u-strasbg.fr/datasets" target="_blank" rel="noopener">Cholec80</a>: 80 gallbladder laparoscopic videos annotated with phase and tool information. (Andru Putra Twinanda) [Before 28/12/19]</li>
<li><a href="http://www.warwick.ac.uk/BIAlab/data/CRChistoLabeledNucleiHE/" target="_blank" rel="noopener">CRCHistoPhenotypes - Labeled Cell Nuclei Data</a> - colorectal cancer?histology images?consisting of nearly 30,000 dotted nuclei with over 22,000 labeled with the cell type (Rajpoot +  Sirinukunwattana) [Before 28/12/19]</li>
<li><a href="http://www.inf-cv.uni-jena.de/Research/Datasets/Cavy+Dataset.html" target="_blank" rel="noopener">Cavy Action Dataset</a> - 16 sequences with 640 x 480 resolutions recorded at 7.5 frames per  second (fps) with approximately 31621506 frames in total (272 GB) of  interacting cavies (guinea pig) (Al-Raziqi and Denzler) [Before  28/12/19]</li>
<li><a href="http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Datasets.html" target="_blank" rel="noopener">Cell Tracking Challenge Datasets</a> - 2D/3D time-lapse video sequences  with ground truth(Ma et al., Bioinformatics 30:1609-1617, 2014) [Before 28/12/19]</li>
<li><a href="http://image.diku.dk/emphysema_database/" target="_blank" rel="noopener">Computed Tomography Emphysema Database</a> (Lauge Sorensen) [Before 28/12/19]</li>
<li><a href="http://bigr.nl/research/projects/copd" target="_blank" rel="noopener">COPD Machine Learning Dataset</a> - A collection of feature datasets derived from lung computed  tomography (CT) images, which can be used in diagnosis of chronic  obstructive pulmonary disease (COPD). The images in this database are  weakly labeled, i.e. per image, a diagnosis(COPD or no COPD) is given,  but it is not known which parts of the lungs are affected. Furthermore,  the images were acquired at different sites and with different scanners. These problems are related to two learning scenarios in machine  learning, namely multiple instance learning or weakly supervised  learning, and transfer learning or domain adaptation. (Veronika  Cheplygina, Isabel Pino Pena, Jesper Holst Pedersen, David A. Lynch,  Lauge S., Marleen de Bruijne) [Before 28/12/19]</li>
<li><a href="https://cremi.org/data" target="_blank" rel="noopener">CREMI: MICCAI 2016 Challenge</a> - 6 volumes of electron microscopy of neural tissue,neuron and synapse  segmentation, synaptic partner annotation. (Jan Funke, Stephan Saalfeld, Srini Turaga, Davi Bock, Eric Perlman) [Before 28/12/19]</li>
<li><a href="http://www.vision.caltech.edu/Video_Datasets/CRIM13/CRIM13/Main.html" target="_blank" rel="noopener">CRIM13 Caltech Resident-Intruder Mouse dataset</a> - 237 10 minute videos (25 fps) annotated with actions (13 classes)  (Burgos-Artizzu, Dollar, Lin, Anderson and Perona) [Before 28/12/19]</li>
<li><a href="http://mv.cvc.uab.es/projects/colon-qa/cvccolondb" target="_blank" rel="noopener">CVC colon DB</a> - annotated video sequences of colonoscopy video. It contains 15 short  colonoscopy sequences, coming from 15 different studies. In each  sequence one polyp is shown. (Bernal, Sanchez, Vilarino) [Before  28/12/19]</li>
<li><a href="http://DIADEMchallenge.org" target="_blank" rel="noopener">DIADEM: Digital Reconstruction of Axonal and Dendritic Morphology Competition</a> (Allen Institute for Brain Science et al) [Before 28/12/19]</li>
<li><a href="http://www2.it.lut.fi/project/imageret/diaretdb1/" target="_blank" rel="noopener">DIARETDB1 - Standard Diabetic Retinopathy Database</a> (Lappeenranta Univ of Technology) [Before 28/12/19]</li>
<li><a href="http://www.isi.uu.nl/Research/Databases/DRIVE/" target="_blank" rel="noopener">DRIVE: Digital Retinal Images for Vessel Extraction</a> (Univ of Utrecht) [Before 28/12/19]</li>
<li><a href="http://www.cs.sfu.ca/~hamarneh/software/DeformIt/index.html" target="_blank" rel="noopener">DeformIt 2.0</a> - Image Data Augmentation Tool: Simulate novel images with ground truth segmentations from a single image-segmentation pair (Brian Booth and  Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a href="https://www.dir-lab.com/" target="_blank" rel="noopener">Deformable Image Registration Lab dataset</a> -  for objective and rigrorous evaluation of deformable image  registration (DIR) spatial accuracy performance. (Richard Castillo et  al.) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/DERMOFIT/datasets.htm" target="_blank" rel="noopener">DERMOFIT Skin Cancer Dataset</a> - 1300 lesions from 10 classes captured under identical controlled  conditions. Lesion segmentation masks are included (Fisher, Rees,  Aldridge, Ballerini, et al) [Before 28/12/19]</li>
<li><a href="http://dermoscopic.blogspot.com/" target="_blank" rel="noopener">Dermoscopy images</a> (Eric Ehrsam) [Before 28/12/19]</li>
<li><a href="https://eatmint.unige.ch/home.php" target="_blank" rel="noopener">EATMINT (Emotional Awareness Tools for Mediated INTeraction) database</a> - The EATMINT database contains multi-modal and multi-user recordings  of affect and social behaviors in a collaborative setting. (Guillaume  Chanel, Gaelle Molinari, Thierry Pun, Mireille Betrancourt) [Before  28/12/19]</li>
<li><a href="http://web.engr.oregonstate.edu/~tgd/bugid/ept29/" target="_blank" rel="noopener">EPT29.</a>This database contains 4842 images of 1613 specimens of 29 taxa of EPTs:(Tom etc.) [Before 28/12/19]</li>
<li><a href="http://www.eyepacs.com/data-analysis" target="_blank" rel="noopener">EyePACS</a> - retinal image database is comprised of over 3 million retinal images of diverse populations with various degrees of diabetic retinopathy (EyePACS)  [Before 28/12/19]</li>
<li><a href="http://www.ics.forth.gr/cvrl/fire" target="_blank" rel="noopener">FIRE Fundus Image Registration Dataset</a> - 134 retinal image pairs and groud truth for registration. (FORTH-ICS) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/drive/folders/1aygMzSDdoq63IqSk-ly8cMq0_owup8UM" target="_blank" rel="noopener">FMD - Fluorescence Microscopy Denoising dataset</a> - 12,000 real fluorescence microscopy images (Zhang, Zhu, Nichols, Wang, Zhang, Smith, Howard) [Before 28/12/19]</li>
<li><a href="https://github.com/mahdihosseini/FoucsPath" target="_blank" rel="noopener">FocusPath</a> - Focus Quality Assessment for Digital Pathology (Microscopy) Images.   864 image pathes are naturally blurred by 16 levels of out-of-focus lens provided with GT scores of focus levels. (Hosseini, Zhang, Plataniotis) [Before 28/12/19]</li>
<li><a href="http://medisp.bme.teiath.gr/hicl/index.html" target="_blank" rel="noopener">Histology Image Collection Library (HICL)</a> - The HICL is a compilation of 3870histopathological images (so far)  from various diseases, such as brain cancer,breast cancer and HPV (Human Papilloma Virus)-Cervical cancer. (Medical Image and Signal Processing  (MEDISP) Lab., Department of BiomedicalEngineering, School of  Engineering, University of West Attica) [Before 28/12/19]</li>
<li><a href="https://groups.oist.jp/bptu/honeybee-tracking-dataset" target="_blank" rel="noopener">Honeybee segmentation dataset</a> - It is a dataset containing positions and orientation angles of  hundreds of bees on a 2D surface of honey comb. (Bozek K, Hebert L,  Mikheyev AS, Stephesn GJ) [Before 28/12/19]</li>
<li><a href="https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/531-mice-behaviour-analysis" target="_blank" rel="noopener">IIT MBADA mice</a> - Mice behavioral data. FLIR A315, spacial resolution of 320??240px at  30fps, 50x50cm open arena, two experts for three different mice pairs,  mice identities. (Italian Inst. of Technology, PAVIS lab) [Before  28/12/19]</li>
<li><a href="https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid" target="_blank" rel="noopener">Indian Diabetic Retinopathy Image Dataset</a> - This dataset consists of retinal fundus images annotated at  pixel-level for lesions associated with Diabetic Retinopathy. Also, it  provides the disease severity of diabetic retinopathy and diabetic  macular edema. This dataset is useful for development and evaluation of  image analysis algorithms for early detection of diabetic retinopathy.  (Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish  Deshmukh, Vivek Sahasrabuddhe, Fabrice Meriaudeau) [Before 28/12/19]</li>
<li><a href="https://ganymed.imib.rwth-aachen.de/irma/datasets_en.php?SELECTED=00009" target="_blank" rel="noopener">IRMA(Image retrieval in medical applications)</a> - This collection compiles anonymous radiographs (Deserno TM, Ott B) [Before 28/12/19]</li>
<li><a href="https://ivdm3seg.weebly.com/data.html" target="_blank" rel="noopener">IVDM3Seg</a> - 24 3D multi-modality MRI data sets of at least 7 IVDs of the lower spine,  collected from 12 subjects in two different stages (Zheng, Li, Belavy)  [Before 28/12/19]</li>
<li><a href="https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/" target="_blank" rel="noopener">JIGSAWS</a> - JHU-ISI Surgical Gesture and Skill Assessment Working Set (a surgical activity dataset for human motion modeling, captured using the da Vinci Surgical System from eight surgeons with different levels of skill  performing five repetitions of three elementary surgical tasks. It  contains: kinematic and video data, plus manual annotations. (Carol  Reiley and Balazs Vagvolgyi) [Before 28/12/19]</li>
<li><a href="http://is-innovation.eu/kid" target="_blank" rel="noopener">KID</a> - A capsule endoscopy database for medical decision support (Anastasios Koulaouzidis and Dimitris Iakovidis) [Before 28/12/19]</li>
<li><a href="http://www.plant-phenotyping.org/CVPPP2014-dataset" target="_blank" rel="noopener">Leaf Segmentation Challenge</a>Tobacco and arabidopsis plant images (Hanno Scharr, Massimo Minervini, Andreas  Fischbach, Sotirios A. Tsaftaris) [Before 28/12/19]</li>
<li><a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" target="_blank" rel="noopener">LIDC-IDRI</a> - Lung Image Database Consortium image collection (LIDC-IDRI) consists  of diagnostic and lung cancer screening thoracic computed tomography  (CT) scans with marked-up annotated lesions. (Before 30/12/19)  [Before  28/12/19]</li>
<li><a href="http://www.lits-challenge.com" target="_blank" rel="noopener">LITS Liver Tumor Segmentation</a> - 130 3D CT scans with segmentations of the liver and liver tumor.  Public benchmark with leaderboard at Codalab.org (Patrick Christ)  [Before 28/12/19]</li>
<li><a href="http://www.mammoimage.org/databases/" target="_blank" rel="noopener">Mammographic Image Analysis Homepage</a> - a collection of databases links [Before 28/12/19]</li>
<li><a href="http://onlinemedicalimages.com" target="_blank" rel="noopener">Medical image database</a> - Database of ultrasound images of breast abnormalities with the ground  truth. (Prof. Stanislav Makhanov, biomedsiit.com) [Before 28/12/19]</li>
<li><a href="http://www.mammoimage.org/databases/" target="_blank" rel="noopener">MiniMammographic Database</a> (Mammographic Image Analysis Society) [Before 28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/hueihan/" target="_blank" rel="noopener">MIT CBCL Automated Mouse Behavior Recognition datasets</a> (Nicholas Edelman) [Before 28/12/19]</li>
<li><a href="http://www.inf-cv.uni-jena.de/fgvcbiodiv" target="_blank" rel="noopener">Moth fine-grained recognition</a> - 675 similar classes, 5344 images (Erik Rodner et al) [Before 28/12/19]</li>
<li><a href="http://celltracking.bio.nyu.edu/" target="_blank" rel="noopener">Mouse Embryo Tracking Database</a> - cell division event detection (Marcelo Cicconet, Kris Gunsalus) [Before 28/12/19]</li>
<li><a href="http://cbia.fi.muni.cz/datasets/" target="_blank" rel="noopener">MUCIC: Masaryk University Cell Image Collection</a> - 2D/3D synthetic images of cells/tissues for benchmarking(Masaryk University) [Before 28/12/19]</li>
<li><a href="https://www.kaggle.com/nih-chest-xrays/data" target="_blank" rel="noopener">NIH Chest X-ray Dataset</a> - 112,120 X-ray images with disease labels from 30,805 unique patients. (NIH) [Before 28/12/19]</li>
<li><a href="http://www.oasis-brains.org/" target="_blank" rel="noopener">OASIS</a> - Open Access  Series of Imaging Studies - 500+ MRI data sets of the brain (Washington  University, Harvard University, Biomedical Informatics Research Network) [Before 28/12/19]</li>
<li><a href="http://www.plant-phenotyping.org/datasets-home" target="_blank" rel="noopener">Plant Phenotyping Datasets</a> - plant data suitable for plant and leaf detection, segmentation,  tracking, and species recognition (M. Minervini, A. Fischbach, H.  Scharr, S. A. Tsaftaris) [Before 28/12/19]</li>
<li><a href="http://www.noldus.com/innovationworks/datasets/ratsi" target="_blank" rel="noopener">RatSI: Rat Social Interaction Dataset</a> - 9 fully annotated (11 class) videos (15 minute, 25 FPS) of two rats  interacting socially in a cage (Malte Lorbach, Noldus Information  Technology) [Before 28/12/19]</li>
<li><a href="http://www.cs.rug.nl/~imaging/databases/retina_database/retinalfeatures_database.html" target="_blank" rel="noopener">Retinal fundus images - Ground truth of vascular bifurcations and crossovers</a> (Univ of Groningen) [Before 28/12/19]</li>
<li><a href="https://saras-esad.grand-challenge.org/Dataset/" target="_blank" rel="noopener">SARAS endoscopic vision challenge for surgeon action detection</a> - 22,601 annotated training frames with 28,055 action instances from 21 different action classes (Cuzzolin, Singh Bawa, Skarga-Bandurova,  Singh) [16/4/20]</li>
<li><a href="https://scorhe.nih.gov" target="_blank" rel="noopener">SCORHE</a> - 1, 2 and 3 mouse behavior videos, 9 behaviors, (Ghadi H. Salem, et al, NIH) [Before 28/12/19]</li>
<li><a href="https://web.northeastern.edu/ostadabbas/2019/06/27/multimodal-in-bed-pose-estimation/" target="_blank" rel="noopener">SLP (Simultaneously-collected multimodal Lying Pose)</a> -  large scale dataset on in-bed poses includes: 2 Data Collection  Settings: (a) Hospital setting: 7 participants, and (b) Home setting:  102 participants (29 females, age range: 20-40). 4 Imaging Modalities:  RGB (regular webcam), IR (FLIR LWIR camera), DEPTH (Kinect v2) and  Pressure Map (Tekscan Pressure Sensing Map). 3 Cover Conditions:  uncover, bed sheet, and blanket. Fully labeled poses with 14 joints.  (Ostadabbas and Liu) [2/1/20]</li>
<li><a href="http://brainiac2.mit.edu/SNEMI3D/" target="_blank" rel="noopener">SNEMI3D</a> - 3D Segmentation of neurites in EM images [Before 28/12/19]</li>
<li><a href="http://cecas.clemson.edu/~ahoover/stare/" target="_blank" rel="noopener">STructured Analysis of the Retina</a> - DESCRIPTION(400+ retinal images, with ground truth segmentations and  medical annotations) (Before 30/12/19)  [Before 28/12/19]</li>
<li><a href="http://www.digitalimaginggroup.ca/members/shuo.php" target="_blank" rel="noopener">Spine and Cardiac data</a> (Digital Imaging Group of London Ontario, Shuo Li) [Before 28/12/19]</li>
<li><a href="http://web.engr.oregonstate.edu/~tgd/bugid/stonefly9/" target="_blank" rel="noopener">Stonefly9</a>This database contains 3826 images of 773 specimens of 9 taxa of Stoneflies (Tom etc.) [Before 28/12/19]</li>
<li><a href="http://www.phagosight.org/synData.php" target="_blank" rel="noopener">Synthetic Migrating Cells</a> -Six artificial migrating cells (neutrophils) over 98 time frames,  various levels of Gaussian/Poisson noise and different paths  characteristics with ground truth. (Dr Constantino Carlos Reyes-Aldasoro et al.) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/ybenezeth/ubfcrppg" target="_blank" rel="noopener">UBFC-RPPG Dataset</a> -  remote photoplethysmography (rPPG) video data and ground truth  acquired with a CMS50E transmissive pulse oximeter (Bobbia, Macwan,  Benezeth, Mansouri, Dubois) [Before 28/12/19]</li>
<li><a href="http://cgvr.informatik.uni-bremen.de/research/asula/index.shtml" target="_blank" rel="noopener">Uni Bremen Open, Abdominal Surgery RGB Dataset</a> - Recording of a complete, open, abdominal surgery using a Kinect v2  that was mounted directly above the patient looking down at patient and  staff. (Joern Teuber, Gabriel Zachmann, University of Bremen) [Before  28/12/19]</li>
<li><a href="http://marathon.csee.usf.edu/Mammography/Database.html" target="_blank" rel="noopener">Univ of Central Florida - DDSM: Digital Database for Screening Mammography</a> (Univ of Central Florida) [Before 28/12/19]</li>
<li><a href="http://vascusynth.cs.sfu.ca" target="_blank" rel="noopener">VascuSynth</a> - 120 3D vascular tree like structures with ground truth (Mengliu Zhao, Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a href="http://vascusynth.cs.sfu.ca/Data.html" target="_blank" rel="noopener">VascuSynth</a> - Vascular Synthesizer generates vascular trees in 3D volumes. (Ghassan Hamarneh, Preet Jassi, Mengliu Zhao) [Before 28/12/19]</li>
<li><a href="http://www.cse.yorku.ca/~mridataset/" target="_blank" rel="noopener">York Cardiac MRI dataset</a> (Alexander Andreopoulos) [Before 28/12/19]</li>
</ol>
<h2 id="Camera-calibration"><a href="#Camera-calibration" class="headerlink" title="Camera calibration"></a>Camera calibration</h2><ol>
<li><a href="http://cvrg.iyte.edu.tr/datasets.htm" target="_blank" rel="noopener">Catadioptric camera calibration images</a> (Yalin Bastanlar) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/" target="_blank" rel="noopener">GoPro-Gyro Dataset</a> - This dataset consists of a number of wide-angle rolling shutter video sequences with corresponding gyroscope measurements (Hannes etc.)  [Before 28/12/19]</li>
<li><a href="http://cmp.felk.cvut.cz/software/LO-RANSAC/index.xhtml" target="_blank" rel="noopener">LO-RANSAC</a> - LO-RANSAC library for estimation of homography and epipolar geometry(K. Lebeda, J. Matas and O. Chum) [Before 28/12/19]</li>
</ol>
<h2 id="Face-and-Eye-Iris-Databases"><a href="#Face-and-Eye-Iris-Databases" class="headerlink" title="Face and Eye/Iris Databases"></a>Face and Eye/Iris Databases</h2><ol>
<li><a href="https://github.com/Juyong/3DFace" target="_blank" rel="noopener">2D-3D face dataset</a> -  This dataset includes pairs of 2D face image and its corresponding 3D  face geometry model with geometry details. (Yudong Guo, Juyong Zhang,  Jianfei Cai, Boyi Jiang, Jianmin Zheng) [Before 28/12/19]</li>
<li><a href="http://ibug.doc.ic.ac.uk/resources/300-VW/" target="_blank" rel="noopener">300 Videos in the Wild (300-VW)</a> - 68 Facial Landmark Tracking (Chrysos, Antonakos, Zafeiriou, Snape, Shen, Kossaifi, Tzimiropoulos, Pantic) [Before 28/12/19]</li>
<li><a href="https://github.com/D-X-Y/landmark-detection/blob/master/dataset.md" target="_blank" rel="noopener">300W-Style</a> - enhanced version of 300W by applying three style changes to the  original images. It is used to facilitate the analysis of the facial  landmark detection problem. (Xuanyi Dong) [29/12/19]</li>
<li><a href="https://www.idiap.ch/dataset/3dmad" target="_blank" rel="noopener">3D Mask Attack Database (3DMAD)</a> - 76500 frames of 17 persons using Kinect RGBD with eye positions (Sebastien Marcel) [Before 28/12/19]</li>
<li><a href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html" target="_blank" rel="noopener">3D facial expression</a> - Binghamton University 3D Static and Dynamic Facial Expression  Databases (Lijun Yin, Jeff Cohn, and teammates) [Before 28/12/19]</li>
<li><a href="ttps://github.com/D-X-Y/landmark-detection/blob/master/dataset.md">AFLW-Style</a> - enhanced version of AFLW by applying three style changes to the  original images. It is used to facilitate the analysis of the facial  landmark detection problem. (Xuanyi Dong) [29/12/19]</li>
<li><a href="https://drive.google.com/file/d/171iZQ8dqx3Yyp5t2gq06DtSWv9RMnNjG/view" target="_blank" rel="noopener">AginG Faces in the Wild v2</a> Database description: AGFW-v2 consists of 36,299 facial images divided  into 11 age groups with a span of five years between groups. On average, there are 3,300 images per group. Facial images in AGFW-v2 are not  public figures and less likely to have significant make-up or facial  modifications, helping embed accurate aging effects during the learning  process. (Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui) [Before  28/12/19]</li>
<li><a href="https://www.idiap.ch/dataset/mobio" target="_blank" rel="noopener">Audio-visual database for face and speaker recognition</a> (Mobile Biometry MOBIO <a href="http://www.mobioproject.org/" target="_blank" rel="noopener">http://www.mobioproject.org/</a>) [Before 28/12/19]</li>
<li><a href="http://spandh.dcs.shef.ac.uk/avlombard/" target="_blank" rel="noopener">Audiovisual Lombard grid speech corpus</a> - a bi-view audiovisual Lombard speech corpus which can be used to  support joint computational-behavioral studies in speech perception  (Alghamdi, Maddock, Marxer, Barker and Brown) [31/12/19]</li>
<li><a href="http://www.ee.surrey.ac.uk/CVSSP/banca/" target="_blank" rel="noopener">BANCA face and voice database</a> (Univ of Surrey) [Before 28/12/19]</li>
<li><a href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html" target="_blank" rel="noopener">Binghampton Univ 3D static and dynamic facial expression database</a> (Lijun Yin, Peter Gerhardstein and teammates) [Before 28/12/19]</li>
<li><a href="http://fera2015-db.sspnet.eu/media/doc/eula.pdf" target="_blank" rel="noopener">Binghamton-Pittsburgh 4D Spontaneous Facial Expression Database</a> - consist of 2D spontaneous facial expression videos and FACS codes. (Lijun Yin et al.) [Before 28/12/19]</li>
<li><a href="https://support.bioid.com/downloads/facedb/index.php" target="_blank" rel="noopener">BioID face database</a> (BioID group) [Before 28/12/19]</li>
<li><a href="http://www.iikt.ovgu.de/BioVid.html" target="_blank" rel="noopener">BioVid Heat Pain Database</a> - This video (and biomedical signal) dataset contains facial and  physiopsychological reactions of 87 study participants who were  subjected to experimentally induced heat pain. (University of Magdeburg  (Neuro-Information Technology group) and University of Ulm (Emotion  Lab)) [Before 28/12/19]</li>
<li><a href="http://zbum.ia.pw.edu.pl/EN/node/46" target="_blank" rel="noopener">Biometric databases</a> - biometric databases related to iris recognition (Adam Czajka) [Before 28/12/19]</li>
<li><a href="http://www.vision.ee.ethz.ch/datasets/b3dac2.en.html" target="_blank" rel="noopener">Biwi 3D Audiovisual Corpus of Affective Communication</a> - 1000 high quality, dynamic 3D scans of faces, recorded while pronouncing a set of English sentences.  [Before 28/12/19]</li>
<li><a href="http://bosphorus.ee.boun.edu.tr/default.aspx" target="_blank" rel="noopener">Bosphorus 3D/2D Database of FACS annotated facial expressions, of head poses and of face occlusions</a> (Bogazici University) [Before 28/12/19]</li>
<li><a href="https://caer-dataset.github.io/" target="_blank" rel="noopener">CAER (Context-Aware Emotion Recognition)</a> - Large scale image and video dataset for emotion recognition, and  facial expression recognition (Lee, Kim, Kim, Park, and Sohn) [29/12/19]</li>
<li><a href="http://www.gag.itu.edu.tr/CPdatabase/" target="_blank" rel="noopener">Caricature/Photomates dataset</a> - a dataset with frontal faces and corresponding Caricature line drawings (Tayfun Akgul) [Before 28/12/19]</li>
<li><a href="http://www.cbsr.ia.ac.cn/IrisDatabase.htm" target="_blank" rel="noopener">CASIA-IrisV3</a> (Chinese Academy of Sciences, T. N. Tan, Z. Sun) [Before 28/12/19]</li>
<li><a href="http://mrl.isr.uc.pt/experimentaldata/public/gaze-casir/" target="_blank" rel="noopener">CASIR Gaze Estimation Database</a> - RGB and depth images (from Kinect V1.0) and ground truth values of  facial features corresponding to experiments for gaze estimation  benchmarking: (Filipe Ferreira etc.) [Before 28/12/19]</li>
<li><a href="http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html" target="_blank" rel="noopener">Celeb-DF</a> - A new large-scale and challenging DeepFake video dataset, Celeb-DF,  for the development and evaluation of DeepFake detection algorithms (Li, Yang, Sun, Qi and Lyu) [30/12/19]</li>
<li><a href="http://vasc.ri.cmu.edu/idb/html/face/facial_expression/" target="_blank" rel="noopener">CMU Facial Expression Database</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a href="http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html" target="_blank" rel="noopener">The CMU Multi-PIE Face Database</a> - more than 750,000 images of 337 people recorded in up to four  sessions over the span of five months. (Jeff Cohn et al.) [Before  28/12/19]</li>
<li><a href="http://www.ri.cmu.edu/publication_view.html?pub_id=3462" target="_blank" rel="noopener">CMU Pose, Illumination, and Expression (PIE) Database</a> (Simon Baker) [Before 28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/FaceData2.html" target="_blank" rel="noopener">CMU/MIT Frontal Faces</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a href="http://vasc.ri.cmu.edu/idb/html/face/frontal_images/" target="_blank" rel="noopener">CMU/MIT Frontal Faces</a> (CMU/MIT) [Before 28/12/19]</li>
<li><a href="https://coma.is.tue.mpg.de/" target="_blank" rel="noopener">CoMA 3D face dataset</a> -  20,466 meshes (3D head scans and registrations in FLAME topology) of  extreme facial expressions captured from 12 different subjects (Ranjan,  Bolkart, Sanyal, Black) [Before 28/12/19]</li>
<li><a href="http://www.csse.uwa.edu.au/~ajmal/databases.html" target="_blank" rel="noopener">CSSE Frontal intensity and range images of faces</a> (Ajmal Mian) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="noopener">CelebA</a> - Large-scale CelebFaces Attributes Dataset(Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a href="http://www.cfpw.io/" target="_blank" rel="noopener">Celebrities in Frontal-Profile in the Wild</a> - 500+ images of celebrities in frontal and profile views (Sengupta,  Cheng, Castillo, Patel, Chellappa, Jacobs) [Before 28/12/19]</li>
<li><a href="http://www.pitt.edu/~emotion/ck-spread.htm" target="_blank" rel="noopener">Cohn-Kanade AU-Coded Expression Database</a> - 500+ expression sequences of 100+ subjects, coded by activated Action Units (Affect Analysis Group, Univ. of Pittsburgh) [Before 28/12/19]</li>
<li><a href="http://www.pitt.edu/~emotion/ck-spread.htm" target="_blank" rel="noopener">Cohn-Kanade AU-Coded Expression Database</a> - for research in automatic facial image analysis and synthesis and for perceptual studies (Jeff Cohn et al.) [Before 28/12/19]</li>
<li><a href="http://www.cs.columbia.edu/CAVE/databases/columbia_gaze/" target="_blank" rel="noopener">Columbia Gaze Data Set</a> - 5,880 images of 56 people over 5 head poses and 21 gaze directions  (Brian A. Smith, Qi Yin, Steven K. Feiner, Shree K. Nayar) [Before  28/12/19]</li>
<li><a href="http://lrv.fri.uni-lj.si/facedb.html" target="_blank" rel="noopener">Computer Vision Laboratory Face Database (CVL Face Database)</a> - Database contains 798 images of 114 persons, with 7 images per person and is freely available for research purposes. (Peter Peer etc.)  [Before 28/12/19]</li>
<li><a href="https://github.com/Mengmi/deepfuturegaze_gan" target="_blank" rel="noopener">Deep future gaze</a> - This dataset consists of 57 sequences on search and retrieval tasks  performed by 55 subjects. Each video clip lasts for around 15 minutes  with the frame rate 10 fps and frame resolution 480 by 640. Each subject is asked to search for a list of 22 items (including lanyard, laptop)  and move them to the packing location (dining table). (National  University of Singapore, Institute for Infocomm Research) [Before  28/12/19]</li>
<li><a href="http://mohammadmahoor.com/databases-codes/" target="_blank" rel="noopener">DISFA+:Extended Denver Intensity of Spontaneous Facial Action Database</a> - an  extension of DISFA (M.H. Mahoor) [Before 28/12/19]</li>
<li><a href="http://mohammadmahoor.com/databases-codes/" target="_blank" rel="noopener">DISFA:Denver Intensity of Spontaneous Facial Action Database</a> - a non-posed facial expression database for those who are interested  in developing computer algorithms for automatic action unit detection  and their intensities described by FACS. (M.H. Mahoor) [Before 28/12/19]</li>
<li><a href="https://github.com/wenguanwang/DHF1K" target="_blank" rel="noopener">DHF1K</a> - 1000 elaborately selected video sequences with fixation annotations from 17 viewers. (Prof. Jianbing Shen) [Before 28/12/19]</li>
<li><a href="http://fcd.eurecom.fr/" target="_blank" rel="noopener">EURECOM Facial Cosmetics Database</a> - 389 images, 50 persons with/without make-up, annotations about the  amount and location of applied makeup. (Jean-Luc DUGELAY et al) [Before  28/12/19]</li>
<li><a href="http://rgb-d.eurecom.fr/" target="_blank" rel="noopener">EURECOM Kinect Face Database</a> - 52 people, 2 sessions, 9 variations, 6 facial landmarks. (Jean-Luc DUGELAY et al) [Before 28/12/19]</li>
<li><a href="http://www.idiap.ch/dataset/eyediap" target="_blank" rel="noopener">EYEDIAP dataset</a> -  The EYEDIAP dataset was designed to train and evaluate gaze estimation  algorithms from RGB and RGB-D data.It contains a diversity of  participants, head poses, gaze targets and sensing conditions. (Kenneth  Funes and Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a href="http://face2bmi.csail.mit.edu/" target="_blank" rel="noopener">Face2BMI Dataset</a> The  Face2BMI dataset contains 2103 pairs of faces, with corresponding  gender, height and previous and current body weights, which allows for  training computer vision models that can predict body-mass index (BMI)  from profile pictures. (Enes Kocabey, Ferda Ofli, Yusuf Aytar, Javier  Marin, Antonio Torralba, Ingmar Weber) [Before 28/12/19]</li>
<li><a href="http://vis-www.cs.umass.edu/fddb/" target="_blank" rel="noopener">FDDB: Face Detection Data set and Benchmark - studying unconstrained face detection</a> (University of Massachusetts Computer Vision Laboratory) [Before 28/12/19]</li>
<li><a href="https://researchdata.sfu.ca/islandora/object/sfu:2722" target="_blank" rel="noopener">FDDB-360</a> - face detection in 360 degree fisheye images (Fu, Alvar, Bajic, and Vaughan) [29/12/19]</li>
<li><a href="http://www.fgnet.rsunit.com/" target="_blank" rel="noopener">FG-Net Aging Database of faces at different ages</a> (Face and Gesture Recognition Research Network) [Before 28/12/19]</li>
<li><a href="https://www.nist.gov/itl/iad/image-group/face-recognition-vendor-test-frvt-2013" target="_blank" rel="noopener">Face Recognition Grand Challenge datasets</a> (FRVT - Face Recognition Vendor Test) [Before 28/12/19]</li>
<li><a href="http://www.qirt.org/liens/FMTV.htm" target="_blank" rel="noopener">FMTV</a> - Laval Face  Motion and Time-Lapse Video Database. 238 thermal/video subjects with a  wide range of poses and facial expressions acquired over 4 years  (Ghiass, Bendada, Maldague) [Before 28/12/19]</li>
<li><a href="http://ies.anthropomatik.kit.edu/publ.php?key=ies_2016_qu_capturing" target="_blank" rel="noopener">Face Super-Resolution Dataset</a> - Ground truth HR-LR face images captured with a dual-camera setup (Chengchao Qu etc.) [Before 28/12/19]</li>
<li><a href="http://vintage.winklerbros.net/facescrub.html" target="_blank" rel="noopener">FaceScrub</a> - A Dataset With Over 100,000 Face Images of 530 People (50:50 male and female) (H.-W. Ng, S. Winkler) [Before 28/12/19]</li>
<li><a href="http://www1.cs.columbia.edu/CAVE/databases/facetracer/" target="_blank" rel="noopener">FaceTracer Database - 15,000 faces</a> (Neeraj Kumar, P. N. Belhumeur, and S. K. Nayar) [Before 28/12/19]</li>
<li><a href="http://www.affectiva.com/facial-expression-dataset/" target="_blank" rel="noopener">Facial Expression Dataset</a> - This dataset consists of 242 facial videos (168,359 frames) recorded  in real world conditions. (Daniel McDuff et al.) [Before 28/12/19]</li>
<li><a href="https://www.micc.unifi.it/resources/datasets/florence-3d-faces/" target="_blank" rel="noopener">Florence 2D/3D Hybrid Face Dataset</a> - bridges the gap between 2D, appearance-based recognition techniques,  and fully 3D approaches (Bagdanov, Del Bimbo, and Masi) [Before  28/12/19]</li>
<li><a href="http://www.itl.nist.gov/iad/humanid/feret/feret_master.html" target="_blank" rel="noopener">Facial Recognition Technology (FERET) Database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://gi4e.unavarra.es/databases/gi4e/" target="_blank" rel="noopener">Gi4E Database</a> -  eye-tracking database with 1300+ images acquired with a standard  webcam, corresponding to different subjects gazing at different points  on a screen, including ground-truth 2D iris and corner points  (Villanueva, Ponz, Sesma-Sanchez, Mikel Porta, and Cabeza) [Before  28/12/19]</li>
<li><a href="https://ai.google/tools/datasets/google-facial-expression/" target="_blank" rel="noopener">Google Facial Expression Comparison dataset</a> - a large-scale facial expression dataset consisting of face image  triplets along with human annotations that specify which two faces in  each triplet form the most similar pair in terms of facial expression,  which is different from datasets that focus mainly on discrete emotion  classification or action unit detection (Vemulapalli, Agarwala) [Before  28/12/19]</li>
<li><a href="http://www.isca-speech.org/iscapad/iscapad.php?module=article&id=11770" target="_blank" rel="noopener">Hannah and her sisters database</a> - a dense audio-visual person-oriented ground-truth annotation of  faces, speech segments, shot boundaries (Patrick Perez, Technicolor)  [Before 28/12/19]</li>
<li><a href="https://www-users.cs.york.ac.uk/~nep/research/Headspace/" target="_blank" rel="noopener">Headspace dataset</a> - The Headspace dataset is a set of 3D images of the full human head,  consisting of 1519 subjects wearing tight fitting latex caps to reduce  the effect of hairstyles. (Christian Duncan, Rachel Armstrong, Alder Hey Craniofacial Unit, Liverpool, UK) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/facesketch.html" target="_blank" rel="noopener">Hong Kong Face Sketch Database</a> [Before 28/12/19]</li>
<li><a href="https://www.idiap.ch/dataset/headpose" target="_blank" rel="noopener">IDIAP Head Pose Database (IHPD)</a> - The dataset contains a set of meeting videos along with the head  groundtruth of individual participants (around 128min)(Sileye Ba and  Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a href="https://www.nist.gov/programs-projects/face-challenges" target="_blank" rel="noopener">IARPA Janus Benchmark datasets</a> - IJB-A, IJB-B, IJB-C, FRVT (NIST) [Before 28/12/19]</li>
<li><a href="http://www.vision.ee.ethz.ch/en/datasets/" target="_blank" rel="noopener">IMDB-WIKI</a> - 500k+ face images with age and gender labels (Rasmus Rothe, Radu Timofte, Luc Van Gool ) [Before 28/12/19]</li>
<li><a href="http://cvit.iiit.ac.in/projects/IMFDB/" target="_blank" rel="noopener">Indian Movie Face database (IMFDB)</a> - a large unconstrained face database consisting of 34512 images of 100 Indian actors collected from more than 100 videos (Vijay Kumar and C V  Jawahar) [Before 28/12/19]</li>
<li><a href="http://www.iranprc.org/en/ifdb.php" target="_blank" rel="noopener">Iranian Face Database</a> - IFDB is the first image database in middle-east, contains color  facial images with age, pose, and expression whose subjects are in the  range of 2-85. (Mohammad Mahdi Dehshibi) [Before 28/12/19]</li>
<li><a href="http://www.kasrl.org/jaffe.html" target="_blank" rel="noopener">Japanese Female Facial Expression (JAFFE) Database</a> (Michael J. Lyons) [Before 28/12/19]</li>
<li><a href="https://childrenfacialexpression.projet.liris.cnrs.fr/" target="_blank" rel="noopener">LIRIS Children Spontaneous Facial Expression Video Database</a> - pontaneous / natural facial expressions of 12 children in diverse  settings with variable video recording scenarios showing six universal  or prototypic emotional expressions (happiness, sadness, anger,  surprise, disgust and fear). Children are recorded in constraint free  environment (no restriction on head movement, no restriction on hands  movement, free sitting setting, no restriction of any sort) while they  watched specially built / selected stimuli. This constraint free  environment allowed us to record spontaneous / natural expression of  children as they occur. The database has been validated by 22 human  raters. (Khan, Crenn, Meyer, Bouakaz) [29/12/19]</li>
<li><a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="noopener">LFW: Labeled Faces in the Wild</a> - unconstrained face recognition [Before 28/12/19]</li>
<li><a href="https://adrianbulat.com/face-alignment" target="_blank" rel="noopener">LS3D-W</a> - a  large-scale 3D face alignment dataset annotated with 68 points  containing faces captured in a “in-the-wild” setting. (Adrian Bulat,  Georgios Tzimiropoulos) [Before 28/12/19]</li>
<li><a href="http://www.escience.cn/people/geshiming/mafa.html" target="_blank" rel="noopener">MAFA: MAsked FAces</a> - 30,811 images with  35,806 labeled MAsked FAces, six main attributes  of each masked face. (Shiming Ge, Jia Li,  Qiting Ye, Zhao Luo) [Before  28/12/19]</li>
<li><a href="http://www.antitza.com/makeup-datasets.html" target="_blank" rel="noopener">Makeup Induced Face Spoofing (MIFS)</a> - 107 makeup-transformations attempting to spoof a target identity. Also other datasets. (Antitza Dantcheva) [Before 28/12/19]</li>
<li><a href="http://www.labri.fr/projet/AIV/MexCulture142.php" target="_blank" rel="noopener">Mexculture142</a> - Mexican Cultural heritage objects and eye-tracker gaze fixations  (Montoya Obeso, Benois-Pineau, Garcia-Vazquez, Ramirez Acosta) [Before  28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/heisele/facerecognition-database.html" target="_blank" rel="noopener">MIT CBCL Face Recognition Database</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a href="http://web.mit.edu/emeyers/www/face_databases.html" target="_blank" rel="noopener">MIT Collation of Face Databases</a> (Ethan Meyers) [Before 28/12/19]</li>
<li><a href="http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html" target="_blank" rel="noopener">MIT eye tracking database (1003 images)</a> (Judd et al) [Before 28/12/19]</li>
<li><a href="http://mmifacedb.eu/" target="_blank" rel="noopener">MMI Facial Expression Database</a> - 2900 videos and high-resolution still images of 75 subjects, annotated for FACS AUs. [Before 28/12/19]</li>
<li><a href="http://www.faceaginggroup.com/projects.html" target="_blank" rel="noopener">MORPH (Craniofacial Longitudinal Morphological Face Database)</a> (University of North Carolina Wilmington) [Before 28/12/19]</li>
<li><a href="http://www.mpi-inf.mpg.de/MPIIGazeDataset" target="_blank" rel="noopener">MPIIGaze dataset</a> - 213,659 samples with eye images and gaze target under different  illumination conditions and nature head movement, collected from 15  participants with their laptop during daily using. (Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling.) [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/data/talking_face/talking_face.html" target="_blank" rel="noopener">Manchester Annotated Talking Face Video Dataset</a> (Timothy Cootes) [Before 28/12/19]</li>
<li><a href="http://megaface.cs.washington.edu/dataset/download.html" target="_blank" rel="noopener">MegaFace</a> - 1 million faces in bounding boxes (Kemelmacher-Shlizerman, Seitz, Nech, Miller, Brossard) [Before 28/12/19]</li>
<li><a href="http://shunzhang.me.pn/papers/eccv2016/" target="_blank" rel="noopener">Music video dataset</a> - 8 music videos from YouTube for developing multi-face tracking  algorithms in unconstrained environments (Shun Zhang, Jia-Bin Huang,  Ming-Hsuan Yang) [Before 28/12/19]</li>
<li><a href="http://www.nist.gov/itl/iad/ig/frgc.cfm" target="_blank" rel="noopener">NIST Face Recognition Grand Challenge (FRGC)</a> (NIST) [Before 28/12/19]</li>
<li><a href="http://www.nist.gov/srd/nistsd18.cfm" target="_blank" rel="noopener">NIST mugshot identification database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://www.videorecognition.com/db/video/faces/cvglab/" target="_blank" rel="noopener">NRC-IIT Facial Video Database</a> - this database contains pairs of short video clips each showing a face of a computer user sitting in front of the monitor exhibiting a  wide  range of facial expressions and  orientations (Dmitry Gorodnichy)  [Before 28/12/19]</li>
<li><a href="http://www.nd.edu/~cvrl/CVRL/Data_Sets.html" target="_blank" rel="noopener">Notre Dame Iris Image Dataset</a> (Patrick J. Flynn) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets" target="_blank" rel="noopener">Notre Dame face, IR face, 3D face, expression, crowd, and eye biometric datasets</a> (Notre Dame) [Before 28/12/19]</li>
<li><a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" rel="noopener">ORL face database: 40 people with 10 views</a> (ATT Cambridge Labs) [Before 28/12/19]</li>
<li><a href="http://www.openu.ac.il/home/hassner/Adience/data.html" target="_blank" rel="noopener">OUI-Adience Faces</a> - unfiltered faces for gender and age classification plus 3D faces (OUI) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data.html" target="_blank" rel="noopener">Oxford: faces, flowers, multi-view, buildings, object categories, motion segmentation, affine covariant regions, misc</a> (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a href="http://imagelab.ing.unimore.it/pandora/" target="_blank" rel="noopener">Pandora</a> - POSEidon: Face-from-Depth for Driver Pose (Borghi, Venturelli, Vezzani, Cucchiara) [Before 28/12/19]</li>
<li><a href="http://www1.cs.columbia.edu/CAVE/databases/pubfig/" target="_blank" rel="noopener">PubFig: Public Figures Face Database</a> (Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar) [Before 28/12/19]</li>
<li><a href="https://qmul-survface.github.io/" target="_blank" rel="noopener">QMUL-SurvFace</a> - A  large-scale face recognition benchmark dedicated for real-world  surveillance face analysis and matching. (QMUL Computer Vision Group)  [Before 28/12/19]</li>
<li><a href="http://vis-www.cs.umass.edu/lfw/#deepfunnel-anchor" target="_blank" rel="noopener">Re-labeled Faces in the Wild</a> - original images, but aligned using “deep funneling” method. (University of Massachusetts, Amherst) [Before 28/12/19]</li>
<li><a href="https://zenodo.org/record/2529036" target="_blank" rel="noopener">RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments</a> 122,531 images with the subjects’ ground truth eye gaze and head pose  labels under free-viewing conditions and large camera-subject distances  (Fischer, Chang, Demiris, Imperial College London) [Before 28/12/19]</li>
<li><a href="http://groups.inf.ed.ac.uk/trimbot2020/DYNAMICFACES/index.html" target="_blank" rel="noopener">S3DFM</a> - Edinburgh Speech-driven 3D Facial Motion Database. 77 people with 10  repetitions of speaking a passphrase: 1 second of 500 frame per second  600x600 pixels of {IR intensity video, registered depth images} plus  synchronized 44.1 Khz audio. There are an additional 26 people (10  repetitions) moving their heads while speaking (Zhang, Fisher) [Before  28/12/19]</li>
<li><a href="http://jov.arvojournals.org/article.aspx?articleid=2193194" target="_blank" rel="noopener">Salient features in gaze-aligned recordings of human visual input</a> - TB of human gaze-contingent data “in the wild” (Frank Schumann etc.) [Before 28/12/19]</li>
<li><a href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php" target="_blank" rel="noopener">SAMM Dataset of Micro-Facial Movements</a> - The dataset contains 159 spontaneous micro-facial movements obtained  from 32 participants from 13 different ethnicities. (A.Davison,  C.Lansley, N.Costen, K.Tan, M.H.Yap) [Before 28/12/19]</li>
<li><a href="http://www.scface.org/" target="_blank" rel="noopener">SCface</a> - Surveillance Cameras Face Database (Mislav Grgic, Kresimir Delac, Sonja Grgic, Bozidar Klimpak) [Before 28/12/19]</li>
<li><a href="http://www.polito.it/cgvg/siblingsDB.html" target="_blank" rel="noopener">SiblingsDB</a> - The SiblingsDB contains two datasets depicting images of individuals  related by sibling relationships. (Politecnico di Torino/Computer  Graphics &amp; Vision Group) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/sof-dataset" target="_blank" rel="noopener">SoF dataset</a> - 42,592 face images with glasses under different illumination  conditions; provided with face region, facial landmarks, facial  expression, subject ID, gender, and age information (Afifi, Abdelhamed)  [29/12/19]</li>
<li><a href="https://data.nal.usda.gov/dataset/data-solving-robot-world-hand-eyes-calibration-problem-iterative-methods" target="_blank" rel="noopener">Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods</a> - These datasets were generated for calibrating robot-camera systems. (Amy Tabb) [Before 28/12/19]</li>
<li>[Spontaneous Emotion Multimodal Database (SEM-db)](<a href="http://staffnet.kingston.ac.uk/~ku43576/?page" target="_blank" rel="noopener">http://staffnet.kingston.ac.uk/~ku43576/?page</a> id=414) - non-posed reactions to visual stimulus data recorded with HD RGB,  depth  and IR frames of the face, EEG signal and eye gaze data  (Fernandez. Montenegro, Gkelias, Argyriou) [Before 28/12/19]</li>
<li><a href="http://www.pitt.edu/~emotion/um-spread.htm" target="_blank" rel="noopener">The UNBC-McMaster Shoulder Pain Expression Archive Database</a> - Painful data: The UNBC-McMaster Shoulder Pain Expression Archive Database (Lucy et al.) [Before 28/12/19]</li>
<li><a href="https://voca.is.tue.mpg.de/" target="_blank" rel="noopener">VOCASET</a> - 4D face dataset  with about 29 minutes of 3D head scans captured at 60 fps and  synchronized audio from 12 speakers (Cudeiro, Bolkart, Laidlaw, Ranjan,  Black) [Before 28/12/19]</li>
<li><a href="https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets" target="_blank" rel="noopener">Trondheim Kinect RGB-D Person Re-identification Dataset</a> (Igor Barros Barbosa) [Before 28/12/19]</li>
<li><a href="http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm" target="_blank" rel="noopener">UB KinFace Database</a> - University of Buffalo kinship verification and recognition database [Before 28/12/19]</li>
<li><a href="http://iris.di.ubi.pt/" target="_blank" rel="noopener">UBIRIS: Noisy Visible Wavelength Iris Image Databases</a> (University of Beira) [Before 28/12/19]</li>
<li><a href="http://umdfaces.io/" target="_blank" rel="noopener">UMDFaces</a> - About 3.7 million  annotated video frames from 22,000 videos and 370,000 annotated still  images. (Ankan Bansal et al.) [Before 28/12/19]</li>
<li><a href="http://gi4e.unavarra.es/databases/hpdb/" target="_blank" rel="noopener">UPNA Head Pose Database</a> - head pose database, with 120 webcam videos containing guided-movement sequences and free-movement sequences, including ground-truth head pose and automatically annotated 2D facial points. (Ariz, Bengoechea,  Villanueva, Cabeza) [Before 28/12/19]</li>
<li><a href="http://gi4e.unavarra.es/databases/shpdb/" target="_blank" rel="noopener">UPNA Synthetic Head Pose Database</a> - a synthetic replica of the UPNA Head Pose Database, with 120 videos  with their 2D ground truth landmarks projections, their corresponding  head pose ground truth, 3D head models and camera parameters. (Larumbe,  Segura, Ariz, Bengoechea, Villanueva, Cabeza) [Before 28/12/19]</li>
<li><a href="http://utiris.wordpress.com/" target="_blank" rel="noopener">UTIRIS cross-spectral iris image databank</a> (Mahdi Hosseini) [Before 28/12/19]</li>
<li><a href="https://www.uva-nemo.org/" target="_blank" rel="noopener">UvA-NEMO Smile Database</a> -   1240 smile videos (597 spontaneous and 643 posed) from 400 subjects,  including age, gender, and kinship annotations (Gevers, Dibeklioglu,  Salah) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/" target="_blank" rel="noopener">VGGFace2</a> - VGGFace2 is a large-scale face recognition dataset covering large  variations in pose, age, illumination, ethnicity and profession. (Oxford Visual Geometry Group) [Before 28/12/19]</li>
<li><a href="http://www.ihitworld.com/index.php/vipsl-database" target="_blank" rel="noopener">VIPSL Database</a> - VIPSL Database is for research on face sketch-photo synthesis and  recognition, including 200 subjects (1 photo and 5 sketches per  subject). (Nannan Wang) [Before 28/12/19]</li>
<li><a href="https://github.com/kreimanlab/VisualSearchZeroShot" target="_blank" rel="noopener">Visual Search Zero Shot Database</a> - Collection of human eyetracking data in three increasingly complex  visual search tasks: object arrays, natural images and Waldo images.  (Kreiman lab) [Before 28/12/19]</li>
<li><a href="http://sufficiency.ece.vt.edu/VT-KFER/" target="_blank" rel="noopener">VT-KFER</a>: A  Kinect-based RGBD+Time Dataset for Spontaneous and Non-Spontaneous  Facial Expression Recognition - 32 subjects, 1,956 sequences of RGBD,  six facial expressions in 3 poses (Aly, Trubanova, Abbott, White, and  Youssef) [Before 28/12/19]</li>
<li><a href="http://grail.cs.washington.edu/projects/deepexpr/ferg-db.html" target="_blank" rel="noopener">Washington Facial Expression Database (FERG-DB)</a> - a database of 6 stylized (Maya) characters with 7 annotated facial  expressions (Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro,  and Barbara Mones) [Before 28/12/19]</li>
<li><a href="http://cs.nju.edu.cn/rl/WebCaricature.htm" target="_blank" rel="noopener">WebCaricature Dataset</a> - The WebCaricature dataset is a large photograph-caricature dataset  consisting of 6042 caricatures and 5974 photographs from 252 persons  collected from the web. (Jing Huo, Wenbin Li, Yinghuan Shi, Yang Gao and Hujun Yin) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE: A Face Detection Benchmark</a> - 32,203 images with 393,703 labeled faces, 61 event classes (Shuo  Yang, Ping Luo, Chen Change Loy, Xiaoou Tang) [Before 28/12/19]</li>
<li><a href="https://researchdata.sfu.ca/pydio_public/c09804" target="_blank" rel="noopener">Wider-360</a> - Datasets for face and object detection in fisheye images (Fu, Bajic, and Vaughan) [29/12/19]</li>
<li><a href="http://www.ee.surrey.ac.uk/CVSSP/xm2vtsdb/" target="_blank" rel="noopener">XM2VTS Face video sequences (295): The extended M2VTS Database (XM2VTS) -</a> (Surrey University) [Before 28/12/19]</li>
<li><a href="http://www.face-rec.org/databases/" target="_blank" rel="noopener">Yale Face Database - 11 expressions of 10 people</a> (A. Georghaides) [Before 28/12/19]</li>
<li><a href="http://www.face-rec.org/databases/" target="_blank" rel="noopener">Yale Face Database B - 576 viewing conditions of 10 people</a> (A. Georghaides) [Before 28/12/19]</li>
<li><a href="https://www-users.cs.york.ac.uk/~nep/research/YEM/" target="_blank" rel="noopener">York 3D Ear Dataset</a> - The York 3D Ear Dataset  is a set of 500 3D ear images, synthesized  from detailed 2D landmarking, and available in both Matlab format (.mat) and PLY format (.ply). (Nick Pears, Hang Dai, Will Smith, University of York) [Before 28/12/19]</li>
<li><a href="http://www-sop.inria.fr/members/Neil.Bruce/eyetrackingdata.zip" target="_blank" rel="noopener">York Univ Eye Tracking Dataset (120 images)</a> (Neil Bruce) [Before 28/12/19]</li>
<li><a href="http://www.cs.tau.ac.il/~wolf/ytfaces/" target="_blank" rel="noopener">YouTube Faces DB</a> - 3,425 videos of 1,595 different people. (Wolf, Hassner, Maoz) [Before 28/12/19]</li>
<li><a href="https://www.tu-chemnitz.de/physik/PHKP/ZurichNatImgDB.html.en" target="_blank" rel="noopener">Zurich Natural Image</a> - the image material used for creating natural stimuli in a series of eye-tracking studies (Frey et al.) [Before 28/12/19]</li>
</ol>
<h2 id="Fingerprints"><a href="#Fingerprints" class="headerlink" title="Fingerprints"></a>Fingerprints</h2><ol>
<li><a href="http://bias.csr.unibo.it/fvc2002/databases.asp" target="_blank" rel="noopener">FVC fingerpring verification competition 2002 dataset</a> (University of Bologna) [Before 28/12/19]</li>
<li><a href="http://bias.csr.unibo.it/fvc2004/databases.asp" target="_blank" rel="noopener">FVC fingerpring verification competition 2004 dataset</a> (University of Bologna) [Before 28/12/19]</li>
<li><a href="http://www.ekds.gov.tr/bio/databases.html" target="_blank" rel="noopener"> Fingerprint Manual Minutiae Marker (FM3) Databases:</a> - Fingerprint Manual Minutiae Marker (FM3) Databases( Mehmet Kayaoglu, Berkay Topcu and Umut Uludag) [Before 28/12/19]</li>
<li><a href="http://srdata.nist.gov/gateway/gateway?keyword=fingerprint" target="_blank" rel="noopener">NIST fingerprint databases</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://www2.inescporto.pt/ip-en/news-events/events/spd2010-fingerprint-singular-points-detection/" target="_blank" rel="noopener">SPD2010 Fingerprint Singular Points Detection Competition</a> (SPD 2010 committee) [Before 28/12/19]</li>
</ol>
<h2 id="General-Images"><a href="#General-Images" class="headerlink" title="General Images"></a>General Images</h2><ol>
<li><a href="http://adrianbarburesearch.blogspot.com/p/renoir-dataset.html" target="_blank" rel="noopener">A Dataset for Real Low-Light Image Noise Reduction</a> - It contains pixel and intensity aligned pairs of images corrupted by  low-light camera noise and their low-noise counterparts. (J. Anaya, A.  Barbu) [Before 28/12/19]</li>
<li><a href="https://dx.doi.org/10.6084/m9.figshare.3370627" target="_blank" rel="noopener">A database of paintings related to Vincent van Gogh</a> - This is the dataset VGDB-2016 built for the paper “From Impressionism to Expressionism: Automatically Identifying Van Gogh’s Paintings”  (Guilherme Folego and Otavio Gomes and Anderson Rocha) [Before 28/12/19]</li>
<li><a href="http://www.cse.wustl.edu/~jacobsn/projects/webcam_dataset/" target="_blank" rel="noopener">AMOS: Archive of Many Outdoor Scenes (20+m)</a> (Nathan Jacobs) [Before 28/12/19]</li>
<li><a href="https://www.dropbox.com/s/eqs536fcnj6ns4x/colorbuilding_dataset.zip?dl=0" target="_blank" rel="noopener">Aerial images</a>Building detection from aerial images using invariant color features and shadow information. (Beril Sirmacek) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/file/d/0B_3Nh0OK9BclUko4ZjkxRDFaMkU/view" target="_blank" rel="noopener">Approximated overlap error dataset</a>Image pairs with sparse sets of ground-truth matches for evaluating local image descriptors (Fabio Bellavia) [Before 28/12/19]</li>
<li><a href="https://auto-da.github.io/" target="_blank" rel="noopener">AutoDA (Automatic Dataset Augmentation)</a> - An automatically constructed image dataset including 12.5 million  images with relevant textual information for the 1000 categories of  ILSVRC2012 (Bai, Yang, Ma, Zhao) [Before 28/12/19]</li>
<li><a href="http://icvl.cs.bgu.ac.il/hyperspectral/" target="_blank" rel="noopener">BGU Hyperspectral Image Database of Natural Scenes</a> (Ohad Ben-Shahar and Boaz Arad) [Before 28/12/19]</li>
<li><a href="http://vision.lems.brown.edu/content/available-software-and-databases" target="_blank" rel="noopener">Brown Univ Large Binary Image Database</a> (Ben Kimia) [Before 28/12/19]</li>
<li><a href="https://www.dropbox.com/sh/3p4x1oc5efknd69/AABwnyoH2EKi6H9Emcyd0pXCa?dl=0" target="_blank" rel="noopener">Butterfly-200</a> - Butterfly-20 is a image dataset for fine-grained image  classification, which contains 25,279 images and covers four levels  categories of 200 species, 116 genera, 23 subfamilies, and 5 families.  (Tianshui Chen) [Before 28/12/19]</li>
<li><a href="https://github.com/mahmoudnafifi/WB_color_augmenter" target="_blank" rel="noopener">CIFAR-10 classes with different WB settings</a> - 15,098 rendered images that reflect real in-camera white-balance settings (Afifi, Brown) [29/12/19]</li>
<li><a href="http://cmp.felk.cvut.cz/~tylecr1/facade/" target="_blank" rel="noopener">CMP Facade Database</a> - Includes 606 rectified images of facades from various places with 12  architectural classes annotated. (Radim Tylecek) [Before 28/12/19]</li>
<li><a href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html" target="_blank" rel="noopener">Caltech-UCSD Birds-200-2011</a> (Catherine Wah) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/drive/u/1/folders/0B_3Nh0OK9BclQkt4empQVU5yVE0" target="_blank" rel="noopener">Color correction dataset</a> - Homography-based registered images for evaluating color correction  algorithms for image stitching. (Fabio Bellavia) [Before 28/12/19]</li>
<li><a href="http://www1.cs.columbia.edu/CAVE/databases/multispectral/" target="_blank" rel="noopener">Columbia Multispectral Image Database</a> (F. Yasuma, T. Mitsunaga, D. Iso, and S.K. Nayar) [Before 28/12/19]</li>
<li><a href="http://mpii.de/visual_turing_test" target="_blank" rel="noopener">DAQUAR (Visual Turing Challenge)</a> - A dataset containing questions and answers about real-world indoor  scenes. (Mateusz Malinowski, Mario Fritz) [Before 28/12/19]</li>
<li><a href="https://noise.visinf.tu-darmstadt.de/" target="_blank" rel="noopener">Darmstadt Noise Dataset</a> - 50 pairs of real noisy images and corresponding ground truth images  (RAW and sRGB) (Tobias Plotz and Stefan Roth) [Before 28/12/19]</li>
<li><a href="https://github.com/tadarsh/movie-trailers-dataset" target="_blank" rel="noopener">Dataset of American Movie Trailers 2010-2014</a> - Contains links to 474 hollywood movie trailers along with associated metadata (genre, budget, runtime,  release, MPAA rating, screens released, sequel indicator) (USC Signal  Analysis and Interpretation Lab) [Before 28/12/19]</li>
<li><a href="http://diml.yonsei.ac.kr/~srkim/DASC/" target="_blank" rel="noopener">DIML Multimodal Benchmark</a> - To evaluate matching performance under photometric and geometric  variations, 100 images of 1200 x 800 size. (Yonsei University) [Before  28/12/19]</li>
<li><a href="http://dped-photos.vision.ee.ethz.ch" target="_blank" rel="noopener">DSLR Photo Enhancement Dataset (DPED)</a> - 22K photos taken synchronously in the wild by three smartphones and  one DSLR camera, useful for comparing infered high quality images from  multiple low quality images (Ignatov, Kobyshev, Timofte, Vanhoey, and  Van Gool). [Before 28/12/19]</li>
<li><a href="http://vislab.berkeleyvision.org/" target="_blank" rel="noopener">Flickr-style</a> - 80K  Flickr photographs annotated with 20 curated style labels, and 85K  paintings annotated with 25 style/genre labels (Sergey Karayev) [Before  28/12/19]</li>
<li><a href="https://yingqianwang.github.io/Flickr1024/" target="_blank" rel="noopener">Flickr1024: A Dataset for Stereo Image Super-resolution</a> - 1024 high-quality images pairs and covers diverse senarios (Wang, Wang, Yang, An, Guo) [Before 28/12/19]</li>
<li><a href="http://www.ics.forth.gr/cvrl/msi/" target="_blank" rel="noopener">Forth Multispectral Imaging Datasets</a> - images from 23 spectral bands each from 5 paintings. Images are  annotated with ground truth data. (Karamaoynas Polykarpos et al) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html" target="_blank" rel="noopener">General 100 Dataset</a> - General-100 dataset contains 100 bmp-format images (with no  compression), which are well-suited for super-resolution training(Dong,  Chao and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
<li><a href="https://github.com/SeungjunNah/DeepDeblur_release" target="_blank" rel="noopener">GOPRO dataset</a> - Blurred image dataset with sharp image ground truth (Nah, Kim, and Lee) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/typelib.htm" target="_blank" rel="noopener">HIPR2 Image Catalogue of different types of images</a> (Bob Fisher et al) [Before 28/12/19]</li>
<li><a href="https://hpatches.github.io/" target="_blank" rel="noopener">HPatches</a> - A benchmark and evaluation of handcrafted and learned local descriptors (Balntas, Lenc, Vedaldi, Mikolajczyk) [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Local_Illumination_HSIs/Local_Illumination_HSIs_2015.html" target="_blank" rel="noopener">Hyperspectral images for spatial distributions of local illumination in natural scenes</a> - Thirty calibrated hyperspectral radiance images of natural scenes  with probe spheres embedded for local illumination estimation.  (Nascimento, Amano &amp; Foster) [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_02.html" target="_blank" rel="noopener">Hyperspectral images of natural scenes - 2002</a> (David H. Foster) [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/david.foster/Hyperspectral_images_of_natural_scenes_04.html" target="_blank" rel="noopener">Hyperspectral images of natural scenes - 2004</a> (David H. Foster) [Before 28/12/19]</li>
<li><a href="http://www2.isprs.org/commissions/comm1/icwg15b/benchmark_main.html" target="_blank" rel="noopener">ISPRS multi-platform photogrammetry dataset</a> - 1: Nadir and oblique aerial images plus 2: Combined UAV and  terrestrial images (Francesco Nex and Markus Gerke) [Before 28/12/19]</li>
<li><a href="http://live.ece.utexas.edu/research/Quality/index.htm" target="_blank" rel="noopener">Image &amp; Video Quality Assessment at LIVE</a> - used to develop picture quality algorithms (the University of Texas at Austin) [Before 28/12/19]</li>
<li><a href="http://www.image-net.org/challenges/LSVRC/" target="_blank" rel="noopener">ImageNet Large Scale Visual Recognition Challenges</a> - Currently 200 object classes and 500+K images (Alex Berg, Jia Deng, Fei-Fei Li and others) [Before 28/12/19]</li>
<li><a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet Linguistically organised (WordNet) Hierarchical Image Database - 10E7 images, 15K categories</a> (Li Fei-Fei, Jia Deng, Hao Su, Kai Li) [Before 28/12/19]</li>
<li><a href="https://collections.durham.ac.uk/catalog?utf8=✓&q=breckon" target="_blank" rel="noopener">Improved 3D Sparse Maps for High-performance Structure from Motion with Low-cost Omnidirectional Robots - Evaluation Dataset</a> - Data set used in research paper doi:10.1109/ICIP.2015.7351744 (Breckon, Toby P., Cavestany, Pedro) [Before 28/12/19]</li>
<li><a href="http://database.mmsp-kn.de" target="_blank" rel="noopener">Konstanz visual quality databases</a> - Large-scale image and video databases for the development and  evaluation of visual quality assessment algorithms. (MMSP group,  University of Konstanz) [Before 28/12/19]</li>
<li><a href="http://www4.comp.polyu.edu.hk/~cslzhang/CDM_Dataset.htm" target="_blank" rel="noopener">Kodak McMaster demosaic dataset</a> - (Zhang, Wu, Buades, Li) [Before 28/12/19]</li>
<li><a href="http://www.inf-cv.uni-jena.de/Research/Datasets/LabelMeFacade+Database.html" target="_blank" rel="noopener">LabelMeFacade Database</a> - 945 labeled building images (Erik Rodner et al) [Before 28/12/19]</li>
<li><a href="http://online.uminho.pt/pessoas/smcn/hsi_spatial/HSI_illumination_2015.html" target="_blank" rel="noopener">Local illumination hyperspectral radiance images</a> - Thirty hyperspectral radiance images of natural scenes with embedded  probe spheres for local illumination estimates(Sgio M. C. Nascimento,  Kinjiro Amano, David H. Foster) [Before 28/12/19]</li>
<li><a href="http://pirsquared.org/research/mcgilldb/" target="_blank" rel="noopener">McGill Calibrated Colour Image Database</a> (Adriana Olmos and Fred Kingdom) [Before 28/12/19]</li>
<li><a href="http://www.sz.tsinghua.edu.cn/labs/vipl/mdid.html" target="_blank" rel="noopener">Multiply Distorted Image Database</a> -a database for evaluating the results of image quality assessment  metrics on multiply distorted images. (Fei Zhou) [Before 28/12/19]</li>
<li><a href="https://github.com/D-X-Y/NAS-Projects/blob/master/NAS-Bench-102.md" target="_blank" rel="noopener">NAS-Bench-102</a> - An algorithm-agnostic nas benchmark with detailed information  (training/validation/test loss/accuracy etc) of 15,625 architectures on  three datasets. (Xuanyi Dong) [29/12/19]</li>
<li><a href="http://gigl.scs.carleton.ca/benchmark_npr_general" target="_blank" rel="noopener">NPRgeneral</a> - A standardized collection of images for evaluating image stylization algorithms. (David Mould, Paul Rosin) [Before 28/12/19]</li>
<li><a href="https://www.nuscenes.org/" target="_blank" rel="noopener">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a href="http://symmetry.cs.nyu.edu/" target="_blank" rel="noopener">NYU Symmetry Database</a> - 176 single-symmetry and 63 multyple-symmetry images (Marcelo Cicconet and Davi Geiger) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/oceandark/home" target="_blank" rel="noopener">OceanDark dataset</a> - 100 low-lighting underwater images from underwater sites in the  Northeast Pacific Ocean. 1400x1000 pixels, varying lighting and  recording conditions (Ocean Networks Canada) [Before 28/12/19]</li>
<li><a href="http://vcipl-okstate.org/pbvs/bench/" target="_blank" rel="noopener">OTCBVS Thermal Imagery Benchmark Dataset Collection</a> (Ohio State Team) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/passta/" target="_blank" rel="noopener">PAnorama Sparsely STructured Areas Datasets</a> - the PASSTA datasets used for evaluation of the image alignment (Andreas Robinson) [Before 28/12/19]</li>
<li><a href="https://qmul-openlogo.github.io/" target="_blank" rel="noopener">QMUL-OpenLogo</a> - A  logo detection benchmark for testing the model generalisation capability in detecting a variety of logo objects in natural scenes with the  majority logo classes unlabelled. (QMUL Computer Vision Group) [Before  28/12/19]</li>
<li><a href="https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=3D0" target="_blank" rel="noopener">RESIDE (Realistic Single Image DEhazing)</a> - The current largest-scale benchmark consisting of both synthetic and  real-world hazy images, for image dehazing research.  RESIDE highlights  diverse data sources and image contents, and serves various training or  evaluation purposes. (Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan  Feng, Wenjun Zeng, Zhangyang Wang) [Before 28/12/19]</li>
<li><a href="https://figshare.com/articles/Rijksmuseum_Challenge_2014/5660617" target="_blank" rel="noopener">Rijksmuseum Challenge 2014</a> - It consist of 100K art objects from the rijksmuseum and comes with an extensive xml files describing each object. (Thomas Mensink and Jan van Gemert) [Before 28/12/19]</li>
<li><a href="http://web.engr.illinois.edu/~cchen156/SID.html" target="_blank" rel="noopener">See in the Dark</a> - 77 Gb of dark images (Chen, Chen, Xu, and Koltun) [Before 28/12/19]</li>
<li><a href="https://www.eecs.yorku.ca/~kamel/sidd/" target="_blank" rel="noopener">Smartphone Image Denoising Dataset (SIDD)</a> - The Smartphone Image Denoising Dataset (SIDD) consists of about  30,000 noisy images with corresponding high-quality ground truth in both raw-RGB and sRGB spaces obtained from 10 scenes with different lighting conditions using five representative smartphone cameras. (Abdelrahman  Abdelhamed, Stephen Lin, Michael S. Brown) [Before 28/12/19]</li>
<li><a href="http://cvil.eecs.yorku.ca/projects/public_html/sRGB_WB_correction/dataset.html" target="_blank" rel="noopener">Rendered WB dataset</a> - 100,000+ rendered sRGB images with different white balance (WB) settings (Afifi, Price, Cohen, Brown) [29/12/19]</li>
<li><a href="http://3drepresentation.stanford.edu/" target="_blank" rel="noopener">Stanford Street View Image, Pose, and 3D Cities Dataset</a> - a large scale dataset of street view images (25 million images and  118 matching image pairs) with their relative camera pose, 3D models of  cities, and 3D metadata of images. (Zamir, Wekel, Agrawal, Malik,  Savarese) [Before 28/12/19]</li>
<li><a href="https://testimages.org/" target="_blank" rel="noopener">TESTIMAGES</a> - Huge and free  collection of sample images designed for analysis and quality assessment of different kinds of displays (i.e. monitors, televisions and digital  cinema projectors) and image processing techniques. (Nicola Asuni)  [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html" target="_blank" rel="noopener">Time-Lapse Hyperspectral Radiance Images of Natural Scenes</a> - Four time-lapse sequences of 7-9 calibrated hyperspectral radiance  images of natural scenes taken over the day. (Foster, D.H., Amano, K.,  &amp; Nascimento, S.M.C.) [Before 28/12/19]</li>
<li><a href="http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html" target="_blank" rel="noopener">Time-lapse hyperspectral radiance images</a> - Four time-lapse sequences of 7-9 calibrated hyperspectral images of  natural scenes, spectra at 10-nm intervals(David H. Foster, Kinjiro  Amano, Sgio M. C. Nascimento) [Before 28/12/19]</li>
<li><a href="http://horatio.cs.nyu.edu/mit/tiny/data/index.html" target="_blank" rel="noopener">Tiny Images Dataset</a> 79 million 32x32 color images (Fergus, Torralba, Freeman) [Before 28/12/19]</li>
<li><a href="http://amandaduarte.com.br/turbid/" target="_blank" rel="noopener">TURBID Dataset</a> -  five different subsets of degraded images with its respective  ground-truth. Subsets Milk and DeepBlue have 20 images each and the  subset Chlorophyll has 42 images (Amanda Duarte) [Before 28/12/19]</li>
<li><a href="http://vision.cs.utexas.edu/projects/snapangle/" target="_blank" rel="noopener">UT Snap Angle 360˚ Dataset</a> - A list of 360˚ videos of four activities (disney, parade, ski,  concert) from youtube (Kristen Grauman, UT Austin) [Before 28/12/19]</li>
<li><a href="http://vision.cs.utexas.edu/projects/ego_snappoints/" target="_blank" rel="noopener">UT Snap Point Dataset</a> - Human judgement on snap point quality of a subset of frames from UT  Egocentric dataset and a newly collected mobile robot dataset (frames    are also included) (Bo Xiong, Kristen Grauman, UT Austin) [Before  28/12/19]</li>
<li><a href="https://ivi.fnwi.uva.nl/cv/intrinseg" target="_blank" rel="noopener">UVA Intrinsic Images and Semantic Segmentation Dataset</a> - RGB dataset with ground-truth albedo, shading, and semantic annotations (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="https://visualdialog.org/" target="_blank" rel="noopener">Visual Dialog</a> - 120k  human-human dialogs on COCO images, 10 rounds of QA per dialog (Das,  Kottur, Gupta, Singh, Yadav, Moura, Parikh, Batra) [Before 28/12/19]</li>
<li><a href="http://visualqa.org/" target="_blank" rel="noopener">Visual Question Answering</a> - 254K imags, 764K questions, ground truth (Agrawal, Lu, Antol, Mitchell, Zitnick, Batra, Parikh) [Before 28/12/19]</li>
<li><a href="https://www.microsoft.com/en-us/download/details.aspx?id=53670" target="_blank" rel="noopener">Visual Question Generation</a> - 15k images (including both object-centric and event-centric images),  75k natural questions asked about the images which can evoke further  conversation(Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret  Mitchell, Xiao dong He, Lucy Vanderwende) [Before 28/12/19]</li>
<li><a href="https://computing.ece.vt.edu/~abhshkdz/vqa-hat/" target="_blank" rel="noopener">VQA Human Attention</a> - 60k human attention maps for visual question answering i.e. where  humans choose to look to answer questions about images (Das, Agrawal,  Zitnick, Parikh, Batra) [Before 28/12/19]</li>
<li><a href="http://mklab.iti.gr/project/wild-web-tampered-image-dataset" target="_blank" rel="noopener">Wild Web tampered image dataset</a> - A large collection of tampered images from Web and social media  sources, including ground-truth annotation masks for tampering  localization (Markos Zampoglou, Symeon Papadopoulos) [Before 28/12/19]</li>
<li><a href="http://www.yfcc100m.org" target="_blank" rel="noopener">YFCC100M: The New Data in Multimedia Research</a> - This publicly available curated dataset of 100 million photos and  videos is free and legal for all. (Bart Thomee, Yahoo Labs and Flickr in San Francisco,etc.) [Before 28/12/19]</li>
</ol>
<h2 id="General-RGBD-and-Depth-Datasets"><a href="#General-RGBD-and-Depth-Datasets" class="headerlink" title="General RGBD and Depth Datasets"></a>General RGBD and Depth Datasets</h2><p>Note: there are 3D datasets elsewhere as well, <em>e.g.</em> in  <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#object" target="_blank" rel="noopener">Objects</a>, <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#scene" target="_blank" rel="noopener">Scenes</a>, and <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#action" target="_blank" rel="noopener">Actions</a>.</p>
<p>See also: <a href="http://www.michaelfirman.co.uk/RGBDdatasets/" target="_blank" rel="noopener">List of RGBD datasets</a>.</p>
<ol>
<li><a href="https://vcl3d.github.io/3D60/" target="_blank" rel="noopener">3D60: 3D Vision Indoor Spherical Panoramas</a> - A multimodal dataset of 360 spherical panoramas containing paired  color images, depth and normal maps, as well as vertical and horizontal  stereo pairs (with their assorted depth and normal maps as well) that  can be used to train or evaluate a variety of 3D vision tasks. (Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Petros Daras) [Before  28/12/19]</li>
<li><a href="http://campar.in.tum.de/personal/slavcheva/3d-printed-dataset/index.html" target="_blank" rel="noopener">3D-Printed RGB-D Object Dataset</a> - 5 objects with groundtruth CAD models and camera trajectories,  recorded with various quality RGB-D sensors. (Siemens &amp; TUM) [Before 28/12/19]</li>
<li><a href="http://www.rovit.ua.es/dataset/3dcomet/" target="_blank" rel="noopener">3DCOMET</a> -  3DCOMET is a dataset for testing 3D data compression methods. (Miguel  Cazorla, Javier Navarrete,Vicente Morell, Miguel Cazorla, Diego Viejo,  Jose Garcia-Rodriguez, Sergio Orts.) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/open?id=14h8dGmx3-CTCpTIiF6yzyfVTMI3u71GB" target="_blank" rel="noopener">3D articulated body</a> - 3D reconstruction of an articulated body with rotation and  translation. Single camera, varying focal. Every scene may have an  articulated body moving. There are four kinds of data sets included. A  sample reconstruction result included which uses only four images of the scene. (Prof Jihun Park) [Before 28/12/19]</li>
<li><a href="http://lgdv.cs.fau.de/publications/publication/Pub.2016.tech.IMMD.IMMD9.volume_6/" target="_blank" rel="noopener">A Dataset for Non-Rigid Reconstruction from RGB-D Data</a> - Eight scenes for reconstructing non-rigid geometry from RGB-D data,  each containing several hundred frames along with our results. (Matthias Innmann, Michael Zollhoefer, Matthias Niessner, Christian Theobalt,  Marc Stamminger) [Before 28/12/19]</li>
<li><a href="http://redwood-data.org/3dscan/" target="_blank" rel="noopener">A Large Dataset of Object Scans</a> - 392 objects in 9 casses, hundreds of frames each (Choi, Zhou, Miller, Koltun) [Before 28/12/19]</li>
<li><a href="http://cvlab-dresden.de/iccv2015-articulation-challenge/" target="_blank" rel="noopener">Articulated Object Challenge</a> -  4 articulated objects consisting of rigids parts connected by 1D  revolute and prismatic joints, 7000+ RGBD images with annotations for 6D pose estimation(Frank Michel, Alexander Krull, Eric Brachmann, Michael. Y. Yang,Stefan Gumhold, Carsten Rother) [Before 28/12/19]</li>
<li><a href="http://rll.eecs.berkeley.edu/bigbird" target="_blank" rel="noopener">BigBIRD</a> - 100  objects with for each object, 600 3D point clouds and 600  high-resolution color images spanning all views (Singh, Sha, Narayan,  Achim, Abbeel) [Before 28/12/19]</li>
<li><a href="http://store.sae.org/caesar/" target="_blank" rel="noopener">CAESAR</a> Civilian American  and European Surface Anthropometry Resource Project - 4000 3D human body scans (SAE International) [Before 28/12/19]</li>
<li><a href="http://www.kyb.tuebingen.mpg.de/nc/employee/details/browatbn.html" target="_blank" rel="noopener">CIN 2D+3D object classification dataset</a> - segmented color and depth images of objects from 18 categories of  common household and office objects (Bjorn Browatzki et al) [Before  28/12/19]</li>
<li><a href="http://corbs.dfki.uni-kl.de/" target="_blank" rel="noopener">CoRBS</a> - an RGB-D SLAM   benchmark, providing the combination of real depth and color data  together with a ground truth trajectory of the camera and a ground truth 3D model of the scene (Oliver Wasenmuller) [Before 28/12/19]</li>
<li><a href="https://research.csiro.au/robotics/databases/synthetic-data-non-rigid-3d-reconstruction/" target="_blank" rel="noopener">CSIRO synthetic deforming people</a> - synthetic RGBD dataset for evaluating non-rigid 3D reconstruction: 2  subjects and 4 camera trajectories (Elanattil and Moghadam) [Before  28/12/19]</li>
<li><a href="http://clopema.felk.cvut.cz/garment_folding_photo_dataset.html" target="_blank" rel="noopener">CTU Garment Folding Photo Dataset</a> - Color and depth images from various stages of garment folding.  (Sushkov R., Melkumov I., Smutn y V. (Czech Technical University in  Prague)) [Before 28/12/19]</li>
<li><a href="http://clopema.felk.cvut.cz/garment_sorting_dataset.html" target="_blank" rel="noopener">CTU Garment Sorting Dataset</a> - Dataset of garment images, detailed stereo images, depth images and  weights. (Petrik V., Wagner L. (Czech Technical University in Prague))  [Before 28/12/19]</li>
<li><a href="http://www.iri.upc.edu/groups/perception/#clothingDataset" target="_blank" rel="noopener">Clothing part dataset</a> - The clothing part dataset consists  of image and depth scans,  acquired with a Kinect, of garments laying on  a table, with over a  thousand part annotations (collar, cuffs, hood, etc) using polygonal  masks. (Arnau Ramisa, Guillem Aleny, Francesc Moreno-Noguer and Carme  Torras) [Before 28/12/19]</li>
<li><a href="http://pr.cs.cornell.edu/sceneunderstanding/data/data.php" target="_blank" rel="noopener">Cornell-RGBD-Dataset</a> - Office Scenes (Hema Koppula) [Before 28/12/19]</li>
<li><a href="http://cvssp.org/projects/4d/dynamic_rgbd_modelling/" target="_blank" rel="noopener">CVSSP Dynamic RGBD Modelling 2015</a> - This dataset contains eight RGBD sequences of general dynamic scenes  captured using the Kinect V1/V2 as well as two synthetic sequences.  (Charles Malleson, CVSSP, University of Surrey) [Before 28/12/19]</li>
<li><a href="http://campar.in.tum.de/personal/slavcheva/deformable-dataset/index.html" target="_blank" rel="noopener">Deformable 3D Reconstruction Dataset</a> - two single-stream RGB-D sequences of dynamically moving mechanical  toys together with ground-truth 3D models in the canonical rest pose.  (Siemens, TUM) [Before 28/12/19]</li>
<li><a href="http://data.4tu.nl/repository/uuid:daea472d-2ca5-4765-9f1b-bd3200de4b41" target="_blank" rel="noopener">Delft Windmill Interior and Exterior Laser Scanning Point Clouds</a> (Beril Sirmacek) [Before 28/12/19]</li>
<li><a href="https://github.com/PatrickChrist/diabetes60" target="_blank" rel="noopener">Diabetes60</a> - RGB-D images of 60 western dishes, home made. Data was recorded using a Microsoft Kinect V2. (Patrick Christ and Sebastian Schlecht) [Before  28/12/19]</li>
<li><a href="https://www.eth3d.net/" target="_blank" rel="noopener">ETH3D</a> - Benchmark for  multi-view stereo and 3D reconstruction, covering a variety of indoor  and outdoor scenes, with ground truth acquired by a high-precision laser scanner. (Thomas Sch??ps, Johannes L. Sch??nberger, Silvano Galliani,  Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger)  [Before 28/12/19]</li>
<li><a href="http://rgb-d.eurecom.fr/" target="_blank" rel="noopener">EURECOM Kinect Face Database</a> - 52 people, 2 sessions, 9 variations, 6 facial landmarks. (Jean-Luc DUGELAY et al) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">G4S meta rooms</a> - RGB-D data 150 sweeps with 18 images per sweep. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://dream.georgiatech-metz.fr/?q=node/76" target="_blank" rel="noopener">Georgiatech-Metz Symphony Lake Dataset</a> - 5 million RGBD outdoor images over 4 years from 121 surveys of a lakeshore. (Griffith and Pradalier) [Before 28/12/19]</li>
<li><a href="https://github.com/google/goldfinch" target="_blank" rel="noopener">Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges</a> - a largescale dataset for finegrained bird (11K species),butterfly  (14K species), aircraft (409 types), and dog (515 breeds) recognition.  (Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander  Toshev, Tom Duerig, James Philbin, Li Fei-Fei) [Before 28/12/19]</li>
<li><a href="https://www-users.cs.york.ac.uk/~nep/research/Headspace/" target="_blank" rel="noopener">Headspace dataset</a> - The Headspace dataset is a set of 3D images of the full human head,  consisting of 1519 subjects wearing tight fitting latex caps to reduce  the effect of hairstyles. (Christian Duncan, Rachel Armstrong, Alder Hey Craniofacial Unit, Liverpool, UK) [Before 28/12/19]</li>
<li><a href="https://github.com/facebookresearch/House3D" target="_blank" rel="noopener">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a href="http://cvssp.org/impart/" target="_blank" rel="noopener">IMPART multi-view/multi-modal 2D+3D film production dataset</a> - LIDAR, video,  3D models, spherical camera, RGBD, stereo, action,  facial expressions, etc. (Univ. of Surrey) [Before 28/12/19]</li>
<li><a href="http://www.mvtec.com/company/research/datasets/mvtec-itodd/" target="_blank" rel="noopener">Industrial 3D Object Detection Dataset (MVTec ITODD)</a> - depth and gray value data of 28 objects in 3500 labeled scenes for 3D object detection and pose estimation with a strong focus on industrial  settings and applications (MVTec Software GmbH, Munich) [Before  28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/kinect2-dataset/" target="_blank" rel="noopener">Kinect v2 Dataset</a> - Efficient Multi-Frequency Phase Unwrapping using Kernel Density Estimation (Felix etc.) [Before 28/12/19]</li>
<li><a href="http://limu.ait.kyushu-u.ac.jp/~agri/komatsuna/" target="_blank" rel="noopener">KOMATSUNA dataset</a> - The datasets is designed for instance segmentation, tracking and  reconstruction for leaves using both sequential multi-view RGB images  and depth images. (Hideaki Uchiyama, Kyushu University) [Before  28/12/19]</li>
<li><a href="http://make3d.cs.cornell.edu/data.html#make3d" target="_blank" rel="noopener">Make3D Laser+Image data</a> - about 1000 RGB outdoor images with aligned laser depth images (Saxena, Chung, Ng, Sun) [Before 28/12/19]</li>
<li><a href="http://www.cim.mcgill.ca/~apl/database/" target="_blank" rel="noopener">McGill-Reparti Artificial Perception Database</a> - RGBD data from four cameras and unfiltered Vicon skeletal data of two human subjects performing simulated assembly tasks on a car door  (Andrew Phan, Olivier St-Martin Cormier, Denis Ouellet, Frank P.  Ferrie). [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Meta rooms</a> - RGB-D data comprised of 28 aligned depth camera images collected by  having robot go to specific place and do 360 degrees of pan with various tilts. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://kovan.ceng.metu.edu.tr/MMStereoDataset/" target="_blank" rel="noopener">METU Multi-Modal Stereo Datasets ???Benchmark Datasets for Multi-Modal Stereo-Vision???</a> - The METU Multi-Modal Stereo Datasets includes benchmark datasets for  for Multi-Modal Stereo-Vision which is composed of two datasets: (1) The synthetically altered stereo image pairs from the Middlebury Stereo  Evaluation Dataset and (2) the visible-infrared image pairs captured  from a Kinect device. (Dr. Mustafa Yaman, Dr. Sinan Kalkan) [Before  28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">MHT RGB-D</a> - collected by a robot every 5 min over 16 days by the University of Lincoln. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://s.fhg.de/mini-rgbd" target="_blank" rel="noopener">Moving INfants In RGB-D (MINI-RGBD)</a> - A synthetic, realistic RGB-D data set for infant pose estimation  containing 12 sequences of moving infants with ground truth joint  positions. (N. Hesse, C. Bodensteiner, M. Arens, U. G. Hofmann, R.  Weinberger, A. S. Schroeder) [Before 28/12/19]</li>
<li><a href="http://www.dtic.ua.es/~agarcia/dataset" target="_blank" rel="noopener">Multi-sensor 3D Object Dataset for Object Recognition with Full Pose Estimation</a> - Multi-sensor 3D Object Dataset for Object Recognition and Pose  Estimation(Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu  Oprea,etc.) [Before 28/12/19]</li>
<li><a href="https://github.com/shahroudy/NTURGB-D" target="_blank" rel="noopener">NTU RGB+D Action Recognition Dataset</a> - NTU RGB+D is a large scale dataset for human action recognition(Amir Shahroudy) [Before 28/12/19]</li>
<li><a href="https://www.nuscenes.org/" target="_blank" rel="noopener">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" target="_blank" rel="noopener">NYU Depth Dataset V2</a> - Indoor Segmentation and Support Inference from RGBD Images [Before 28/12/19]</li>
<li><a href="http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/" target="_blank" rel="noopener">Oakland 3-D Point Cloud Dataset</a> (Nicolas Vandapel) [Before 28/12/19]</li>
<li><a href="http://www.pacman-project.eu/datasets/" target="_blank" rel="noopener">Pacman project</a> - Synthetic RGB-D images of 400 objects from 20 classes. Generated from  3D mesh models (Vladislav Kramarev, Umit Rusen Aktas, Jeremy L. Wyatt.)  [Before 28/12/19]</li>
<li><a href="http://adas.cvc.uab.es/phav/" target="_blank" rel="noopener"> Procedural Human Action Videos</a> - This dataset contains about 40,000 videos for human action  recognition that had been generated using a 3D game engine. The dataset  contains about 6 million frames which can be used to train and evaluate  models not only action recognition but also models for depth map  estimation, optical flow, instance segmentation, semantic segmentation,  3D and 2D pose estimation, and attribute learning. (Cesar Roberto de  Souza) [Before 28/12/19]</li>
<li><a href="https://arxiv.org/abs/1601.05511" target="_blank" rel="noopener">RGB-D-based Action Recognition Datasets</a> - Paper that includes the list and links of different rgb-d action  recognition datasets. (Jing Zhang, Wanqing Li, Philip O. Ogunbona,  Pichao Wang, Chang Tang) [Before 28/12/19]</li>
<li><a href="http://www.umiacs.umd.edu/~amyers/part-affordance-dataset/" target="_blank" rel="noopener">RGB-D Part Affordance Dataset</a> - RGB-D images and ground-truth affordance labels for 105 kitchen,  workshop and garden tools, and 3 cluttered scenes (Myers, Teo,  Fermuller, Aloimonos) [Before 28/12/19]</li>
<li><a href="http://www.scan-net.org" target="_blank" rel="noopener">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</a> - ScanNet is a dataset of richly-annotated RGB-D scans of real-world  environments containing 2.5M RGB-D images in more than 1500 scans,  annotated with 3D camera poses, surface reconstructions, and  instance-level semantic segmentations. (Angela Dai, Angel X. Chang,  Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Niessner)  [Before 28/12/19]</li>
<li><a href="http://www.scenenn.net" target="_blank" rel="noopener">SceneNN: A Scene Meshes Dataset with aNNotations</a> - RGB-D scene dataset with 100+ indoor scenes, labeled triangular mesh, voxel and pixel. (Hua, Pham, Nguyen, Tran, Yu, and Yeung) [Before  28/12/19]</li>
<li><a href="http://www.semantic3d.net/" target="_blank" rel="noopener">Semantic-8</a>: 3D point cloud classification with 8 classes (ETH Zurich) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Small office data sets</a> - Kinect depth images every 5 seconds beginning in April 2014 and on-going. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://lttm.dei.unipd.it/paper_data/fusion/" target="_blank" rel="noopener">Stereo and ToF dataset with ground truth</a> - The dataset contains 5 different scenes acquired with a  Time-of-flight sensor and a stereo setup. Ground truth information is  also provided. (Carlo Dal Mutto, Pietro Zanuttigh, Guido M. Cortelazzo)  [Before 28/12/19]</li>
<li><a href="http://www.synthia-dataset.net" target="_blank" rel="noopener">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a href="http://taskonomy.stanford.edu/" target="_blank" rel="noopener">Taskonomy</a> - Over 4.5  million real images each with ground truth for 25 semantic, 2D, and 3D  tasks. (Zamir, Sax, Shen, Guibas, Malik, Savarese) [Before 28/12/19]</li>
<li><a href="http://www.cs.toronto.edu/~harel/TAUAgent/home.html" target="_blank" rel="noopener">TAU Agent Dataset</a> - a high-resolution RGB-D dataset, created using Blender. Contains 530  high-resolution RGB images with corresponding pixel-wise ground truth  depth maps (Haim, Elmalem, Giryes, Bronstein, and Marom) [30/12/19]</li>
<li><a href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php" target="_blank" rel="noopener">THU-READ(Tsinghua University RGB-D Egocentric Action Dataset)</a> - THU-READ is a large-scale dataset for action recognition in RGBD  videos with pixel-lever hand annotation. (Yansong Tang, Yi Tian, Jiwen  Lu, Jianjiang Feng, Jie Zhou) [Before 28/12/19]</li>
<li><a href="https://www.research.ed.ac.uk/portal/en/datasets/trimbot2020-dataset-for-garden-navigation-and-bush-trimming(9f9de786-5e58-4bca-9279-f1d7ffddda41).html" target="_blank" rel="noopener">TrimBot2020 Dataset for Garden Navigation</a> – sensor RGBD data recorded from cameras and other sensors mounted on a robotic platform as well as additional external sensors capturing the  garden (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="https://vision.in.tum.de/data/datasets/rgbd-dataset" target="_blank" rel="noopener">TUM RGB-D Benchmark</a> - Dataset and benchmark for the evaluation of RGB-D visual odometry and SLAM algorithms (Jorgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard and Daniel Cremers) [Before 28/12/19]</li>
<li><a href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/" target="_blank" rel="noopener">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a href="http://cgvr.informatik.uni-bremen.de/research/asula/index.shtml" target="_blank" rel="noopener">Uni Bremen Open, Abdominal Surgery RGB Dataset</a> - Recording of a complete, open, abdominal surgery using a Kinect v2  that was mounted directly above the patient looking down at patient and  staff. (Joern Teuber, Gabriel Zachmann, University of Bremen) [Before  28/12/19]</li>
<li><a href="http://marathon.csee.usf.edu/range/DataBase.html" target="_blank" rel="noopener">USF Range Image Database</a> - 400+ laser range finder and structured light camera images, many with ground truth segmentations (Adam et al.) [Before 28/12/19]</li>
<li><a href="http://rgbd-dataset.cs.washington.edu/" target="_blank" rel="noopener">Washington RGB-D Object Dataset</a> - 300 common household objects and 14 scenes. (University of Washington and Intel Labs Seattle) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Witham Wharf</a> - For RGB-D of eight locations collect by robot every 10 min over ~10  days by the University of Lincoln. (John Folkesson et al.) [Before  28/12/19]</li>
<li><a href="https://www-users.cs.york.ac.uk/~nep/research/YEM/" target="_blank" rel="noopener">York 3D Ear Dataset</a> - The York 3D Ear Dataset  is a set of 500 3D ear images, synthesized  from detailed 2D landmarking, and available in both Matlab format (.mat) and PLY format (.ply). (Nick Pears, Hang Dai, Will Smith, University of York) [Before 28/12/19]</li>
</ol>
<h2 id="General-Videos"><a href="#General-Videos" class="headerlink" title="General Videos"></a>General Videos</h2><ol>
<li><a href="http://www2.compute.dtu.dk/~sohau/augmentations/" target="_blank" rel="noopener">AlignMNIST</a> - An artificially extended version of the MNIST handwritten dataset. (en Hauberg) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/audiovisualresearch" target="_blank" rel="noopener">Audio-Visual Event (AVE) dataset</a>- AVE dataset contains 4143 YouTube videos covering 28 event categories  and videos in AVE dataset are temporally labeled with audio-visual event boundaries. (Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and  Chenliang Xu) [Before 28/12/19]</li>
<li><a href="https://www.verlab.dcc.ufmg.br/semantic-hyperlapse/cvpr2018-dataset/" target="_blank" rel="noopener">Dataset of Multimodal Semantic Egocentric Video (DoMSEV)</a> - Labeled 80-hour Dataset of Multimodal Semantic Egocentric Videos  (DoMSEV) covering a wide range of activities, scenarios, recorders,  illumination and weather conditions. (UFMG, Michel Silva, Washington  Ramos, Jo??o Ferreira, Felipe Chamone, Mario Campos, Erickson R.  Nascimento) [Before 28/12/19]</li>
<li><a href="http://davischallenge.org/" target="_blank" rel="noopener">DAVIS: Video Object Segmentation dataset 2016</a> - A Benchmark Dataset and Evaluation Methodology for Video Object  Segmentation (F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.  Gross, and A. Sorkine-Hornung) [Before 28/12/19]</li>
<li><a href="http://davischallenge.org/" target="_blank" rel="noopener">DAVIS: Video Object Segmentation dataset 2017</a> -  The 2017 DAVIS Challenge on Video Object Segmentation (J.  Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool) [Before 28/12/19]</li>
<li><a href="https://iplab.dmi.unict.it/EGO-CH/" target="_blank" rel="noopener">EGO-CH</a> - a large  egocentric video dataset acquired by real visitors in two different  cultural sites. The dataset includes more than 27 hours of video  acquired by 70 different subjects. The overall dataset includes labels  for 26 environments and over 200 Points of Interest (POIs). (Giovanni  Maria Farinella) [31/12/19]</li>
<li><a href="https://github.com/facebookresearch/FAIR-Play" target="_blank" rel="noopener">FAIR-Play</a> - 1,871 video clips (~5 hrs) and their corresponding binaural audio clips recorded in a music room (Gao and Grauman) [29/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/" target="_blank" rel="noopener">GoPro-Gyro Dataset</a> - ego centric videos (Linkoping Computer Vision Laboratory) [Before 28/12/19]</li>
<li><a href="http://live.ece.utexas.edu/research/Quality/index.htm" target="_blank" rel="noopener">Image &amp; Video Quality Assessment at LIVE</a> - used to develop picture quality algorithms (the University of Texas at Austin) [Before 28/12/19]</li>
<li><a href="http://itee.uq.edu.au/~shenht/UQ_VIDEO/" target="_blank" rel="noopener">Large scale YouTube video dataset</a> - 156,823 videos (2,907,447 keyframes) crawled from YouTube videos (Yi Yang) [Before 28/12/19]</li>
<li><a href="https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset" target="_blank" rel="noopener">Movie Memorability Dataset</a>  - memorable movie clips and ground truth of detail memorability, 660  short movie excerpts extracted from 100 Hollywood-like movies (Cohendet, Yadati, Duong and Demarty) [Before 28/12/19]</li>
<li><a href="http://movieqa.cs.toronto.edu/" target="_blank" rel="noopener">MovieQA</a> - each machines to understand stories by answering questions about them. 15000 multiple choice QAs, 400+ movies. (M. Tapaswi, Y. Zhu, R. Stiefelhagen, A.  Torralba, R. Urtasun, and S. Fidler) [Before 28/12/19]</li>
<li><a href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html" target="_blank" rel="noopener">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a href="http://moments.csail.mit.edu/" target="_blank" rel="noopener">Moments in Time Dataset</a> - Moments in Time Dataset 1M 3-second videos annotated with action type,  the largest dataset of its kind for action recognition and understanding in video. (Monfort, Oliva, et al.) [Before 28/12/19]</li>
<li><a href="http://itee.uq.edu.au/~shenht/UQ_VIDEO/" target="_blank" rel="noopener">Near duplicate video retrieval dataset</a> - This database consists of 156,823 videos sequences (2,907,447  keyframes), which were crawled from YouTube during the period of July  2010 to September 2010. (Jingkuan Song, Yi Yang, Zi Huang, Heng Tao  Shen, Richang Hong) [Before 28/12/19]</li>
<li><a href="https://github.com/gyglim/personalized-highlights-dataset" target="_blank" rel="noopener">PHD2: Personalized Highlight Detection Dataset</a> - PHD2 is a dataset with personalized highlight information, which  allows to train highlight detection models that use information about  the user, when making predictions. (Ana Garcia del Molino, Michael  Gygli) [Before 28/12/19]</li>
<li><a href="http://cs.stanford.edu/people/karpathy/deepvideo/" target="_blank" rel="noopener">Sports-1M</a> - Dataset for sports video classification containing 487 classes and  1.2M videos. (Andrej Karpathy and George Toderici and Sanketh Shetty and Thomas Leung and Rahul Sukthankar and Li Fei-Fei.) [Before 28/12/19]</li>
<li><a href="https://www.nuscenes.org/" target="_blank" rel="noopener">nuTonomy scenes dataset (nuScenes)</a> - The nuScenes dataset is a large-scale autonomous driving dataset. It  features: Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS),  1000 scenes of 20s each, 1,440,000 camera images, 400,000 lidar sweeps,  two diverse cities: Boston and Singapore, left versus right hand  traffic, detailed map information, manual annotations for 25 object  classes, 1.1M 3D bounding boxes annotated at 2Hz, attributes such as  visibility, activity and pose. (Caesar et al) [Before 28/12/19]</li>
<li><a href="https://seungjunnah.github.io/Datasets/reds" target="_blank" rel="noopener">REDS (REalistic and Dynamic Scenes)</a> - high-quality realistic blurry video dataset with reference sharp  frames (improved version of GOPRO) (Nah, Baik, Hong, Moon, Son, Timofte  and Lee) [4/1/20]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Video Sequences</a>used for research on Euclidean upgrades based on minimal assumptions about the camera(Kenton McHenry) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/stacking-dataset/" target="_blank" rel="noopener">Video Stacking Dataset</a> - A Virtual Tripod for Hand-held Video Stacking on Smartphones (Erik Ringaby etc.) [Before 28/12/19]</li>
<li><a href="https://www.interdigital.com/data_sets/video-memorability-dataset" target="_blank" rel="noopener">VideoMem Dataset</a> - The VideoMem or Video Memorability Database is a collection of  sound-less video excerpts and their corresponding ground-truth  memorability files. The memorability scores are computed based on the  measurement of short-term and long-term memory performances when  recognizing small video excerpts a few minutes after viewing them for  the short-term case, and 24 to 72 hours later, for the long-term case.  It is accompanied with video features extracted from the video excerpts. It is intended to be used for understanding the memorability of videos  and for assessing the quality of methods for predicting the memorability of multimedia content. (Cohendet, Demarty, Duong and Engilberge)  [6/1/20]</li>
<li><a href="https://sites.google.com/site/videosearch100m/home" target="_blank" rel="noopener">YFCC100M videos</a> - A benchmark on the video subset of YFCC100M which includes the  videos, he video content features and the API to a sate-of-the-art video content engine. (Lu Jiang) [Before 28/12/19]</li>
<li><a href="http://www.yfcc100m.org" target="_blank" rel="noopener">YFCC100M: The New Data in Multimedia Research</a> - This publicly available curated dataset of 100 million photos and  videos is free and legal for all. (Bart Thomee, Yahoo Labs and Flickr in San Francisco,etc.) [Before 28/12/19]</li>
<li><a href="https://research.google.com/youtube-bb/" target="_blank" rel="noopener">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
<li><a href="https://research.google.com/youtube8m" target="_blank" rel="noopener">YouTube-8M</a> -  Dataset for video classification in the wild, containing pre-extracted  frame level features from 8M videos, and 4800 classes. (Sami  Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev,George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan) [Before  28/12/19]</li>
<li><a href="http://vision.eecs.yorku.ca/research/dynamic-scenes/" target="_blank" rel="noopener">YUP++ / Dynamic Scenes dataset</a> - 20 outdoor scene classes, each with 60 colour videos (each 5 seconds, 480 pixels wide, 24-30 fps) from 60 different scenes. Half of the  videos are with a static camera and half with a moving camera  (Feichtenhofer, Pinz, Wildes) [Before 28/12/19]</li>
</ol>
<h2 id="Hand-Hand-Grasp-Hand-Action-and-Gesture-Databases"><a href="#Hand-Hand-Grasp-Hand-Action-and-Gesture-Databases" class="headerlink" title="Hand, Hand Grasp, Hand Action and Gesture Databases"></a>Hand, Hand Grasp, Hand Action and Gesture Databases</h2><ol>
<li><a href="https://sites.google.com/view/11khands" target="_blank" rel="noopener">11k Hands</a> -  11,076 hand images (1600 x 1200 pixels) of 190 subjects, of varying ages between 18 - 75, with metadata (id, gender, age, skin color,  handedness, which hand, accessories, etc). (Mahmoud Afifi) [Before  28/12/19]</li>
<li><a href="https://www.twentybn.com/datasets/jester" target="_blank" rel="noopener">20bn-Jester</a> - densely-labeled video clips that show humans performing predefined hand gestures in front of a laptop camera or webcam (Twenty Billion Neurons  GmbH) [Before 28/12/19]</li>
<li><a href="http://www.iis.ee.ic.ac.uk/~dtang/hand.html" target="_blank" rel="noopener">3D Articulated Hand Pose Estimation with Single Depth Images</a> (Tang, Chang, Tejani, Kim, Yu) [Before 28/12/19]</li>
<li><a href="http://hpes.bii.a-star.edu.sg/" target="_blank" rel="noopener">A-STAR Annotated Hand-Depth Image Dataset and its Performance Evaluation</a> - depth data and data glove data, 29 images of 30 volunteers, Chinese  number counting and American Sign Language (Xu and Cheng) [Before  28/12/19]</li>
<li><a href="http://bosphorus.ee.boun.edu.tr/hand/" target="_blank" rel="noopener">Bosphorus Hand Geometry Database and Hand-Vein Database</a> (Bogazici University) [Before 28/12/19]</li>
<li><a href="http://robocoffee.org/datasets/" target="_blank" rel="noopener">A Dataset of Human Manipulation Actions</a> - RGB-D of 25 objects and 6 actions (Alessandro Pieropan) [Before 28/12/19]</li>
<li><a href="http://www.demcare.eu/results/datasets" target="_blank" rel="noopener">DemCare dataset</a> - DemCare dataset consists of a set of diverse data collection from  different sensors and is useful for human activity recognition from  wearable/depth and static IP camera, speech recognition for Alzheimmer’s disease detection and physiological data for gait analysis and  abnormality detection. (K. Avgerinakis, A.Karakostas, S.Vrochidis, I.  Kompatsiaris) [Before 28/12/19]</li>
<li><a href="http://research.ibm.com/dvsgesture/" target="_blank" rel="noopener">DVS128 Gesture Dataset</a> - Event-based dataset, containing sequences of 11 hand gestures,  performed by 29 subjects under several illumination conditions,captured  using a DVS128 sensor. Each sequence is annotated with the start and  stop times of each gesture. (Amir, Taba, Berg, Melano, McKinstry, Di  Nolfo, Nayak, Andreopoulos, Garreau, Mendoza, Kusnitz, Debole, Esser,  Delbruck, Flickner, and Modha)  [7/1/20]</li>
<li><a href="http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html" target="_blank" rel="noopener">EgoGesture Dataset</a> - First-person view gestures with 83 classes, 50 subjects, 6 scenes,  24161 RGB-D video samples (Zhang, Cao, Cheng, Lu) [Before 28/12/19]</li>
<li><a href="http://vision.soic.indiana.edu/projects/egohands/" target="_blank" rel="noopener">EgoHands</a> - A large dataset with over 15,000 pixel-level-segmented hands recorded from egocentric cameras of people interacting with each other. (Sven  Bambach) [Before 28/12/19]</li>
<li><a href="https://github.com/aurooj/Hand-Segmentation-in-the-Wild" target="_blank" rel="noopener">EgoYouTubeHands dataset</a> - An egocentric hand segmentation dataset consists of 1290 annotated  frames from YouTube videos recorded in unconstrained real-world  settings. The videos have variation in environment, number of  participants, and actions. This dataset is useful to study hand  segmentation problem in unconstrained settings. (Aisha Urooj, A. Borji)  [Before 28/12/19]</li>
<li><a href="http://cvrlcode.ics.forth.gr/handtracking/" target="_blank" rel="noopener">FORTH Hand tracking library</a> (FORTH) [Before 28/12/19]</li>
<li><a href="http://wildhog.ics.uci.edu:9090/full.html" target="_blank" rel="noopener">General HANDS: general hand detection and pose challenge</a> - 22 sequences with different gestures, activities and viewpoints (UC Irvine) [Before 28/12/19]</li>
<li><a href="http://www.gregrogez.net/research/egovision4health/gun-71/" target="_blank" rel="noopener">Grasp UNderstanding (GUN-71) dataset</a> -  12,000 first-person RGB-D images of object manipulation scenes  annotated using a taxonomy of 71 fine-grained grasps. (Rogez, Supancic  and Ramanan) [Before 28/12/19]</li>
<li><a href="http://www-vpu.eps.uam.es/DS/HGds/" target="_blank" rel="noopener">A Hand Gesture Detection Dataset</a> (Javier Molina et al) [Before 28/12/19]</li>
<li><a href="http://www.intelligence.tuc.gr/~petrakis/downloads/spatial-datasets-evaluation.zip" target="_blank" rel="noopener">Hand gesture and marine silhouettes</a> (Euripides G.M. Petrakis) [Before 28/12/19]</li>
<li><a href="http://www.cs.technion.ac.il/~twerd/HandNet/" target="_blank" rel="noopener">HandNet: annotated depth images of articulated hands</a> 214971 annotated depth images of hands captured by a RealSense RGBD  sensor of hand poses. Annotations:  per pixel classes, 6D fingertip  pose, heatmap. Train: 202198, Test: 10000, Validation: 2773. Recorded at GIP Lab, Technion.   [Before 28/12/19]</li>
<li><a href="https://github.com/aurooj/Hand-Segmentation-in-the-Wild" target="_blank" rel="noopener">HandOverFace dataset</a> -  A hand segmentation dataset consists of 300 annotated frames from  the web to study the hand-occluding-face problem. (Aisha Urooj, A.  Borji) [Before 28/12/19]</li>
<li><a href="http://www.idiap.ch/resource/gestures/" target="_blank" rel="noopener">IDIAP Hand pose/gesture datasets</a> (Sebastien Marcel) [Before 28/12/19]</li>
<li><a href="http://lttm.dei.unipd.it/downloads/gesture/" target="_blank" rel="noopener">Kinect and Leap motion gesture recognition dataset</a> -  The dataset contains 1400 different gestures acquired with both the  Leap Motion and the Kinect devices(Giulio Marin, Fabio Dominio, Pietro  Zanuttigh) [Before 28/12/19]</li>
<li><a href="http://lttm.dei.unipd.it/downloads/gesture/" target="_blank" rel="noopener">Kinect and Leap motion gesture recognition dataset</a> - The dataset contains several different static gestures acquired with  the Creative Senz3D camera. (A. Memo, L. Minto, P. Zanuttigh) [Before  28/12/19]</li>
<li><a href="http://cvrr.ucsd.edu/LISA/hand.html" target="_blank" rel="noopener">LISA CVRR-HANDS 3D</a> - 19 gestures performed by 8 subjects as car driver and passengers (Ohn-Bar and Trivedi) [Before 28/12/19]</li>
<li><a href="http://handtracker.mpi-inf.mpg.de/projects/handtracker_iccv2013/dexter1.htm" target="_blank" rel="noopener">MPI Dexter 1 Dataset for Evaluation of 3D Articulated Hand Motion Tracking</a> - Dexter 1: 7 sequences of challenging, slow and fast hand motions, RGB + depth (Sridhar, Oulasvirta, Theobalt) [Before 28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/um/people/yichenw/handtracking/" target="_blank" rel="noopener">MSR Realtime and Robust Hand Tracking from Depth</a> - (Qian, Sun, Wei, Tang, Sun) [Before 28/12/19]</li>
<li><a href="https://www.mutah.edu.jo/biometrix/hand-images-databases.html" target="_blank" rel="noopener">Mobile and Webcam Hand images database</a> - MOHI and WEHI - 200 people, 30 images each (Ahmad Hassanat) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/file/d/1f8tUHid1KmnwbgskGMXmobOxMfbxIgHM/view?usp=sharing" target="_blank" rel="noopener">NTU-Microsoft Kinect HandGesture Dataset</a> - This is a RGB-D dataset of hand gestures, 10 subjects x 10 hand  gestures x 10 variations. (Zhou Ren, Junsong Yuan, Jingjing Meng, and  Zhengyou Zhang) [Before 28/12/19]</li>
<li><a href="http://www.c3imaging.org/?page_id=3D772" target="_blank" rel="noopener">NUIG_Palm1</a> -  Database of palmprint images acquired in unconstrained conditions using  consumer devices for palmprint recognition experiments. (Adrian-Stefan  Ungureanu) [Before 28/12/19]</li>
<li><a href="http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm" target="_blank" rel="noopener">NYU Hand Pose Dataset</a> - 8252 test-set and 72757 training-set frames of captured RGBD data  with ground-truth hand-pose, 3 views (Tompson, Stein, Lecun, Perlin)  [Before 28/12/19]</li>
<li><a href="https://team.inria.fr/stars/praxis-dataset/" target="_blank" rel="noopener">PRAXIS gesture dataset</a> - RGB-D upper-body data from 29 gestures, 64 volunteers, several  repetitions, many volunteers have some cognitive impairment (Farhood  Negin, INRIA) [Before 28/12/19]</li>
<li><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html" target="_blank" rel="noopener">Rendered Handpose Dataset</a> - Synthetic dataset for 2D/ 3D Handpose Estimation with RGB, depth,  segmentation masks and 21 keypoints per hand (Christian Zimmermann and  Thomas Brox) [Before 28/12/19]</li>
<li><a href="https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-50.php" target="_blank" rel="noopener">RWTH-Boston-50</a> and <a href="https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-104.php" target="_blank" rel="noopener">RWTH-Boston-104</a> - American Sign Language hand gesture video datasets, containing 201  annotated sentences captured by 4 cameras (2 B/W stereo, 1 color, one  side view B/W) atg 30 fps and 312*242 pixels. The 50 dataset has 483  utterances of 50 words. (Dreuw, Keysers, Forster, Deselaers, Rybach,  Zahedi, Ney) [14/3/20]</li>
<li><a href="http://ee.sut.ac.ir/showcvmain.aspx?id=5" target="_blank" rel="noopener">Sahand Dynamic Hand Gesture Database</a> -  This database contains 11 Dynamic gestures designed to convey the  functions of mouse and touch screens to computers. (Behnam Maleki,  Hossein Ebrahimnezhad) [Before 28/12/19]</li>
<li><a href="http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm" target="_blank" rel="noopener">Sheffield gesture database</a> - 2160 RGBD hand gesture sequences, 6 subjects, 10 gestures, 3  postures, 3 backgrounds, 2 illuminations (Ling Shao) [Before 28/12/19]</li>
<li><a href="http://www.hci.iis.u-tokyo.ac.jp/~cai-mj/utgrasp_dataset.html" target="_blank" rel="noopener">UT Grasp Data Set</a> - 4 subjects grasping a variety of objectss with a variety of grasps (Cai, Kitani, Sato) [Before 28/12/19]</li>
<li><a href="http://www.eng.yale.edu/grablab/humangrasping/" target="_blank" rel="noopener">Yale human grasping data set</a> - 27 hours of video with tagged grasp, object, and task data from two  housekeepers and two machinists (Bullock, Feix, Dollar) [Before  28/12/19]</li>
</ol>
<h2 id="Image-Video-and-Shape-Database-Retrieval"><a href="#Image-Video-and-Shape-Database-Retrieval" class="headerlink" title="Image, Video and Shape Database Retrieval"></a>Image, Video and Shape Database Retrieval</h2><ol>
<li><a href="https://vision.in.tum.de/~laehner/Elastic2D3D/" target="_blank" rel="noopener">2D-to-3D Deformable Sketches</a> - A collection of deformable 2D contours in pointwise correspondence  with deformable 3D meshes of the same class; around 10 object classes  are provided, including humans and animals. (Lahner, Rodola) [Before  28/12/19]</li>
<li><a href="http://www.dais.unive.it/~cosmo/deformableclutter/" target="_blank" rel="noopener">3D Deformable Objects in Clutter</a> - A dataset for 3D deformable object-in-clutter, with point-wise ground truth correspondence across hundreds of scenes and spanning multiple  classes (humans, animals). (Cosmo, Rodola, Masci, Torsello, Bronstein)  [Before 28/12/19]</li>
<li><a href="http://lear.inrialpes.fr/~jegou/data.php" target="_blank" rel="noopener">ANN_SIFT1M</a> - 1M Flickr images encoded by 128D SIFT descriptors (Jegou et al) [Before 28/12/19]</li>
<li><a href="http://vision.lems.brown.edu/content/available-software-and-databases" target="_blank" rel="noopener">Brown Univ 25/99/216 Shape Databases</a> (Ben Kimia) [Before 28/12/19]</li>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a> - 60K 32x32 images from 10 classes, with a 512D GIST descriptor (Alex Krizhevsky) [Before 28/12/19]</li>
<li><a href="http://www.ir-facility.org/clef-ip" target="_blank" rel="noopener">CLEF-IP 2011 evaluation on patent images</a> [Before 28/12/19]</li>
<li><a href="http://www.cs.cmu.edu/~mengtial/proj/sketch/" target="_blank" rel="noopener">Contour Drawing Dataset</a> - a dataset of 5,000 paired images and contour drawings for the study  of visual understanding and sketch generation (Li, Lin, Měch, Yumer, and Ramanan) [9/1/20]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" target="_blank" rel="noopener">DeepFashion</a> - Large-scale Fashion Database(Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang) [Before 28/12/19]</li>
<li><a href="http://diameter.itn.liu.se/emodb/" target="_blank" rel="noopener">EMODB</a> - Thumbnails  of images in the picsearch image search engine together with the  picsearch emotion keywords (Reiner Lenz etc.) [Before 28/12/19]</li>
<li><a href="http://www.mfdemirci.etu.edu.tr/Etu10Silhouette.rar" target="_blank" rel="noopener">ETU10 Silhouette Dataset</a> - The dataset consists of 720 silhouettes of 10 objects, with 72 views  per object. (M. Akimaliev and M.F. Demirci) [Before 28/12/19]</li>
<li><a href="https://github.com/cvjena/eu-flood-dataset" target="_blank" rel="noopener">European Flood 2013</a> - 3,710 images of a flood event in central Europe, annotated with  relevance regarding 3 image retrieval tasks (multi-label) and important  image regions. (Friedrich Schiller University Jena, Deutsches  GeoForschungsZentrum Potsdam) [Before 28/12/19]</li>
<li><a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank" rel="noopener">Fashion-MNIST</a> - A MNIST-like fashion product database. (Han Xiao, Zalando Research) [Before 28/12/19]</li>
<li><a href="http://www.cise.ufl.edu/~anand/GatorBait_100.tgz" target="_blank" rel="noopener">Fish Shape Database</a> - It’s a Fish Shape Database with 100, 2D point set shapes. (Adrian M. Peter) [Before 28/12/19]</li>
<li><a href="http://shannon.cs.illinois.edu/DenotationGraph/" target="_blank" rel="noopener">Flickr 30K</a> - images, actions and captions (Peter Young et al) [Before 28/12/19]</li>
<li><a href="http://personal.ee.surrey.ac.uk/Personal/R.Hu/SBIR.html" target="_blank" rel="noopener">Flickr15k - Sketch based Image Retrieval (SBIR) Benchmark</a> - Dataset of 330 sketches and 15,024 photos comprising 33 object  categories,benchmark dataset commonly used to evaluate Sketch based  Image Retrieval (SBIR) algorithms. (Hu and Collomosse, CVIU 2013)  [Before 28/12/19]</li>
<li><a href="http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture" target="_blank" rel="noopener">Hands in action (HIC) IJCV dataset</a> - Data (images, models, motion) for tracking 1 hand or 2 hands with/o 1 object. Includes both *single-view RGB-D sequences (1 subject, &gt;18  annotated sequences, 4 objects, complete RGB image), and *multi-view RGB sequences (1 subject, HD, 8 views, 8 sequences - 1 annotated, 2  objects). (Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo  Aponte, Marc Pollefeys, Juergen Gall) [Before 28/12/19]</li>
<li><a href="http://www-i6.informatik.rwth-aachen.de/imageclef/08/photo/" target="_blank" rel="noopener">IAPR TC-12 Image Benchmark</a> (Michael Grubinger) [Before 28/12/19]</li>
<li><a href="http://imageclef.org/SIAPRdata" target="_blank" rel="noopener">IAPR-TC12 Segmented and annotated image benchmark (SAIAPR TC-12):</a> (Hugo Jair Escalante) [Before 28/12/19]</li>
<li><a href="http://www.imageclef.org/2010/PhotoAnnotation" target="_blank" rel="noopener">ImageCLEF 2010 Concept Detection and Annotation Task</a> (Stefanie Nowak) [Before 28/12/19]</li>
<li><a href="http://www.imageclef.org/2011/photo" target="_blank" rel="noopener">ImageCLEF 2011 Concept Detection and Annotation Task</a> - multi-label classification challenge in Flickr photos [Before 28/12/19]</li>
<li><a href="https://lear.inrialpes.fr/~jegou/data.php#copydays" target="_blank" rel="noopener">INRIA Copydays dataset</a> - for evaluation of copy detection: JPEG, cropping and “strong” copy attacks. (INRIA) [Before 28/12/19]</li>
<li><a href="https://lear.inrialpes.fr/~jegou/data.php#holidays" target="_blank" rel="noopener">INRIA Holidays dataset</a> - for evaluation of image search: 500 queries and 991 corresponding relevant images (Jegou, Douze and Schmid) [Before 28/12/19]</li>
<li><a href="https://www.researchgate.net/publication/333579741_MA14KD_ORIGINAL_Dataset_Description_Visual_Attraction_of_Movie_Trailers" target="_blank" rel="noopener">MA14KD (Movie Attraction 14K Dataset) Dataset</a> - 14K movie/TV trailers, 10 features each, links to a rating dataset  (Elahi, Moghaddam, Hosseini, Trattner, Tkalčič) [Before 28/12/19]</li>
<li><a href="http://kovan.ceng.metu.edu.tr/LogoDataset/" target="_blank" rel="noopener">METU Trademark dataset</a>The METU Dataset is composed of more than 900K real logos belonging to  companies worldwide. (Usta Bilgi Sistemleri A.S. and Grup Ofis Marka  Patent A.S) [Before 28/12/19]</li>
<li><a href="http://www.cim.mcgill.ca/~shape/benchMark/" target="_blank" rel="noopener">McGill 3D Shape Benchmark</a> (Siddiqi, Zhang, Macrini, Shokoufandeh, Bouix, Dickinson) [Before 28/12/19]</li>
<li><a href="http://mano.is.tue.mpg.de" target="_blank" rel="noopener">MPI MANO &amp; SMPL+H dataset</a> - Models, 4D scans and registrations for the statistical models MANO  (hand-only) and SMPL+H (body+hands). For MANO there are ~2k static 3D  scans of 31 subjects performing up to 51 poses. For SMPL+H we include 39 4D sequences of 11 subjects. (Javier Romero, Dimitrios Tzionas and  Michael J Black) [Before 28/12/19]</li>
<li><a href="http://grail.cs.washington.edu/projects/mview/" target="_blank" rel="noopener">Multiview Stereo Evaluation</a> - Each dataset is registered with a “ground-truth” 3D model acquired  via a laser scanning process(Steve Seitz  et al) [Before 28/12/19]</li>
<li><a href="http://www.itl.nist.gov/iad/vug/sharp/contest/2014/SBR/data.html" target="_blank" rel="noopener">NIST SHREC - 2014 NIST retrieval contest databases and links</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://www.itl.nist.gov/iad/vug/sharp/contest/2013/SBR/data.html" target="_blank" rel="noopener">NIST SHREC - 2013 NIST retrieval contest databases and links</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://www.itl.nist.gov/iad/vug/sharp/contest/2010/NonRigidShapes/" target="_blank" rel="noopener">NIST SHREC 2010 - Shape Retrieval Contest of Non-rigid 3D Models</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://www-nlpir.nist.gov/projects/trecvid/" target="_blank" rel="noopener">NIST TREC Video Retrieval Evaluation Database</a> (USA National Institute of Standards and Technology) [Before 28/12/19]</li>
<li><a href="http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm" target="_blank" rel="noopener">NUS-WIDE</a> - 269K Flickr images annotated with 81 concept tags, enclded as a 500D BoVW descriptor (Chau et al) [Before 28/12/19]</li>
<li><a href="http://shape.cs.princeton.edu/benchmark/index.cgi" target="_blank" rel="noopener">Princeton Shape Benchmark</a> (Princeton Shape Retrieval and Analysis Group) [Before 28/12/19]</li>
<li><a href="http://files.is.tue.mpg.de/dtzionas/GCPR_2013" target="_blank" rel="noopener">PairedFrames - evaluation of 3D pose tracking error</a> - Synthetic and Real dataset to test 3D pose tracking/refinement with  pose initialization close/far to/from minima. Establishes testing frame  pairs of increasing difficulty, to measure the pose estimation error  separately, without employing a full tracking pipeline. (Dimitrios  Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a href="http://itee.uq.edu.au/~shenht/UQ_IMH/index.htm" target="_blank" rel="noopener">Queensland cross media dataset</a> - millions of images and text documents for “cross-media” retrieval (Yi Yang) [Before 28/12/19]</li>
<li><a href="http://files.is.tue.mpg.de/dtzionas/Skeleton-Reconstruction" target="_blank" rel="noopener">Reconstructing Articulated Rigged Models from RGB-D Videos (RecArt-D)</a> - Dataset of objects deforming during manipulation. Includes 4 RGB-D  sequences (RGB image complete), result of deformable tracking for each  object, as well as 3D mesh and Ground-Truth 3D skeleton for each object. (Dimitrios Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a href="http://files.is.tue.mpg.de/dtzionas/In-Hand-Scanning" target="_blank" rel="noopener">Reconstruction from Hand-Object Interactions (R-HOI)</a> - Dataset of one hand interacting with an unknown object. Includes 4  RGB-D sequences, in total 4 objects, the RGB image is complete. Includes tracked 3D motion and Ground-Truth meshes for the objects. (Dimitrios  Tzionas, Juergen Gall) [Before 28/12/19]</li>
<li><a href="http://cmp.felk.cvut.cz/revisitop/" target="_blank" rel="noopener">Revisiting Oxford and Paris (RevisitOP)</a> - Improved and more challenging version (fixed errors, new annotation  and evaluation protocols, new query images) of the well known  landmark/building retrieval datasets accompanied with 1M distractor  images. (F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, O. Chum)  [Before 28/12/19]</li>
<li><a href="http://www.dais.unive.it/~shrec2016/index.php" target="_blank" rel="noopener">SHREC’16 Deformable Partial Shape Matching</a> - A collection of around 400 3D deformable shapes undergoing strong  partiality transformations, with point-to-point ground truth  correspondence included. (Cosmo, Rodola, Bronstein, Torsello) [Before  28/12/19]</li>
<li><a href="http://cs.txstate.edu/~yl12/SBR2016/data.html" target="_blank" rel="noopener">SHREC 2016 - 3D Sketch-Based 3D Shape Retrieval</a> - data to evaluate the performance of different 3D sketch-based 3D  model retrieval algorithms using a hand-drawn 3D sketch query dataset on a generic 3D model dataset  (Bo Li) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/shrec17/" target="_blank" rel="noopener">SHREC’17 Deformable Partial Shape Retrieval</a> - A collection of around 4000 deformable 3D shapes undergoing severe  partiality transformations, in the form of irregular missing parts and  range data; ground truth class information is provided. (Lahner, Rodola) [Before 28/12/19]</li>
<li><a href="http://watertight.ge.imati.cnr.it" target="_blank" rel="noopener">SHREC Watertight Models Track (of SHREC 2007)</a> - 400 watertight 3D models (Daniela Giorgi) [Before 28/12/19]</li>
<li><a href="http://partial.ge.imati.cnr.it" target="_blank" rel="noopener">SHREC Partial Models Track (of SHREC 2007)</a> - 400 watertight 3D DB models and 30 reduced watertight query models (Daniela Giorgi) [Before 28/12/19]</li>
<li><a href="http://www.cs.virginia.edu/~vicente/sbucaptions/" target="_blank" rel="noopener">SBU Captions Dataset</a> - image captions collected for 1 million images from Flickr (Ordonez, Kulkarni and Berg) [Before 28/12/19]</li>
<li><a href="https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html" target="_blank" rel="noopener">Sketch me That Shoe</a> - Sketch-based object retrieval in a fine-grained setting. Match  sketches to specific shoes and chairs. (Qian Yu, QMUL, T. Hospedales  Edinburgh/QMUL). [Before 28/12/19]</li>
<li><a href="http://tosca.cs.technion.ac.il/book/resources_data.html" target="_blank" rel="noopener">TOSCA 3D shape database</a> (Bronstein, Bronstein, Kimmel) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/view/totally-looks-like-dataset" target="_blank" rel="noopener">Totally Looks Like</a> - A benchmark for assessment of predicting human-based image similarity (Amir Rosenfeld, Markus D. Solbach, John Tsotsos) [Before 28/12/19]</li>
<li><a href="http://crcv.ucf.edu/data/Cross-View/" target="_blank" rel="noopener">UCF-CrossView Dataset: Cross-View Image Matching for Geo-localization in Urban Environments</a> - A new dataset of street view and bird’s eye view images for  cross-view image geo-localization. (Center for Research in Computer  Vision, University of Central Florida) [Before 28/12/19]</li>
<li><a href="https://research.google.com/youtube8m/" target="_blank" rel="noopener">YouTube-8M Dataset</a> - A Large and Diverse Labeled Video Dataset for Video Understanding Research. (Google Inc.) [Before 28/12/19]</li>
</ol>
<h2 id="Object-Databases"><a href="#Object-Databases" class="headerlink" title="Object Databases"></a>Object Databases</h2><ol>
<li><a href="http://www.csse.uwa.edu.au/~ajmal/databases.html" target="_blank" rel="noopener">2.5D/3D Datasets of various objects and scenes</a> (Ajmal Mian) [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">3D Object Recognition Stereo Dataset</a>This dataset consists of 9 objects and 80 test images.  (Akash Kushal and Jean Ponce) [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">3D Photography Dataset</a>a collection of ten multiview data sets captured in our lab(Yasutaka Furukawa and Jean Ponce) [Before 28/12/19]</li>
<li><a href="http://campar.in.tum.de/personal/slavcheva/3d-printed-dataset/index.html" target="_blank" rel="noopener">3D-Printed RGB-D Object Dataset</a> - 5 objects with groundtruth CAD models and camera trajectories,  recorded with various quality RGB-D sensors(Siemens &amp; TUM) [Before  28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">3DNet Dataset</a> - The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. (John Folkesson et al.)  [Before 28/12/19]</li>
<li><a href="https://deep-geometry.github.io/abc-dataset/" target="_blank" rel="noopener">ABC Dataset</a> - A million CAD models, including ground analytical descriptions  (spline patches), dense meshes, point clouds, normals. (Koch, Matveev,  Jiang, Williams, Artemov, Burnaev, Alexa, Zorin, Panozzo) [2/1/20]</li>
<li><a href="https://github.com/Yang7879/3D-RecGAN-extended" target="_blank" rel="noopener">Aligned 2.5D/3D datasets of various objects</a> - Synthesized and real-world datasets for object reconstruction from a  single depth view. (Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen) [Before 28/12/19]</li>
<li><a href="http://aloi.science.uva.nl" target="_blank" rel="noopener">Amsterdam Library of Object Images (ALOI): 100K views of 1K objects</a> (University of Amsterdam/Intelligent Sensory Information Systems) [Before 28/12/19]</li>
<li><a href="https://cvwc2019.github.io/challenge.html" target="_blank" rel="noopener">ATRW - Amur Tiger Re-identification in the Wild</a> - 8,000 Amur tiger video clips of 92 individuals (MakerCollider and WWF) [26/1/20]</li>
<li><a href="https://cvml.ist.ac.at/AwA2/" target="_blank" rel="noopener">Animals with Attributes 2</a> - 37322 (freely licensed) images of 50 animal classes with 85 per-class binary attributes. (Christoph H. Lampert, IST Austria) [Before  28/12/19]</li>
<li><a href="http://hemanthdv.org/OfficeHome-Dataset/" target="_blank" rel="noopener">ASU Office-Home Dataset</a> - Object recognition dataset of everyday objects for domain adaptation  (Venkateswara, Eusebio, Chakraborty, Panchanathan) [Before 28/12/19]</li>
<li><a href="http://kinectdata.com/" target="_blank" rel="noopener">B3DO: Berkeley 3-D Object Dataset</a> - household object detection (Janoch et al) [Before 28/12/19]</li>
<li><a href="http://www.cs.bris.ac.uk/~damen/BEOID/" target="_blank" rel="noopener">Bristol Egocentric Object Interactions Dataset</a> - egocentric object interactions with synchronised gaze (Dima Damen) [Before 28/12/19]</li>
<li><a href="https://github.com/jcpeterson/cifar-10h" target="_blank" rel="noopener">CIFAR-10H</a> - a  new dataset of soft labels reflecting human perceptual uncertainty for  the 10,000-image CIFAR-10 test set (Peterson, Battleday, Griffiths,  Russakovsky)  [14/1/20]</li>
<li><a href="http://vision.cs.uiuc.edu/CORE/" target="_blank" rel="noopener">CORE image dataset</a> -  to help learn more detailed models and for exploring cross-category  generalization in object recognition. (Ali Farhadi, Ian Endres, Derek  Hoiem, and David A. Forsyth) [Before 28/12/19]</li>
<li><a href="http://clopema.felk.cvut.cz/color_and_depth_dataset.html" target="_blank" rel="noopener">CTU Color and Depth Image Dataset of Spread Garments</a> - Images of spread garments with annotated corners. (Wagner, L., Krejov D., and Smutn V. (Czech Technical University in Prague)) [Before  28/12/19]</li>
<li><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html" target="_blank" rel="noopener">Caltech 101 (now 256) category object recognition database</a> (Li Fei-Fei, Marco Andreeto, Marc’Aurelio Ranzato) [Before 28/12/19]</li>
<li><a href="http://perceive.dieei.unict.it/index-dataset.php?name=Fish_Species" target="_blank" rel="noopener">Catania Fish Species Recognition</a> - 15 fish species, with about 20,000 sample training images and additional test images (Concetto Spampinato) [Before 28/12/19]</li>
<li><a href="https://github.com/nightrome/cocostuff" target="_blank" rel="noopener">COCO-Stuff dataset</a> - 164K images labeled with ‘things’ and ‘stuff’ (Caesar, Uijlings, Ferrari) [Before 28/12/19]</li>
<li><a href="http://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php" target="_blank" rel="noopener">Columbia COIL-100 3D object multiple views</a> (Columbia University) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/data/tvg/sjetley/" target="_blank" rel="noopener">Country Flags in the Wild</a> - 12,854 train images and 6,110 test images of the flags of 224  different countries manually cropped to loosely fit to the inlying  flags. (Jetley) [Before 28/12/19]</li>
<li><a href="https://gdo152.llnl.gov/cowc/" target="_blank" rel="noopener">COWC</a> - Cars Overhead  with Context. 32,716 unique annotated cars. 58,247 unique negative  examples. 15 cm per pixel resolution, from six distinct locations.  (Lawrence Livermore National Laboratory) [Before 28/12/19]</li>
<li><a href="https://domaingeneralization.github.io" target="_blank" rel="noopener">Deeper, Broader and Artier Domain Generalization</a> - Domain generalisation task dataset. (Da Li, QMUL) [Before 28/12/19]</li>
<li><a href="http://www.inf.fh-dortmund.de/personen/professoren/peters/pages/research/ImageDatabase/ImageDatabase.html" target="_blank" rel="noopener">Densely sampled object views: 2500 views of 2 objects, eg for view-based recognition and modeling</a> (Gabriele Peters, Universiteit Dortmund) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/UTENSILS/" target="_blank" rel="noopener">Edinburgh Kitchen Utensil Database</a> - 897 raw and binary images of 20 categories of kitchen utensil, a  resource for training future domestic assistance robots (D. Fullerton,  A. Goel, R. B. Fisher) [Before 28/12/19]</li>
<li><a href="http://www.ub.edu/cvub/edub-obj/" target="_blank" rel="noopener">EDUB-Obj</a> - Egocentric dataset for object localization and segmentation. (Marc Bolaños and Petia Radeva.) [Before 28/12/19]</li>
<li><a href="https://docs.google.com/file/d/0B10RxHxW3I92ZUtDU0RkMGlnNkU/edit?pref=2&pli=1" target="_blank" rel="noopener">Ellipse finding dataset</a> (Dilip K. Prasad et al) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/" target="_blank" rel="noopener">FGVC-Aircraft Benchmark</a> - 10,200 images of aircraft, with 100 images for each of 102 different  aircraft model variants (Maji, Kannala, Rahtu, Blaschko, Vedaldi)  [Before 28/12/19]</li>
<li><a href="https://etsin.avointiede.fi/dataset/urn-nbn-fi-csc-kata20170615175247247938" target="_blank" rel="noopener">FIN-Benthic</a> - This is a dataset for automatic fine-grained classification of  benthic macroinvertebrates. There are 15074 images from 64 categories.  The number of images per category varies from 577 to 7. (Jenni  Raitoharju, Ekaterina Riabchenko, Iftikhar Ahmad, Alexandros Iosifidis,  Moncef Gabbouj, Serkan Kiranyaz, Ville Tirronen, Johanna Arje) [Before  28/12/19]</li>
<li><a href="http://rubi.ucsd.edu/GERMS/" target="_blank" rel="noopener">GERMS</a> - The object set we  use for GERMS data collection consists of 136 stuffed toys of different  microorganisms. The toys are divided into 7 smaller categories, formed  by semantic division of the toy microbes. The motivation for dividing  the objects into smaller categories is to provide benchmarks with  different degrees of difficulty. (Malmir M, Sikka K, Forster D, Movellan JR, Cottrell G.) [Before 28/12/19]</li>
<li><a href="http://dmery.ing.puc.cl/index.php/material/gdxray/" target="_blank" rel="noopener">GDXray:X-ray images for X-ray testing and Computer Vision</a> - GDXray includes five groups of images: Castings,  Welds*,Baggages,  Nature and Settings. (Domingo Mery, Catholic University of Chile)  [Before 28/12/19]</li>
<li><a href="http://cs.gmu.edu/~robot/gmu-kitchens.html" target="_blank" rel="noopener">GMU Kitchens Dataset</a> - instance level annotation of 11 common household products from  BigBird dataset across 9 different kitchens (George Mason University)  [Before 28/12/19]</li>
<li><a href="https://www.labri.fr/projet/AIV/dossierSiteRoBioVis/GraspingInTheWildV2.htm" target="_blank" rel="noopener">Grasping In The Wild</a> - Egocentric video dataset of natural everyday life objects. 16 objects in 7 kitchens. (Benois-Pineau, Larrousse, de Rugy) [Before 28/12/19]</li>
<li><a href="http://www.emt.tugraz.at/~pinz/data/GRAZ_02/" target="_blank" rel="noopener">GRAZ-02 Database (Bikes, cars, people)</a> (A. Pinz) [Before 28/12/19]</li>
<li><a href="https://downloads.greyc.fr/Greyc3DColoredMeshDatabase/" target="_blank" rel="noopener">GREYC 3D</a> - The GREYC 3D Colored mesh database is a set of 15 real objects with  different colors, geometries and textures that were acquired using a 3D  color laser scanner. (Anass Nouri, Christophe Charrier, Olivier Lezoray) [Before 28/12/19]</li>
<li><a href="http://benchmark.ini.rub.de/?section=gtsdb&subsection=dataset" target="_blank" rel="noopener">GTSDB: German Traffic Sign Detection Benchmark</a> and <a href="http://benchmark.ini.rub.de/?section=gtsrb&subsection=news" target="_blank" rel="noopener">GTSRB: German Traffic Sign Recognition Benchmark</a> (Ruhr-Universitat Bochum) [Before 28/12/19]</li>
<li><a href="https://robotology.github.io/iCubWorld/" target="_blank" rel="noopener">ICubWorld</a> -  iCubWorld datasets are collections of images acquired by recording from  the cameras of the iCub humanoid robot while it observes daily objects.  (Giulia Pasquale, Carlo Ciliberto, Giorgio Metta, Lorenzo Natale,  Francesca Odone and Lorenzo Rosasco.) [Before 28/12/19]</li>
<li><a href="http://www.mvtec.com/company/research/datasets/mvtec-itodd/" target="_blank" rel="noopener">Industrial 3D Object Detection Dataset (MVTec ITODD)</a> - depth and gray value data of 28 objects in 3500 labeled scenes for 3D object detection and pose estimation with a strong focus on industrial  settings and applications (MVTec Software GmbH, Munich) [Before  28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html" target="_blank" rel="noopener">Instagram Food Dataset</a> - A database of 800,000 food images and associated metadata posted to  Instagram over 6 week period. Supports food type recognition and social  network analysis. (T. Hospedales. Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="http://3dinterpreter.csail.mit.edu/" target="_blank" rel="noopener">Keypoint-5 dataset</a> - a dataset of five kinds of furniture with their 2D keypoint labels  (Jiajun Wu, Tianfan Xue, Joseph Lim, Yuandong Tian, Josh Tenenbaum,  Antonio Torralba, Bill Freeman) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">KTH-3D-TOTAL</a> - RGB-D Data with objects on desktops annotated. 20 Desks, 3 times per  day, over 19 days. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://vision.gel.ulaval.ca/~jflalonde/projects/6dofObjectTracking/index.html" target="_blank" rel="noopener">Laval 6 DOF Object Tracking Dataset</a> - A Dataset of 297 RGB-D sequences with 11 objects for 6 DOF object  Tracking. (Mathieu Garon, Denis Laurendeau, Jean-Francois Lalonde)  [Before 28/12/19]</li>
<li><a href="http://cvrr.ucsd.edu/vivachallenge/index.php/traffic-light/traffic-light-detection/" target="_blank" rel="noopener">LISA Traffic Light Dataset</a> - 6 light classes in various lighting conditions (Jensen, Philipsen, Mogelmose, Moeslund, and Trivedi) [Before 28/12/19]</li>
<li><a href="http://cvrr.ucsd.edu/LISA/lisa-traffic-sign-dataset.html" target="_blank" rel="noopener">LISA Traffic Sign Dataset</a> -  video of 47 US sign types with 7855 annotations on 6610 frames (Mogelmose, Trivedi, and Moeslund) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/research/objrec/posedb/" target="_blank" rel="noopener">Linkoping 3D Object Pose Estimation Database</a> (Fredrik Viksten and Per-Erik Forssen) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/research/datasets/traffic-signs-dataset/" target="_blank" rel="noopener">Linkoping Traffic Signs Dataset</a> - 3488 traffic signs in 20K images (Larsson and Felsberg) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Longterm Labeled</a> - This dataset contains a subset of the observations from the longterm  dataset (longterm dataset above). (John Folkesson et al.) [Before  28/12/19]</li>
<li><a href="https://github.com/arubior/main-product-dataset" target="_blank" rel="noopener">Main Product Detection Dataset</a> - Contains textual metadata of fashion products and their images with  bounding boxes of the main product (the one referred by the text). (A.  Rubio, L. Yu, E. Simo-Serra and F. Moreno-Noguer) [Before 28/12/19]</li>
<li><a href="https://github.com/bircatmcri/MCIndoor20000" target="_blank" rel="noopener">MCIndoor20000</a> - 20,000 digital images from three different indoor object categories:  doors, stairs, and hospital signs. (Bashiri, LaRose, Peissig, and Tafti) [Before 28/12/19]</li>
<li><a href="http://www.labri.fr/projet/AIV/MexCulture142.php" target="_blank" rel="noopener">Mexculture142</a> - Mexican Cultural heritage objects and eye-tracker gaze fixations  (Montoya Obeso, Benois-Pineau, Garcia-Vazquez, Ramirez Acosta) [Before  28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/CarData.html" target="_blank" rel="noopener">MIT CBCL Car Data</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/streetscenes/" target="_blank" rel="noopener">MIT CBCL StreetScenes Challenge Framework:</a> (Stan Bileschi) [Before 28/12/19]</li>
<li><a href="http://mscoco.org/" target="_blank" rel="noopener">Microsoft COCO</a> - Common Objects in Context (Tsung-Yi Lin et al) [Before 28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/projects/objectclassrecognition/default.aspx" target="_blank" rel="noopener">Microsoft Object Class Recognition image databases</a> (Antonio Criminisi, Pushmeet Kohli, Tom Minka, Carsten Rother, Toby Sharp, Jamie Shotton, John Winn) [Before 28/12/19]</li>
<li><a href="http://research.microsoft.com/en-us/um/people/jiansun/SalientObject/salient_object.htm" target="_blank" rel="noopener">Microsoft salient object databases (labeled by bounding boxes)</a> (Liu, Sun Zheng, Tang, Shum) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Moving Labled</a> - This dataset extends the longterm datatset with more locations within the same office environment at KTH. (John Folkesson et al.) [Before  28/12/19]</li>
<li><a href="http://dl.allaboutbirds.org/nabirds" target="_blank" rel="noopener">NABirds Dataset</a> -  70,000 annotated photographs of the 400 species of birds commonly  observed in North America (Grant Van Horn) [Before 28/12/19]</li>
<li><a href="http://ml.nec-labs.com/download/data/videoembed/" target="_blank" rel="noopener">NEC Toy animal object recognition or categorization database</a> (Hossein Mobahi) [Before 28/12/19]</li>
<li><a href="http://www.cs.nyu.edu/~ylclab/data/norb-v1.0-small/" target="_blank" rel="noopener">NORB 50 toy image database</a> (NYU) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/jingjingmengsite/research/ntu-voi" target="_blank" rel="noopener">NTU-VOI: NTU Video Object Instance Dataset</a> - video clips with frame-level bounding box annotations of object  instances for evaluating object instance search and localization in  large scale videos. (Jingjing Meng, et. al.) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/objrec/posedb/" target="_blank" rel="noopener">Object Pose Estimation Database</a> - This database contains 16 objects, each sampled at 5 degrees angle  increments along two rotational axes (F. Viksten etc.) [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Object Recognition Database</a>This database features modeling shots of eight objects and 51 cluttered test shots containing multiple objects. (Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. ) [Before 28/12/19]</li>
<li><a href="https://github.com/brendenlake/omniglot/" target="_blank" rel="noopener">Omniglot</a> - 1623 different handwritten characters from 50 different alphabets (Lake, Salakhutdinov, Tenenbaum) [Before 28/12/19]</li>
<li><a href="https://storage.googleapis.com/openimages/web/index.html" target="_blank" rel="noopener">Open Images Dataset V4</a>15,440,132 boxes on 600 categories, 30,113,078 image-level labels on 19,794  categories. (Ferrari, Duerig, Gomes) [Before 28/12/19]</li>
<li><a href="http://users.cecs.anu.edu.au/~koniusz/openmic-dataset/index.php" target="_blank" rel="noopener">Open Museum Identification Challenge (Open MIC)</a>Open MIC contains photos of exhibits captured in 10 distinct exhibition  spaces (painting, sculptures, jewellery, etc.) of several museums and  the protocols for the domain adaptation and few-shot learning problems.  (P. Koniusz, Y. Tas, H. Zhang, M. Harandi, F. Porikli, R. Zhang) [Before 28/12/19]</li>
<li><a href="https://ikw.uos.de/~cv/projects/3Dcubes" target="_blank" rel="noopener">Osnabrück Synthetic Scalable Cube Dataset</a> - 830000 different cubes captured from 12 different viewpoints for ANN  training (Schöning, Behrens, Faion, Kheiri, Heidemann &amp; Krumnack)  [Before 28/12/19]</li>
<li><a href="http://modelnet.cs.princeton.edu/" target="_blank" rel="noopener">Princeton ModelNet</a> - 127,915 CAD Models, 662 Object Categories, 10 Categories with Annotated Orientation (Wu, Song, Khosla, Yu, Zhang, Tang, Xiao) [Before 28/12/19]</li>
<li><a href="http://www.pacman-project.eu/datasets/" target="_blank" rel="noopener">PacMan datasets</a> - RGB and 3D synthetic and real data for graspable cookware and crockery (Jeremy Wyatt) [Before 28/12/19]</li>
<li><a href="http://www.eecs.qmul.ac.uk/~dl307/project_iccv2017" target="_blank" rel="noopener">PACS (Photo Art Cartoon Sketch)</a> - An object category recognition dataset dataset for testing domain  generalisation: How well can a classifier trained on object images in  one domain recognise objects in another domain? (Da Li QMUL, T.  Hospedales. Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2007 Challange Image Database (motorbikes, cars, cows)</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2008 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2009 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2010 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2011 Challange Image Database</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL 2012 Challange Image Database</a> Category classification, detection, and segmentation, and still-image  action classification (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">PASCAL Image Database (motorbikes, cars, cows)</a> (PASCAL Consortium) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" target="_blank" rel="noopener">PASCAL Parts dataset</a> - PASCAL VOC with segmentation annotation for semantic parts of objects (Alan Yuille) [Before 28/12/19]</li>
<li><a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/" target="_blank" rel="noopener">PASCAL-Context dataset</a> - annotations for 400+ additional categories (Alan Yuille) [Before 28/12/19]</li>
<li><a href="http://cvgl.stanford.edu/projects/pascal3d.html" target="_blank" rel="noopener">PASCAL 3D/Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild</a> - 12 class, 3000+ images each with 3D annotations (Yu Xiang, Roozbeh Mottaghi, Silvio Savarese) [Before 28/12/19]</li>
<li><a href="http://phys101.csail.mit.edu/" target="_blank" rel="noopener">Physics 101 dataset</a> - a  video dataset of 101 objects in five different scenarios (Jiajun Wu,  Joseph Lim, Hongyi Zhang, Josh Tenenbaum, Bill Freeman) [Before  28/12/19]</li>
<li><a href="https://vision.eng.au.dk/plant-seedlings-dataset/" target="_blank" rel="noopener">Plant seedlings dataset</a> - High-resolution images of 12 weed species.  (Aarhus University) [Before 28/12/19]</li>
<li><a href="https://collections.durham.ac.uk/catalog?utf8=✓&q=breckon" target="_blank" rel="noopener">Raindrop Detection</a> - Improved Raindrop Detection using Combined Shape and Saliency  Descriptors with Scene Context Isolation - Evaluation Dataset (Breckon,  Toby P., Webster, Dereck D.) [Before 28/12/19]</li>
<li><a href="http://www.cs.virginia.edu/~vicente/referit/" target="_blank" rel="noopener">ReferIt Dataset (IAPRTC-12 and MS-COCO)</a> -  referring expressions for objects in images from the IAPRTC-12 and  MS-COCO datasets (Kazemzadeh, Matten, Ordonez, and Berg) [Before  28/12/19]</li>
<li><a href="http://sailvos.web.illinois.edu" target="_blank" rel="noopener">SAIL-VOS</a> - The  Semantic Amodal Instance Level Video Object Segmentation (SAIL-VOS)  dataset provides accurate ground truth annotations to develop methods  for reasoning about occluded parts of objects while enabling to take  temporal information into account (Hu, Chen, Hui, Huang, Schwing)  [29/12/19]</li>
<li><a href="http://www.lmars.whu.edu.cn/prof_web/shaozhenfeng/datasets/SeaShips(7000).zip" target="_blank" rel="noopener">SeaShips</a> - 31455 side images of boats near land, from 7 classes, extracted from  surveillance video (Shao, Wu, Wang, Du, Li) [Before 28/12/19]</li>
<li><a href="https://www.shapenet.org/" target="_blank" rel="noopener">ShapeNet</a> - 3D models of 55  common object categories with about 51K unique 3D models. Also 12K  models over 270 categories. (Princeton, Stanford and TTIC) [Before  28/12/19]</li>
<li><a href="http://www.bicv.org/datasets/short-100/" target="_blank" rel="noopener">SHORT-100 dataset</a> - 100 categories of products found on a typical shopping list. It aims  to benchmark the performance of algorithms for recognising hand-held  objects from either snapshots or videos acquired using hand-held or  wearable cameras. (Jose Rivera-Rubio, Saad Idrees, Anil A. Bharath)  [Before 28/12/19]</li>
<li><a href="http://sor3d.vcl.iti.gr/" target="_blank" rel="noopener">SOR3D</a> - The SOR3D dataset  consists of over 20k instances of human-object interactions, 14 object  types, and 13 object affordances. (pyridon Thermos) [Before 28/12/19]</li>
<li><a href="https://kelvins.esa.int/satellite-pose-estimation-challenge/data/" target="_blank" rel="noopener">Space Object Pose Estimation Challenge Dataset</a> - 12000  synthetic images for training, 2998  similar synthetic test  images, and 305 real images (Space Rendezvous Laboratory (SLAB))  [26/1/20]</li>
<li><a href="http://vision.stanford.edu/aditya86/ImageNetDogs/main.html" target="_blank" rel="noopener">Stanford Dogs Dataset</a> - The Stanford Dogs dataset contains images of 120 breeds of dogs from  around the world. This dataset has been built using images and  annotation from ImageNet for the task of fine-grained image  categorization. (Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng  Yao, Li Fei-Fei, Stanford University) [Before 28/12/19]</li>
<li><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">SVHN: Street View House Numbers Dataset</a> - like MNIST, but an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real  world problem (recognizing digits and numbers in natural scene images).  (Netzer, Wang, Coates, Bissacco, Wu, Ng) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/" target="_blank" rel="noopener">Swedish Leaf Dataset</a> - These images contains leaves from 15 treeclasses (Oskar J. O. S?derkvist) [Before 28/12/19]</li>
<li><a href="http://cmp.felk.cvut.cz/t-less" target="_blank" rel="noopener">T-LESS</a> - An RGB-D  dataset for 6D pose estimation of texture-less objects. (Tomas Hodan,  Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis Lourakis, Xenophon  Zabulis) [Before 28/12/19]</li>
<li><a href="http://www.sysu-hcp.net/taobao-commodity-dataset/" target="_blank" rel="noopener">Taobao Commodity Dataset</a> - TCD contains 800 commodity images (dresses, jeans, T-shirts, shoes  and hats) for image salient object detection from the shops on the  Taobao website. (Keze Wang, Keyang Shi, Liang Lin, Chenglong Li )  [Before 28/12/19]</li>
<li><a href="https://github.com/Tencent/tencent-ml-images" target="_blank" rel="noopener">TenCent open-source multi-label image database</a> - 17,609,752 training and 88,739 validation image URLs, which are  annotated with up to 11,166 categories (Wu, Chen, Fan, Zhang, Hou, Liu,  Zhang) [16/4/20]</li>
<li><a href="https://github.com/yaoyao-liu/tiered-imagenet-tools" target="_blank" rel="noopener">tieredImageNet dataset</a> - a larger subset of ILSVRC-12 with 608 classes (779,165 images)  grouped into 34 higher-level nodes in the ImageNet human-curated  hierarchy. (Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,  Larochelle, Zemel) [17/1/20]</li>
<li><a href="https://github.com/pauloabelha/ToolArtec" target="_blank" rel="noopener">ToolArtec point clouds</a> - 50 kitchen tool 3D scans (ply) from an Artec EVA scanner. See also <a href="https://github.com/pauloabelha/ToolKinect" target="_blank" rel="noopener">ToolKinect</a> - 13 scans using a Kinect 2 and <a href="https://github.com/pauloabelha/ToolWeb" target="_blank" rel="noopener">ToolWeb</a> - 116 point clouds of synthetic household tools with mass and  affordance groundtruth for 5 tasks. (Paulo Abelha) [Before 28/12/19]</li>
<li><a href="https://repo.acin.tuwien.ac.at/tmp/permanent/dataset_index.php" target="_blank" rel="noopener">TUW Object Instance Recognition Dataset</a> - Annotations of object instances and their 6DoF pose for cluttered  indoor scenes observed from various viewpoints and represented as Kinect RGB-D point clouds (Thomas, A. Aldoma, M. Zillich, M. Vincze) [Before  28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">TUW dat sets</a> - Several RGB-D Ground truth and annotated data sets from TUW. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="http://agamenon.tsc.uah.es/Investigacion/gram/traffic_signs.html" target="_blank" rel="noopener">UAH Traffic Signs Dataset</a> (Arroyo etc.) [Before 28/12/19]</li>
<li><a href="http://cogcomp.cs.illinois.edu/Data/Car/" target="_blank" rel="noopener">UIUC Car Image Database</a> (UIUC) [Before 28/12/19]</li>
<li><a href="http://www.eecs.umich.edu/vision/data/3Ddataset.zip" target="_blank" rel="noopener">UIUC Dataset of 3D object categories</a> (S. Savarese and L. Fei-Fei) [Before 28/12/19]</li>
<li><a href="https://www.kaggle.com/bistaumanga/usps-dataset" target="_blank" rel="noopener">USPS Handwritten Digits dataset</a> - 7291 train and 2007 test images. The images are 16*16 grayscale pixels (Hull) [Before 28/12/19]</li>
<li><a href="http://vcipl-okstate.org/pbvs/bench/Data/12/VAIS.zip" target="_blank" rel="noopener">VAIS</a> - VAIS contains simultaneously acquired unregistered thermal and  visible images of ships acquired from piers, and it was created to  faciliate autonomous ship development. (Mabel Zhang, Jean Choi, Michael  Wolf, Kostas Daniilidis, Christopher Kanan) [Before 28/12/19]</li>
<li><a href="http://www.dsi.unive.it/~rodola/data.html" target="_blank" rel="noopener">Venezia 3D object-in-clutter recognition and segmentation</a> (Emanuele Rodola) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/s1151656/resources.html" target="_blank" rel="noopener">Visual Attributes Dataset</a> visual attribute annotations for over 500 object classes (animate and  inanimate) which are all represented in ImageNet. Each object class is  annotated with visual attributes based on a taxonomy of 636 attributes  (e.g., has fur, made of metal, is round). [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Visual Hull Data Sets</a>a collection of visual hull datasets (Svetlana Lazebnik, Yasutaka Furukawa, and Jean Ponce) [Before 28/12/19]</li>
<li><a href="https://researchdata.sfu.ca/islandora/object/sfu:2724" target="_blank" rel="noopener">VOC-360</a> - Dataset for object detection and segmentation in fisheye images (Fu, Bajic, and Vaughan) [29/12/19]</li>
<li><a href="http://www.ycbbenchmarks.com/" target="_blank" rel="noopener">YCB Benchmarks – Object and Model Set</a> - 77 objects in 5 categories (food, kitchen, tool, shape, task) each  with 600 RGBD and high-res RGB images, calibration data, segmentation  masks, mesh models (Calli, Dollar, Singh, Walsman, Srinivasa, Abbeel)  [Before 28/12/19]</li>
<li><a href="https://research.google.com/youtube-bb/" target="_blank" rel="noopener">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
</ol>
<h2 id="People-static-and-dynamic-human-body-pose"><a href="#People-static-and-dynamic-human-body-pose" class="headerlink" title="People (static and dynamic), human body pose"></a>People (static and dynamic), human body pose</h2><ol>
<li><a href="https://drive.google.com/open?id=14h8dGmx3-CTCpTIiF6yzyfVTMI3u71GB" target="_blank" rel="noopener">3D articulated body</a> - 3D reconstruction of an articulated body with rotation and  translation. Single camera, varying focal. Every scene may have an  articulated body moving. There are four kinds of data sets included. A  sample reconstruction result included which uses only four images of the scene. (Prof Jihun Park) [Before 28/12/19]</li>
<li><a href="http://buff.is.tue.mpg.de" target="_blank" rel="noopener">BUFF dataset</a> - About 10K  scans of people in clothing and the estimated body shape of people  underneath. Scans contain texture so synthetic videos/images are easy to generate. (Zhang, Pujades, Black and Pons-Moll) [Before 28/12/19]</li>
<li><a href="https://github.com/VRU-intention/casr/blob/master/readme.txt" target="_blank" rel="noopener">CASR: Cyclist Arm Sign Recognition</a> - Small clips of ~10 seconds showing cyclists performing arm signs. The videos are acquired with a consumer-graded  camera. There are  219 arm  sign  actions annotated. (Zhijie Fang, Antonio M. Lopez) [13/1/20]</li>
<li><a href="http://dyna.is.tue.mpg.de" target="_blank" rel="noopener">Dynamic Dyna</a> - More than 40K 4D 60fps high resolution scans and models of people very accurately  registered. Scans contain texture so synthetic videos/images are easy to generate. (Pons-Moll, Romero, Mahmood and Black) [Before 28/12/19]</li>
<li><a href="http://dfaust.is.tue.mpg.de" target="_blank" rel="noopener">Dynamic Faust</a> - More than  40K 4D 60fps high resolution scans of people very accurately registered. Scans contain texture so synthetic videos/images are easy to generate.  (Bogo, Romero, Pons-Moll and Black) [Before 28/12/19]</li>
<li><a href="https://smpl-x.is.tue.mpg.de" target="_blank" rel="noopener">EHF dataset</a> - 100 curated frames (+ code) of one subject in minimal clothing performing various  expressive poses involving the body, hands and face. Each frame contains a full-body RGB image, detected 2D OpenPose features (body, hands,  face), a 3D scan of the subject, and a 3D SMPL-X mesh as pseudo  ground-truth (Pavlakos, Choutas, Ghorbani, Bolkart, Osman, Tzionas,  Black) [Before 28/12/19]</li>
<li><a href="http://files.is.tuebingen.mpg.de/classner/gp/" target="_blank" rel="noopener">Extended Chictopia dataset</a> - 14K image Chictopia dataset with additional processed annotations  (face) and SMPL body model fits to the images. (Lassner, Pons-Moll and  Gehler) [Before 28/12/19]</li>
<li><a href="http://bensapp.github.io/flic-dataset.html" target="_blank" rel="noopener">Frames Labeled In Cinema (FLIC)</a> - 20928 frames labeled with human pose (Sapp, Taskar) [Before 28/12/19]</li>
<li><a href="http://wangzheallen.github.io/GPA" target="_blank" rel="noopener">GPA: geometric pose affordance dataset</a> - Dataset of real 3D people interacting with real 3D scenes. 300k  static RGB frames of 13 subject in 8 scenes with ground-truth scene  meshes, and motion capture script focus on the interaction between  subject and scene geometry, human dynamics, and mimic of human action  with scene geometry around. (Wang, Chen, Rathore, Shin, Fowlkes)  [29/12/19]</li>
<li><a href="https://vision.in.tum.de/data/datasets/kids" target="_blank" rel="noopener">KIDS dataset</a> - A collection of 30 high-resolution 3D shapes undergoing  nearly-isometric and non-isometric deformations, with point-to-point  ground truth as well as ground truth for left-to-right bilateral  symmetry. (Rodola, Rota Bulo, Windheuser, Vestner, Cremers) [Before  28/12/19]</li>
<li><a href="http://www.sysu-hcp.net/kinect2-human-pose-dataset-k2hpd/" target="_blank" rel="noopener">Kinect2 Human Pose Dataset (K2HPD)</a> - Kinect2 Human Pose Dataset (K2HPD) includes about 100K depth images  with various human poses under challenging scenarios. (Keze Wang, Liang  Lin, Shengfu Zhai, Dengke Dong) [Before 28/12/19]</li>
<li><a href="http://www.comp.leeds.ac.uk/mat4saj/lsp.html" target="_blank" rel="noopener">Leeds Sports Pose Dataset</a> - 2000 pose annotated images of mostly sports people (Johnson, Everingham) [Before 28/12/19]</li>
<li><a href="http://hcp.sysu.edu.cn/lip/" target="_blank" rel="noopener">Look into Person Dataset</a> - 50,000 images with elaborated pixel-wise annotations with 19 semantic  human part labels and 2D hposes with 16 key points. (Gong, Liang, Zhang, Shen, Lin) [Before 28/12/19]</li>
<li><a href="http://www.manga109.org/en/index.html" target="_blank" rel="noopener">Manga109: manga (comic) dataset</a> - 109 volumes, more than 21,000 pages, 109 volumes, more than 21,000 pages (Kiyoharu Aizawa) [29/12/19]</li>
<li><a href="http://www.coe.neu.edu/Research/AClab/pose/colorManneNumbered.zip" target="_blank" rel="noopener">Mannequin in-bed pose datasets via RGB webcam</a> - This in-bed pose dataset is collected via regular webcam in a  simulated hospital room at Northeastern University. (Shuangjun Liu and  Sarah Ostadabbas, ACLab) [Before 28/12/19]</li>
<li><a href="http://www.coe.neu.edu/Research/AClab/pose/IRS_MANNE_rawN.zip" target="_blank" rel="noopener">Mannequin IRS in-bed dataset</a> - This in-bed pose dataset is collected via our infrared selective  (IRS) system in a simulated hospital room at Northeastern University.  (Shuangjun Liu and Sarah Ostadabbas, ACLab) [Before 28/12/19]</li>
<li><a href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/" target="_blank" rel="noopener">MoPoTS-3D</a> - Multi-person 3D body pose benchmark for monocular RGB based methods,  with 20 sequences in indoor and outdoor settings (MPI For Informatics)  [Before 28/12/19]</li>
<li><a href="http://gvv.mpi-inf.mpg.de/3dhp-dataset/" target="_blank" rel="noopener">MPI-INF-3DHP</a> - Single-person 3D body pose dataset and evaluation benchmark, with  extensive pose coverage across a broad set of activities, and extensive  scope of appearance augmentation. Multi-view RGB frames are available  for the training set, and monocular view frames for the test set. (MPI  For Informatics) [Before 28/12/19]</li>
<li><a href="http://mano.is.tue.mpg.de" target="_blank" rel="noopener">MPI MANO &amp; SMPL+H dataset</a> - Models, 4D scans and registrations for the statistical models MANO  (hand-only) and SMPL+H (body+hands). For MANO there are ~2k static 3D  scans of 31 subjects performing up to 51 poses. For SMPL+H we include 39 4D sequences of 11 subjects. (Javier Romero, Dimitrios Tzionas and  Michael J Black) [Before 28/12/19]</li>
<li><a href="http://human-pose.mpi-inf.mpg.de/" target="_blank" rel="noopener">MPII Human Pose Dataset</a> - 25K images containing over 40K people with annotated body joints,   410 human activities {Andriluka, Pishchulin, Gehler, Schiele) [Before  28/12/19]</li>
<li><a href="http://human-pose.mpi-inf.mpg.de/" target="_blank" rel="noopener">MPII Human Pose Dataset</a> - MPII Human Pose dataset is a de-facto standard benchmark for  evaluation of articulated human pose estimation. (Mykhaylo Andriluka,  Leonid Pishchulin, Peter Gehler, Bernt Schiele) [Before 28/12/19]</li>
<li><a href="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/" target="_blank" rel="noopener">MuCo-3DHP</a> - Large scale dataset of composited multi-person RGB images with 3D  pose annotations, generated from MPI-INF-3DHP dataset (MPI For  Informatics) [Before 28/12/19]</li>
<li><a href="http://camma.u-strasbg.fr/datasets" target="_blank" rel="noopener">MVOR: A Multi-view Multi-person RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation</a> - multi-view images captured by 3 RGB-D cameras during real clinical interventions (Padoy) [Before 28/12/19]</li>
<li><a href="https://people.eecs.berkeley.edu/~nzhang/piper.html" target="_blank" rel="noopener">People In Photo Albums</a> - Social media photo dataset with images from Flickr, and manual   annotations on person heads and their identities. (Ning Zhang and  Manohar Paluri and Yaniv Taigman and  Rob Fergus and Lubomir Bourdev)  [Before 28/12/19]</li>
<li><a href="https://graphics.tu-bs.de/people-snapshot" target="_blank" rel="noopener">People Snapshot Dataset</a> - Monocular video of 24 subjects rotating in front of a fixed camera.  Annotation in form of segmentation and 2D joint positions is provided.  (Alldieck, Magnor, Xu, Theobalt, Pons-Moll) [Before 28/12/19]</li>
<li><a href="https://goo.gl/DKuhlY" target="_blank" rel="noopener">Person Recognition in Personal Photo Collections</a> - we introduced three harder splits for evaluation and long-term  attribute annotations and per-photo timestamp metadata. (Oh, Seong Joon  and Benenson, Rodrigo and Fritz, Mario and Schiele, Bernt) [Before  28/12/19]</li>
<li><a href="http://www-prima.inrialpes.fr/Pointing04/data-face.html" target="_blank" rel="noopener">Pointing’04 ICPR Workshop Head Pose Image Database</a> [Before 28/12/19]</li>
<li><a href="http://www.cidis.espol.edu.ec/es/content/dataset-pose-estimation" target="_blank" rel="noopener">Pose estimation</a> - This dataset has a total of 155,530 images. These images were  obtained through the recording of members of CIDIS, in 4 sessions. In  total, 10 videos with a duration of 4 minutes each were obtained. The  participants were asked to bring different clothes, in order to give  variety to the images. After this, the frames of the videos were  separated at a rate of 5 frames per second. All these images were  captured from a top view perspective. The original images have a  resolution of 1280x720 pixels. (CIDIS) [Before 28/12/19]</li>
<li><a href="https://prox.is.tue.mpg.de" target="_blank" rel="noopener">PROX dataset</a> - Dataset  (+code) of real 3D people interacting with real 3D scenes. “Quantitative PROX”: 180 static RGB-D frames of 1 subject in 1 scene with  ground-truth SMPL-X meshes. “Qualitative PROX”: 100K dynamic RGB-D  sequences of 20 subjects in 12 scenes with pseudo ground-truth SMPL-X  meshes. (Hassan, Choutas, Tzionas, Black) [Before 28/12/19]</li>
<li><a href="https://vision.in.tum.de/~laehner/shrec2016/" target="_blank" rel="noopener">SHREC’16 Topological KIDS</a> - A collection of 40 high-resolution and low-resolution 3D shapes  undergoing nearly-isometric deformations in addition to strong  topological artifacts, self-contacts and mesh gluing, with  point-to-point ground truth. (Lahner, Rodola) [Before 28/12/19]</li>
<li><a href="https://vision.in.tum.de/~laehner/shrec2016/" target="_blank" rel="noopener">SURREAL</a> - 60,000 synthetic videos of people under large variations in shape,  texture, view-point and pose. (Varol, Romero, Martin, Mahmood, Black,  Laptev, Schmid) [Before 28/12/19]</li>
<li><a href="https://www.tnt.uni-hannover.de/project/TNT15/" target="_blank" rel="noopener">TNT 15 dataset</a> - Several sequences of video synchronised by 10 Inertial Sensors (IMU)  worn at the extremities. (von Marcard, Pons-Moll and Rosenhahn) [Before  28/12/19]</li>
<li><a href="http://mrl.isr.uc.pt/experimentaldata/public/uc-3d/" target="_blank" rel="noopener">UC-3D Motion Database</a> - Available data types encompass high resolution Motion Capture,  acquired with MVN Suit from Xsens and Microsoft Kinect RGB and depth  images. (Institute of Systems and Robotics, Coimbra, Portugal) [Before  28/12/19]</li>
<li><a href="http://up.is.tuebingen.mpg.de" target="_blank" rel="noopener">United People (UP) Dataset</a> - ˜8,000 images with keypoint and foreground segmentation annotations  as well as 3D body model fits. (Lassner, Romero, Kiefel, Bogo, Black,  Gehler) [Before 28/12/19]</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/data/pose/" target="_blank" rel="noopener">VGG Human Pose Estimation datasets</a> including the BBC Pose  (20 videos with an overlaid sign language  interpreter), Extended BBC Pose (72 additional training videos), Short  BBC Pose (5 one hour videos with sign language signers), and ChaLearn  Pose (23 hours of Kinect data of 27 persons performing 20 Italian  gestures). (Charles, Everingham, Pfister, Magee, Hogg, Simonyan,  Zisserman) [Before 28/12/19]</li>
<li><a href="http://fsukno.atspace.eu/Data.htm#VLRF" target="_blank" rel="noopener">VRLF: Visual Lip Reading Feasibility</a> - audio-visual corpus of 24 speakers recorded in Spanish (Fernandez-Lopez, Martinez and Sukno) [Before 28/12/19]</li>
</ol>
<h2 id="People-Detection-and-Tracking-Databases"><a href="#People-Detection-and-Tracking-Databases" class="headerlink" title="People Detection and Tracking Databases"></a>People Detection and Tracking Databases</h2><ol>
<li><a href="http://www.cvc.uab.es/DGaitDB/" target="_blank" rel="noopener">3D KINECT Gender Walking data base</a> (L. Igual, A. Lapedriza, R. Borr&agrave;s from UB, CVC and UOC, Spain) [Before 28/12/19]</li>
<li><a href="https://www.kaggle.com/aalborguniversity/trimodal-people-segmentation" target="_blank" rel="noopener">AAU VAP Trimodal People Segmentation Dataset</a> - People detection and segmentation dataset captured with depth, RGB,  and thermal sensors (Palmero, Clapés, Bahnsen, Møgelmose, Moeslund,  Escalera) [Before 28/12/19]</li>
<li><a href="https://asankagp.github.io/aerialgaitdataset/" target="_blank" rel="noopener">Aerial Gait Dataset</a> - people walking as viewed from an aerial (moving) platform (Perera, Law, Chahl) [Before 28/12/19]</li>
<li><a href="http://www.sites.univ-rennes2.fr/costel/corpetti/agoraset/Site/AGORASET.html" target="_blank" rel="noopener">AGORASET: a dataset for crowd video analysis</a> (Nicolas Courty et al) [Before 28/12/19]</li>
<li>[CASIA gait database](<a href="http://www.cbsr.ia.ac.cn/english/Gait" target="_blank" rel="noopener">http://www.cbsr.ia.ac.cn/english/Gait</a> Databases.asp) (Chinese Academy of Sciences) [Before 28/12/19]</li>
<li><a href="http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/" target="_blank" rel="noopener">CAVIAR project video sequences with tracking and behavior ground truth</a> (CAVIAR team/Edinburgh University - EC project IST-2001-37540) [Before 28/12/19]</li>
<li><a href="http://domedb.perception.cs.cmu.edu/" target="_blank" rel="noopener">CMU Panoptic Studio Dataset</a> - Multiple people social interaction dataset captured by 500+  synchronized video cameras, with 3D full body skeletons and calibration  data. (H. Joo, T. Simon, Y. Sheikh) [Before 28/12/19]</li>
<li><a href="http://www.ee.cuhk.edu.hk/~jshao/CUHKcrowd_files/cuhk_crowd_dataset.htm" target="_blank" rel="noopener">CUHK Crowd Dataset</a> - 474 video clips from 215 crowded scenes (Shao, Loy, and Wang) [Before 28/12/19]</li>
<li><a href="https://docs.google.com/forms/d/1MF0gAXWKeO1hpsuHlSpPBS8D5JR-r-QOPtdUoFQJONo/viewform?formkey=dF9pZ1BFZkNiMG1oZUdtTjZPalR0MGc6MA" target="_blank" rel="noopener">CUHK01 Dataset</a> : Person re-id dataset with 3, 884 images of 972 pedestrians (Rui Zhao et al) [Before 28/12/19]</li>
<li><a href="https://docs.google.com/forms/d/1lsUmbCvVllfz3zD54ws_I4ZFTkC71ysHBMGwtwKSukk/viewform?formkey=dHZtSGIwTnVDUEdWMFktQWU2bTZ0N3c6MA#gid=0" target="_blank" rel="noopener">CUHK02 Dataset</a> : Person re-id dataset with five camera view settings. (Rui Zhao et al) [Before 28/12/19]</li>
<li><a href="https://docs.google.com/forms/d/1RZsVgPXCFCEVzmx4HNgC1yxCy5f7o8eTIbvZMhS4sLU/viewform?formkey=dHRkMkFVSUFvbTJIRkRDLWRwZWpONnc6MA#gid=0" target="_blank" rel="noopener">CUHK03 Dataset</a> : Person re-id dataset with 13,164 images of 1,360 pedestrians (Rui Zhao et al) [Before 28/12/19]</li>
<li><a href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="noopener">Caltech Pedestrian Dataset</a> (P. Dollar, C. Wojek, B. Schiele and P. Perona) [Before 28/12/19]</li>
<li><a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Pedestrian_Segmentatio/daimler_pedestrian_segmentatio.html" target="_blank" rel="noopener">Daimler Pedestrian Detection Benchmark</a> 21790 images with 56492 pedestrians plus empty scenes. (D. M. Gavrila et al) [Before 28/12/19]</li>
<li><a href="http://www.i3a.uclm.es/louise/nais/fusiondatasets_EN.htm" target="_blank" rel="noopener">Datasets (Color &amp; Infrared) for Fusion</a> A series of images in color and infrared captured from a parallel  two-camera setup under different environmental conditions. (Juan  Serrano-Cuerda, Antonio Fernandez-Caballero, Maria T. Lopez) [Before  28/12/19]</li>
<li><a href="http://www.robesafe.com/personal/jnuevo/Datasets.html" target="_blank" rel="noopener">Driver Monitoring Video Dataset</a> (RobeSafe + Jesus Nuevo-Chiquero) [Before 28/12/19]</li>
<li><a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank" rel="noopener">DukeMTMC: Duke Multi-Target Multi-Camera tracking dataset</a> - 8 cameras, 85 min, 2m frames, 2000 people of video (Ergys Ristani,  Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo Tomasi) [Before  28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/FORUMTRACKING/" target="_blank" rel="noopener">Edinburgh overhead camera person tracking dataset</a> (Bob Fisher, Bashia Majecka, Gurkirt Singh, Rowland Sillito) [Before 28/12/19]</li>
<li><a href="http://gvvperfcapeva.mpi-inf.mpg.de" target="_blank" rel="noopener">GVVPerfcapEva</a> -  Repository of human shape and performance capture data, including full  body skeletal, hand tracking, body shape, face performance, interactions (Christian Theobalt) [Before 28/12/19]</li>
<li><a href="https://jurie.users.greyc.fr/datasets/hat.html" target="_blank" rel="noopener">HAT</a> Database of 27 human attributes (Gaurav Sharma, Frederic Jurie) [Before 28/12/19]</li>
<li><a href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html" target="_blank" rel="noopener">Immediacy Dataset</a> - This dataset is designed for estimation personal relationships. (Xiao Chu et al.) [Before 28/12/19]</li>
<li><a href="http://dressedhuman.gforge.inria.fr/" target="_blank" rel="noopener">Inria Dressed human bodies in motion benchmark</a> - Benchmark containing 3D motion sequences of different subjects,  motions, and clothing styles that allows to quantitatively measure the  accuracy of body shape estimates. (Jinlong Yang, Jean-Sbastien Franco,  Franck H=E9troy-Wheeler, and Stefanie Wuhrer) [Before 28/12/19]</li>
<li><a href="http://pascal.inrialpes.fr/data/human/" target="_blank" rel="noopener">INRIA Person Dataset</a> (Navneet Dalal) [Before 28/12/19]</li>
<li><a href="http://vision.soic.indiana.edu/firstthird-eccv2018/" target="_blank" rel="noopener">IU ShareView</a> - IU ShareView dataset consists of nine sets of synchronized (two  first-person) videos with a total of 1,227 pixel-level ground truth  segmentation maps of 2,654 annotated person instances. (Mingze Xu,  Chenyou Fan, Yuchen Wang, Michael S. Ryoo, David J. Crandall) [Before  28/12/19]</li>
<li><a href="http://cvrg.iyte.edu.tr/datasets.htm" target="_blank" rel="noopener">Izmir</a> -  omnidirectional and panoramic image dataset (with annotations) to be  used for human and car detection (Yalin Bastanlar) [Before 28/12/19]</li>
<li><a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/" target="_blank" rel="noopener">Joint Attention in Autonomous Driving (JAAD)</a> -  The dataset includes instances of pedestrians and cars  intended  primarily for the purpose of behavioural studies and  detection in the  context of autonomous driving. (Iuliia Kotseruba, Amir Rasouli and John  K. Tsotsos) [Before 28/12/19]</li>
<li><a href="http://jtl.lassonde.yorku.ca/2017/05/person-following-cnn/" target="_blank" rel="noopener">JTL Stereo Tacking Dataset for Person Following Robots</a> - 11 different indoor and outdoor places for the task of robots  following people under challenging situations (Chen, Sahdev, Tsotsos)  [Before 28/12/19]</li>
<li><a href="https://soonminhwang.github.io/rgbt-ped-detection/" target="_blank" rel="noopener">KAIST Multispectral Pedestrian Detection Benchmark</a> - 95k color-thermal pairs (640x480, 20Hz) images, with 103,128 dense  annotations and 1,182 unique pedestrians (Hwang, Park, Kim, Choi, Kweon) [Before 28/12/19]</li>
<li><a href="http://mahnob-db.eu/mimicry/" target="_blank" rel="noopener">MAHNOB: MHI-Mimicry database</a> - A 2 person, multiple camera and microphone database for studying  mimicry in human-human interaction scenarios. (Sun, Lichtenauer,  Valstar, Nijholt, and Pantic) [Before 28/12/19]</li>
<li><a href="http://cbcl.mit.edu/software-datasets/PedestrianData.html" target="_blank" rel="noopener">MIT CBCL Pedestrian Data</a> (Center for Biological and Computational Learning) [Before 28/12/19]</li>
<li><a href="http://dyna.is.tue.mpg.de/" target="_blank" rel="noopener">MPI DYNA</a> - A Model of Dynamic Human Shape in Motion (Max Planck Tubingen) [Before 28/12/19]</li>
<li><a href="http://faust.is.tue.mpg.de/" target="_blank" rel="noopener">MPI FAUST Dataset</a> A data  set containing 300 real, high-resolution human scans, with automatically computed ground-truth correspondences (Max Planck Tubingen) [Before  28/12/19]</li>
<li><a href="http://jhmdb.is.tue.mpg.de/" target="_blank" rel="noopener">MPI JHMDB dataset</a> -  Joint-annotated Human Motion Data Base - 21 actions, 928 clips, 33183  frames (Jhuang, Gall, Zuffi, Schmid and Black) [Before 28/12/19]</li>
<li><a href="http://mosh.is.tue.mpg.de/" target="_blank" rel="noopener">MPI MOSH</a> Motion and Shape  Capture from Markers. MOCAP data, 3D shape meshes, 3D high resolution  scans.  (Max Planck Tubingen) [Before 28/12/19]</li>
<li><a href="http://community.wvu.edu/~samotiian/datasets.html" target="_blank" rel="noopener">MVHAUS-PI</a> - a multi-view human interaction recognition dataset (Saeid et al.) [Before 28/12/19]</li>
<li><a href="http://www.liangzheng.org/Project/project_reid.html" target="_blank" rel="noopener">Market-1501 Dataset</a> - 32,668 annotated bounding boxes of 1,501 identities from up to 6 cameras (Liang Zheng et al) [Before 28/12/19]</li>
<li><a href="http://imagelab.ing.unimore.it/files/EGO-HPE.zip" target="_blank" rel="noopener">Modena and Reggio Emilia first person head motion videos</a> (Univ of Modena and Reggio Emilia) [Before 28/12/19]</li>
<li><a href="http://www.demcare.eu/results/datasets" target="_blank" rel="noopener">Multimodal Activities of Daily Living</a> - including video, audio, physiological, sleep, motion and plug sensors. (Alexia Briasouli) [Before 28/12/19]</li>
<li><a href="https://motchallenge.net/" target="_blank" rel="noopener">Multiple Object Tracking Benchmark</a> - A collection of datasets with ground truth, plus a performance league table (ETHZ, U. Adelaide, TU Darmstadt) [Before 28/12/19]</li>
<li><a href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html" target="_blank" rel="noopener">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a href="http://nyx.ethz.ch/" target="_blank" rel="noopener">NYU Multiple Object Tracking Benchmark</a> (Konrad Schindler et al) [Before 28/12/19]</li>
<li><a href="http://www.ics.forth.gr/cvrl/fbody/" target="_blank" rel="noopener">Occluded Articulated Human Body Dataset</a> - Body pose extraction and tracking under occlusions, 6 RGB-D sequences in total (3500 frames) with one, two and three users, marker-based  ground truth data. (Markos Sigalas, Maria Pateraki, Panos Trahanias)  [Before 28/12/19]</li>
<li><a href="http://www.oxuva.net" target="_blank" rel="noopener">OxUva</a> - A large-scale long-term  tracking dataset composed of 366 long videos of about 14 hours in total, with separate dev (public annotations) and test sets (hidden  annotations), featuring target object disappearance and continuous  attributes. (Jack Valmadre, Luca Bertinetto, Joao F. Henriques, Ran Tao, Andrea Vedaldi, Arnold Smeulders, Philip Torr, Efstratios Gavves)  [Before 28/12/19]</li>
<li><a href="http://www.am.sanken.osaka-u.ac.jp/BiometricDB/index.html" target="_blank" rel="noopener">OU-ISIR Gait Database</a> - six video-based gait data sets, two inertial sensor-based gait  datasets, and a gait-relevant biometric score data set. (Yasushi  Makihara) [Before 28/12/19]</li>
<li><a href="https://computing.ece.vt.edu/~santol/projects/zsl_via_visual_abstraction/parse/index.html" target="_blank" rel="noopener">PARSE Dataset Additional Data</a> - facial expression, gaze direction, and gender (Antol, Zitnick, Parikh) [Before 28/12/19]</li>
<li><a href="http://www.ics.uci.edu/~dramanan/papers/parse/index.html" target="_blank" rel="noopener">PARSE Dataset of Articulated Bodies</a> - 300 images of humans and horses (Ramanan) [Before 28/12/19]</li>
<li><a href="https://data.vision.ee.ethz.ch/daid/MOT/pathtrack_release_v1.0.zip" target="_blank" rel="noopener">PathTrack dataset: a large-scale MOT dataset</a> - PathTrack is a large scale multi-object tracking dataset of more than 15,000 person trajectories in 720 sequences. (Santiago Manen, Michael  Gygli, Dengxin Dai, Luc Van Gool) [Before 28/12/19]</li>
<li><a href="http://www-vpu.eps.uam.es/DS/PDbm/" target="_blank" rel="noopener">PDbm: People Detection benchmark repository</a> - realistic sequences, manually annotated people detection ground truth and a complete evaluation framework (Garc??a-Mart??n, Mart??nez,  Besc??s) [Before 28/12/19]</li>
<li><a href="http://www-vpu.eps.uam.es/DS/PDds/" target="_blank" rel="noopener">PDds: A Person Detection dataset</a> - several annotated surveillance sequences of different levels of  complexity (Garc??a-Mart??n, Mart??nez, Besc??s) [Before 28/12/19]</li>
<li><a href="http://www.cvg.reading.ac.uk/PETS2009/a.html" target="_blank" rel="noopener">PETS 2009 Crowd Challange dataset</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a href="http://www.cvg.reading.ac.uk/PETS2009/a.html" target="_blank" rel="noopener">PETS Winter 2009 workshop data</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a href="http://www.cvg.reading.ac.uk/PETS2015/a.html" target="_blank" rel="noopener">PETS: 2015 Performance Evaluation of Tracking and Surveillance</a> (Reading University &amp; James Ferryman) [Before 28/12/19]</li>
<li><a href="http://www.cvg.reading.ac.uk/" target="_blank" rel="noopener">PETS: 2015 Performance Evaluation of Tracking and Surveillance</a> (Reading University &amp; Luis Patino) [Before 28/12/19]</li>
<li><a href="http://www.cvg.reading.ac.uk/PETS2016/a.html" target="_blank" rel="noopener">PETS 2016 datasets</a> - multi-camera (including thermal cameras) video recordings of human  behavior around a stationary vehicle and around a boat (Thomas Cane)  [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/piropodatabase/" target="_blank" rel="noopener">PIROPO</a> - People in Indoor ROoms with Perspective and Omnidirectional cameras,  with more than 100,000 annotated frames (GTI-UPM, Spain) [Before  28/12/19]</li>
<li><a href="https://github.com/BathVisArtData/PeopleArt" target="_blank" rel="noopener">People-Art</a> - a databased containing people labelled in photos and artwork (Qi Wu and Hongping Cai) [Before 28/12/19]</li>
<li><a href="https://github.com/BathVisArtData/PhotoArt50" target="_blank" rel="noopener">Photo-Art-50</a> - a databased containing 50 object classes annoted in photos and artwork (Qi Wu and Hongping Cai) [Before 28/12/19]</li>
<li><a href="http://www.changedetection.net/" target="_blank" rel="noopener">Pixel-based change detection benchmark dataset</a> (Goyette et al) [Before 28/12/19]</li>
<li><a href="https://drive.google.com/open?id=0BzU4ETbYHM6faEdhZ0hMNmtqUTA" target="_blank" rel="noopener">Precarious Dataset</a> - unusual people detection dataset (Huang) [Before 28/12/19]</li>
<li><a href="https://github.com/dasabir/RAiD_Dataset" target="_blank" rel="noopener">RAiD</a> - Re-Identification Across Indoor-Outdoor Dataset: 43 people, 4 cameras, 6920 images (Abir Das et al) [Before 28/12/19]</li>
<li><a href="https://github.com/mengzhengrpi/mengzhengrpi.github.io/blob/master/index.md" target="_blank" rel="noopener">RPIfield</a> - Person re-identification dataset containing 4108 person images with  timestamps. (Meng Zheng, Srikrishna Karanam, Richard J. Radke) [Before  28/12/19]</li>
<li><a href="https://sites.google.com/site/dilipprasad/home/singapore-maritime-dataset" target="_blank" rel="noopener">Singapore Maritime Dataset</a> - Visible range videos and Infrared videos. (Dilip K. Prasad) [Before 28/12/19]</li>
<li><a href="https://web.northeastern.edu/ostadabbas/2019/06/27/multimodal-in-bed-pose-estimation/" target="_blank" rel="noopener">SLP (Simultaneously-collected multimodal Lying Pose)</a> -  large scale dataset on in-bed poses includes: 2 Data Collection  Settings: (a) Hospital setting: 7 participants, and (b) Home setting:  102 participants (29 females, age range: 20-40). 4 Imaging Modalities:  RGB (regular webcam), IR (FLIR LWIR camera), DEPTH (Kinect v2) and  Pressure Map (Tekscan Pressure Sensing Map). 3 Cover Conditions:  uncover, bed sheet, and blanket. Fully labeled poses with 14 joints.  (Ostadabbas and Liu) [2/1/20]</li>
<li><a href="http://www.synthia-dataset.net" target="_blank" rel="noopener">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a href="http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/" target="_blank" rel="noopener">Shinpuhkan 2014</a> - A Person Re-identification dataset containing 22,000 images of 24  people captured by 16 cameras. (Yasutomo Kawanishi et al.) [Before  28/12/19]</li>
<li><a href="http://cvgl.stanford.edu/projects/groupdiscovery/" target="_blank" rel="noopener">Stanford Structured Group Discovery dataset</a> - Discovering Groups of People in Images (W. Choi et al) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org" target="_blank" rel="noopener">TrackingNet</a> - Large-scale dataset for tracking in the wild: more than 30k annotated sequences for training, more than 500 sequestered sequences for  testing, evaluation server and leaderboard for fair ranking. (Matthias  Muller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi and Bernard  Ghanem) [Before 28/12/19]</li>
<li><a href="http://www.idi.ntnu.no/grupper/vis/tbnd/" target="_blank" rel="noopener">Transient Biometrics Nails Dataset V01</a> (Igor Barros Barbosa) [Before 28/12/19]</li>
<li><a href="http://www.dabi.temple.edu/~hbling/data/TColor-128/TColor-128.html" target="_blank" rel="noopener">Temple Color 128 - Color Tracking Benchmark</a> - Encoding Color Information for Visual Tracking (P. Liang, E. Blasch, H. Ling) [Before 28/12/19]</li>
<li><a href="https://www.mmk.ei.tum.de/verschiedenes/tum-gaid-database/" target="_blank" rel="noopener">TUM Gait from Audio, Image and Depth (GAID) database</a> - containing tracked RGB video, tracked depth video, and audio for 305  subjects (Babaee, Hofmann, Geiger,  Bachmann,  Schuller,  Rigoll)  [Before 28/12/19]</li>
<li><a href="http://vrai.dii.univpm.it/re-id-dataset" target="_blank" rel="noopener">TVPR (Top View Person Re-identification) dataset</a> - person re-identification using an RGB-D camera in a Top-View  configuration: indoor 23 sessions, 100 people, 8 days (Liciotti,  Paolanti, Frontoni, Mancini and Zingaretti) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html" target="_blank" rel="noopener">UCLA Aerial Event Dataset</a> - Human activities in aerial videos with annotations of people,  objects, social groups, activities and roles (Shu, Xie, Rothrock,  Todorovic, and Zhu) [Before 28/12/19]</li>
<li><a href="http://crcv.ucf.edu/projects/crowd.php" target="_blank" rel="noopener">Univ of Central Florida - Crowd Dataset</a> (Saad Ali) [Before 28/12/19]</li>
<li><a href="http://www.cs.ucf.edu/~sali/Projects/CrowdSegmentation/index.html" target="_blank" rel="noopener">Univ of Central Florida - Crowd Flow Segmentation datasets</a> (Saad Ali) [Before 28/12/19]</li>
<li><a href="https://vision.soe.ucsc.edu/node/178" target="_blank" rel="noopener">VIPeR: Viewpoint Invariant Pedestrian Recognition</a> - 632 pedestrian image pairs taken from arbitrary viewpoints under  varying illumination conditions. (Gray, Brennan, and Tao) [Before  28/12/19]</li>
<li><a href="http://www.votchallenge.net/" target="_blank" rel="noopener">Visual object tracking challenge datasets</a> - The VOT datasets is a collection of fully annotated visual object  tracking datasets used in the single-target short-term visual object  tracking challenges. (The VOT committee) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html" target="_blank" rel="noopener">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
<li><a href="http://www-vpu.eps.uam.es/DS/WUds/" target="_blank" rel="noopener">WUds: Wheelchair Users Dataset</a> - wheelchair users detection data, to extend people detection,  providing a more general solution to detect people in environments such  as independent and assisted living, hospitals, healthcare centers and  senior residences (Mart??n-Nieto, Garc??a-Mart??n, Mart??nez) [Before  28/12/19]</li>
<li><a href="https://github.com/facebookresearch/xR-EgoPose" target="_blank" rel="noopener">xR-EgoPose</a> - Photorealistic synthetic dataset for 3D human pose estimation from an ego-centric perspective (Tome, Peluse, Agapito and Badino) [4/1/20]</li>
<li><a href="https://research.google.com/youtube-bb/" target="_blank" rel="noopener">YouTube-BoundingBoxes</a> - 5.6 million accurate human-annotated BB from 23 object classes  tracked across frames, from 240,000 YouTube videos, with a strong focus  on the person class (1.3 million boxes) (Real, Shlens, Pan, Mazzocchi,  Vanhoucke, Khan, Kakarla et al) [Before 28/12/19]</li>
</ol>
<h2 id="Remote-Sensing"><a href="#Remote-Sensing" class="headerlink" title="Remote Sensing"></a>Remote Sensing</h2><ol>
<li><a href="https://www.airs-dataset.com" target="_blank" rel="noopener">Aerial Imagery for Roof Segmentation (AIRS)</a> - 457 km2 coverage of orthorectified aerial images with over 220,000  buildings for roof segmentation. (Lei Wang, Qi Chen) [Before 28/12/19]</li>
<li><a href="http://www.patreo.dcc.ufmg.br/downloads/brazilian-cerrado-savanna-dataset/" target="_blank" rel="noopener">Brazilian Cerrado-Savanna Scenes Dataset</a> - Composition of IR-R-G scenes taken by RapidEye sensor for vegetation  classification in Brazilian Cerrado-Savanna. (K. Nogueira, J. A. dos  Santos, T. Fornazari, T. S. Freire, L. P. Morellato, R. da S. Torres)  [Before 28/12/19]</li>
<li><a href="http://www.patreo.dcc.ufmg.br/downloads/brazilian-coffee-dataset/" target="_blank" rel="noopener">Brazilian Coffee Scenes Dataset</a> - Composition of IR-R-G scenes taken by SPOT sensor for identification  of coffee crops in Brazilian mountains. (O. A. B. Penatti, K. Nogueira,  J. A. dos Santos.) [Before 28/12/19]</li>
<li><a href="http://biz.nevsehir.edu.tr/ozgunok/en/duyurular" target="_blank" rel="noopener">Building Detection Benchmark</a> -14 images acquired from IKONOS (1 m) and QuickBird (60 cm)(Ali Ozgun Ok and Caglar Senaras) [Before 28/12/19]</li>
<li><a href="http://wwwp.fc.unesp.br/~papa/recogna/remote_sensing.html" target="_blank" rel="noopener">CBERS-2B, Landsat 5 TM, Geoeye, Ikonos-2 MS and ALOS-PALSAR</a> - land-cover classification using optical images(D. Osaku et al. ) [Before 28/12/19]</li>
<li><a href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/" target="_blank" rel="noopener">Data Fusion Contest 2015 (Zeebruges)</a> - This dataset provides a RGB aerial dataset (5cm) and a Lidar point  cloud (65pts/m2) over the harbor of the city of Zeebruges (Belgium). It  also provided a DSM derived from the point cloud and a semantic  segmentation ground truth of five of the seven 10000 x 10000 pixels  tiles. An evaluation server is used to evaluate the results on the two  other tiles. (Image analysis and Data Fusion Technical Committee, IEEE  Geoscience, Remote Sensing Society) [Before 28/12/19]</li>
<li><a href="http://www.grss-ieee.org/community/technical-committees/data-fusion/2017-ieee-grss-data-fusion-contest-2/" target="_blank" rel="noopener">Data Fusion Contest 2017</a> - This dataset provides satellite (Landsat, Sentinel 2) and vector GIS  layers (e.g. buildings and road footprint) for nine cities worldwide.  The task is to predict land use classes useful for climate models at a  100m prediction grid, given data of different resolution and types of  features. 5 cities come with labels, 4 others are kept hidden for  scoring on an evaluation server. (Image analysis and Data Fusion  Technical Committee, IEEE Geoscience, Remote Sensing Society) [Before  28/12/19]</li>
<li><a href="http://deepglobe.org/" target="_blank" rel="noopener">deepGlobe challenge</a> - This  datasets comprises three challenges, road extraction, buildings  detection and semantic segmentation of land cover. A series of satellite images from Digital Globe (RGB, 50 cm resolution) and labels over  several countries worldwide are provided. The results were presented at  the DeepGlobe workshop at CVPR 2018. (Facebook, Digital Globe) [Before  28/12/19]</li>
<li><a href="http://deepglobe.org" target="_blank" rel="noopener">DeepGlobe Satellite Image Understanding Challenge</a> - Datasets and evaluation platforms for three deep learning tasks on  satellite images: road extraction, building detection, and land type  classification. (Demir, Ilke and Koperski, Krzysztof and Lindenbaum,  David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh) [Before 28/12/19]</li>
<li><a href="https://captain-whu.github.io/DOTA" target="_blank" rel="noopener">DOTA</a> - 2806 large  aerial images with 188,282 over 15 categories (Xia, Bai, Ding, Zhu,  Belongie, Luo, Datcu, Pelillo, Zhang) [Before 28/12/19]</li>
<li><a href="https://v-sense.scss.tcd.ie/dublincity/" target="_blank" rel="noopener">DublinCity: Annotated LiDAR Point Cloud and its Applications</a> - Annotated (13 labels) aerial lidar scan of central Dublin (Zolanvari, Ruano, Rana, Cummins, da Silva, Rahbar, Smolic) [Before 28/12/19]</li>
<li><a href="http://www.ics.forth.gr/cvrl/msi" target="_blank" rel="noopener">FORTH Multispectral Imaging (MSI) datasets</a> - 5 datasets for Multispectral Imaging (MSI), annotated with ground truth data (Polykarpos Karamaoynas) [Before 28/12/19]</li>
<li><a href="http://wwwp.fc.unesp.br/~papa/recogna/remote_sensing.html" target="_blank" rel="noopener">Furnas and Tiete</a> - sediment yield classification( Pisani et al.) [Before 28/12/19]</li>
<li><a href="http://www.escience.cn/people/liuzikun/DataSet.html" target="_blank" rel="noopener">HSRC</a> - High Resolution Optical Satellite Image Dataset for Ship Recognition. 1061 ships images over 3 subclass levels (Liu, Yuan, Weng, Yang)  [Before 28/12/19]</li>
<li><a href="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html" target="_blank" rel="noopener">ISPRS 2D semantic labeling</a> - Height models and true ortho-images with a ground sampling distance  of 5cm have been prepared over the city of Potsdam/Germany (Franz  Rottensteiner, Gunho Sohn, Markus Gerke, Jan D. Wegner) [Before  28/12/19]</li>
<li><a href="http://www2.isprs.org/commissions/comm3/wg4/3d-semantic-labeling.html" target="_blank" rel="noopener">ISPRS 3D semantic labeling</a> - nine class airborne laser scanning data (Franz Rottensteiner, Gunho Sohn, Markus Gerke, Jan D. Wegner) [Before 28/12/19]</li>
<li><a href="https://project.inria.fr/aerialimagelabeling/" target="_blank" rel="noopener">Inria Aerial Image Labeling Dataset</a> -  9000 square kilometeres of color aerial imagery over U.S. and  Austrian cities. (Emmanuel Maggiori, Yuliya Tarabalka, Guillaume  Charpiat, Pierre Alliez.) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/tomalampert/data-sets" target="_blank" rel="noopener">Lampert’s Spectrogram Analysis</a> - Passive sonar spectrogram images derived from time-series  data,??these spectrograms are generated from recordings of acoustic  energy radiated from propeller and engine machinery in underwater sea  recordings. (Thomas Lampert) [Before 28/12/19]</li>
<li><a href="http://www.cvl.isy.liu.se/en/research/datasets/ltir/" target="_blank" rel="noopener">Linkoping Thermal InfraRed dataset</a> - The LTIR dataset is a thermal infrared dataset for evaluation of  Short-Term Single-Object (STSO) tracking (Linkoping University) [Before  28/12/19]</li>
<li><a href="http://www.iuii.ua.es/datasets/masati/index.html" target="_blank" rel="noopener">MASATI: MAritime SATellite Imagery dataset</a> - MASATI is a dataset composed of optical aerial imagery with 6212  samples which were obtained from Microsoft Bing Maps. They were labeled  and classified into 7 classes of maritime scenes: land, coast, sea,  coast-ship, sea-ship, sea with multi-ship, sea-ship in detail.  (University of Alicante) [Before 28/12/19]</li>
<li><a href="https://github.com/GatorSense/MUUFLGulfport" target="_blank" rel="noopener">MUUFL Gulfport Hyperspectral and LiDAR data set</a> - Co-registered aerial hyperspectral and lidar data over the University of Southern Mississippi Gulfpark campus containing several sub-pixel  targets. (Gader, Zare, Close, Aitken, Tuell) [Before 28/12/19]</li>
<li><a href="http://www.escience.cn/people/gongcheng/NWPU-RESISC45.html" target="_blank" rel="noopener">NWPU-RESISC45</a> - A large-scale benchmark dataset used for remote sensing image scene  classification containing 31500 images covered by 45 scene classes.  (Cheng, Han, Lu) [Before 28/12/19]</li>
<li><a href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html" target="_blank" rel="noopener">NWPU VHR-10 dataset</a> - 800 high resolution satellite images of 10 classes (airplane, ship,  storage tank, baseballdiamond, tennis court, basketball court, ground  track field, harbor, bridge, and vehicle) (Cheng, Han, Zhou, Guo)  [Before 28/12/19]</li>
<li><a href="https://github.com/rmkemker/RIT-18" target="_blank" rel="noopener">RIT-18</a> - a  high-resolution multispectral dataset for semantic segmentation. (Ronald Kemker, Carl Salvaggio, Christopher Kanan) [Before 28/12/19]</li>
<li><a href="https://ieee-dataport.org/documents/sar-ship-dataset-detection-discrimination-and-analysis" target="_blank" rel="noopener">SAR SHIP DATASET</a> - 43 Synthetic Aperture Radar images (Schwegmann, Kleynhans, Salmon, Mdakane, Meyer) [Before 28/12/19]</li>
<li><a href="https://dronedataset.icg.tugraz.at/" target="_blank" rel="noopener">Semantic Drone Dataset</a> - 20 houses from nadir (bird’s eye) view acquired at 5 to 30 meters  above ground. 400 public and 200 private high resolution images of  6000x4000px (24Mpx).  [Before 28/12/19]</li>
<li><a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html" target="_blank" rel="noopener">UC Merced Land Use Dataset</a> 21 class land use image dataset with 100 images per class, largely  urban, 256x256 resolution, 1 foot pixels (Yang and Newsam) [Before  28/12/19]</li>
<li><a href="http://crcv.ucf.edu/data/Cross-View/" target="_blank" rel="noopener">UCF-CrossView Dataset: Cross-View Image Matching for Geo-localization in Urban Environments</a> - A new dataset of street view and bird’s eye view images for  cross-view image geo-localization. (Center for Research in Computer  Vision, University of Central Florida) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/michelevolpiresearch/data/zurich-dataset" target="_blank" rel="noopener">Zurich Summer dataset</a> - t is intended for semantic segmentation of very high resolution  satellite images of urban scenes, with incomplete ground truth (Michele  Volpi and Vitto Ferrari.) [Before 28/12/19]</li>
<li><a href="http://rpg.ifi.uzh.ch/zurichmavdataset.html" target="_blank" rel="noopener">Zurich Urban Micro Aerial Vehicle Dataset</a> - time synchronized aerial high-resolution images of 2 km of Zurich,  with associated other data (Majdik, Till, Scaramuzza) [Before 28/12/19]</li>
</ol>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><ol>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/UTENSILS/" target="_blank" rel="noopener">Edinburgh Kitchen Utensil Database</a> - 897 raw and binary images of 20 categories of kitchen utensil, a  resource for training future domestic assistance robots (D. Fullerton,  A. Goel, R. B. Fisher) [Before 28/12/19]</li>
<li><a href="http://rpg.ifi.uzh.ch/davis_data.html" target="_blank" rel="noopener">Event-Camera Dataset</a> - This presents the world’s first collection of datasets with an  event-based camera for high-speed robotics (E. Mueggler, H. Rebecq, G.  Gallego, T. Delbruck, D. Scaramuzza) [Before 28/12/19]</li>
<li><a href="https://collections.durham.ac.uk/catalog?utf8=✓&q=breckon" target="_blank" rel="noopener">Improved 3D Sparse Maps for High-performance Structure from Motion with Low-cost Omnidirectional Robots - Evaluation Dataset</a> - Data set used in research paper doi:10.1109/ICIP.2015.7351744 (Breckon, Toby P., Cavestany, Pedro) [Before 28/12/19]</li>
<li><a href="http://www.raghavendersahdev.com/place-recognition.html" target="_blank" rel="noopener">Indoor Place Recognition Dataset for localization of Mobile Robots</a> - The dataset contains 17 different places built  from 2 different  robots (virtualMe and pioneer) (Raghavender Sahdev, John K. Tsotsos.)  [Before 28/12/19]</li>
<li><a href="http://jtl.lassonde.yorku.ca/2017/05/person-following-cnn/" target="_blank" rel="noopener">JTL Stereo Tacking Dataset for Person Following Robots</a> - 11 different indoor and outdoor places for the task of robots  following people under challenging situations (Chen, Sahdev, Tsotsos)  [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Meta rooms</a> - RGB-D data comprised of 28 aligned depth camera images collected by  having robot go to specific place and do 360 degrees of pan with various tilts. (John Folkesson et al.) [Before 28/12/19]</li>
<li><a href="https://hijeffery.github.io/PanoNavi/" target="_blank" rel="noopener">PanoNavi dataset</a> - A panoramic dataset for robot navigation, consisted of 5 videos lasting about 1 hour. (Lingyan Ran) [Before 28/12/19]</li>
<li><a href="http://kos.informatik.uni-osnabrueck.de/3Dscans/" target="_blank" rel="noopener">Robotic 3D Scan Repository</a> - 3D point clouds from robotic experiments of scenes (Osnabruck and Jacobs Universities) [Before 28/12/19]</li>
<li><a href="https://data.nal.usda.gov/dataset/data-solving-robot-world-hand-eyes-calibration-problem-iterative-methods" target="_blank" rel="noopener"> Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods</a> - These datasets were generated for calibrating robot-camera systems. (Amy Tabb) [Before 28/12/19]</li>
<li><a href="http://www.rovit.ua.es/dataset/vidrilo/" target="_blank" rel="noopener">ViDRILO</a> -  ViDRILO is a dataset containing 5 sequences of annotated RGB-D images  acquired with a mobile robot in two office buildings under challenging  lighting conditions. (Miguel Cazorla, J. Martinez-Gomez, M. Cazorla, I.  Garcia-Varea and V. Morell.) [Before 28/12/19]</li>
<li><a href="http://www.cas.kth.se/data/strands/data.html" target="_blank" rel="noopener">Witham Wharf</a> - For RGB-D of eight locations collect by robot every 10 min over ~10  days by the University of Lincoln. (John Folkesson et al.) [Before  28/12/19]</li>
</ol>
<h2 id="Scenes-or-Places-Scene-Segmentation-or-Classification"><a href="#Scenes-or-Places-Scene-Segmentation-or-Classification" class="headerlink" title="Scenes or Places, Scene Segmentation or Classification"></a>Scenes or Places, Scene Segmentation or Classification</h2><ol>
<li><a href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2017" target="_blank" rel="noopener">3DRMS Challenge Dataset 2017</a> - real garden stereo image pairs with camera poses and semantic  annotation captured by a small mobile robot (TrimBot2020 consortium)  [26/2/20]</li>
<li><a href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2018" target="_blank" rel="noopener">3DRMS Challenge Dataset 2018</a> - synthetic garden stereo image pairs with depths, camera poses and semantic annotation (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/" target="_blank" rel="noopener">Barcelona</a> - 15,150 images, urban views of Barcelona (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a href="http://diml.yonsei.ac.kr/~srkim/MIIC/" target="_blank" rel="noopener">Cross-modal Landmark Identification Benchmark</a> - Dandmark-identification benchmark taken under varying weather  conditions, which consists of 17 landmark images taken under several  weather conditions, e.g., sunny, cloudy, snowy, and sunset. (Yonsei  University) [Before 28/12/19]</li>
<li><a href="http://3dvis.ri.cmu.edu/data-sets/localization/" target="_blank" rel="noopener">CMU Visual Localization Data Set</a> - Dataset collected over the period of a year using the Navlab 11 equipped with IMU, GPS, INS, Lidars and cameras. (Hernan Badino, Daniel Huber and Takeo Kanade) [Before 28/12/19]</li>
<li><a href="http://www.csc.kth.se/~pronobis/research/ullah07cold/" target="_blank" rel="noopener">COLD (COsy Localization Database) - place localization</a> (Ullah, Pronobis, Caputo, Luo, and Jensfelt) [Before 28/12/19]</li>
<li><a href="http://davischallenge.org/" target="_blank" rel="noopener">DAVIS: Video Object Segmentation dataset 2016</a> - A Benchmark Dataset and Evaluation Methodology for Video Object  Segmentation (F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.  Gross, and A. Sorkine-Hornung) [Before 28/12/19]</li>
<li><a href="http://davischallenge.org/" target="_blank" rel="noopener">DAVIS: Video Object Segmentation dataset 2017</a> -  The 2017 DAVIS Challenge on Video Object Segmentation (J.  Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool) [Before 28/12/19]</li>
<li><a href="http://www.ub.edu/cvub/egocentric-dataset-of-the-university-of-barcelona-segmentation-edub-seg/" target="_blank" rel="noopener">EDUB-Seg</a>- Egocentric dataset for event segmentation. (Mariella Dimiccoli, Marc  Bolaños, Estefania Talavera, Maedeh Aghaei, Stavri G. Nikolov, and Petia Radeva.) [Before 28/12/19]</li>
<li><a href="https://github.com/cvjena/eu-flood-dataset" target="_blank" rel="noopener">European Flood 2013</a> - 3,710 images of a flood event in central Europe, annotated with  relevance regarding 3 image retrieval tasks (multi-label) and important  image regions. (Friedrich Schiller University Jena, Deutsches  GeoForschungsZentrum Potsdam) [Before 28/12/19]</li>
<li><a href="https://vision.eng.au.dk/fieldsafe/" target="_blank" rel="noopener">Fieldsafe</a> - A multi-modal dataset for obstacle detection in agriculture.  (Aarhus University) [Before 28/12/19]</li>
<li><a href="http://www-cvr.ai.uiuc.edu/ponce_grp/data/#texture" target="_blank" rel="noopener">Fifteen Scene Categories</a> - A dataset of fifteen natural scene categories. (Fei-Fei Li and Aude Oliva) [Before 28/12/19]</li>
<li><a href="http://figrim.mit.edu/" target="_blank" rel="noopener">FIGRIM (Fine Grained Image Memorability Dataset)</a> - A subset of images from the SUN database used for human memory  experiments, and provided along with memorability scores. (Bylinskii,  Isola, Bainbridge, Torralba, Oliva) [Before 28/12/19]</li>
<li><a href="http://dhoiem.cs.illinois.edu/projects/context/" target="_blank" rel="noopener">Geometric Context - scene interpretation images</a> (Derek Hoiem) [Before 28/12/19]</li>
<li><a href="https://github.com/cvdfoundation/google-landmark" target="_blank" rel="noopener">GLDv2: Google Landmarks Dataset v2</a> - 4,132,914 training images, 761,757 index images, and 117,577 test  images annotated with labels representing human-made and natural  landmarks (Weyand, Araujo, Cao, Sim) [16/4/20]</li>
<li><a href="https://wp.uni-koblenz.de/hyko/" target="_blank" rel="noopener">HyKo: A Spectral Dataset for Scene Understanding</a> - The HyKo dataset was captured with compact, low-cost, snapshot mosaic (SSM) imaging cameras, which are able to capture a whole spectral cube  in one shot recorded from a moving vehicle enabling hyperspectral scene  analysis for road scene understanding. (Active Vision Group, University  of Koblenz-Landau) [Before 28/12/19]</li>
<li><a href="https://github.com/visipedia/inat_comp" target="_blank" rel="noopener">iNaturalist Species Classification and Detection Dataset</a> - The iNaturalist 2017 species classification and detection dataset has been collected and annotated by citizen scientists and contains 859,000 images from over 5,000 different species of plants and animals.  (Caltech) [Before 28/12/19]</li>
<li><a href="http://www.raghavendersahdev.com/place-recognition.html" target="_blank" rel="noopener">Indoor Place Recognition Dataset for localization of Mobile Robots</a> - The dataset contains 17 different places built  from 2 different  robots (virtualMe and pioneer) (Raghavender Sahdev, John K. Tsotsos.)  [Before 28/12/19]</li>
<li><a href="http://web.mit.edu/torralba/www/indoor.html" target="_blank" rel="noopener">Indoor Scene Recognition</a> - 67 Indoor categories, 15620 images (Quattoni and Torralba) [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/" target="_blank" rel="noopener">Intrinsic Images in the Wild (IIW)</a> -  Intrinsic Images in the Wild, is a large-scale, public dataset for  evaluating intrinsic image decompositions of indoor scenes (Sean Bell,  Kavita Bala, Noah Snavely) [Before 28/12/19]</li>
<li><a href="https://github.com/HKBU-HPML/IRS" target="_blank" rel="noopener">IRS: Large Synthetic Indoor Robotics Stereo Dataset</a> - 103,316 samples covering a wide range of indoor scenes, such as home, office, store and restaurant (Wang, Zheng, Yan, Deng, Zhao, Chu)  [Before 28/12/19]</li>
<li><a href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/" target="_blank" rel="noopener">LM+SUN</a> - 45,676 images, mainly urban or human related scenes (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a href="https://europe.naverlabs.com/blog/making-maps-evergreen/" target="_blank" rel="noopener">Mallscape dataset</a> - a collection of 33K localized and time-stamped images captured in two large shopping malls during two different sessions temporally separated by several months, enabling to evaluate Point-of-Interests (POI) change detection methods in realistic conditions (Revaud, Sampaio De Rezende,  Heo, You, Jeong) [2/1/20]</li>
<li><a href="http://vcipl-okstate.org/pbvs/bench/Data/12/VAIS.zip" target="_blank" rel="noopener">Maritime Imagery in the Visible and Infrared Spectrums</a> - VAIS contains simultaneously acquired unregistered thermal and  visible images of ships acquired from piers (Zhang, Choi, Daniilidis,  Wolf,  &amp; Kanan) [Before 28/12/19]</li>
<li><a href="http://www.iuii.ua.es/datasets/masati/index.html" target="_blank" rel="noopener">MASATI: MAritime SATellite Imagery dataset</a> - MASATI is a dataset composed of optical aerial imagery with 6212  samples which were obtained from Microsoft Bing Maps. They were labeled  and classified into 7 classes of maritime scenes: land, coast, sea,  coast-ship, sea-ship, sea with multi-ship, sea-ship in detail.  (University of Alicante) [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/minc/" target="_blank" rel="noopener">Materials in Context (MINC)</a> - The Materials in Context Database (MINC) builds on OpenSurfaces, but  includes millions of point annotations of material labels. (Sean Bell,  Paul Upchurch, Noah Snavely, Kavita Bala) [Before 28/12/19]</li>
<li><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/" target="_blank" rel="noopener">MIT Intrinsic Images</a> - 20 objects (Roger Grosse, Micah K. Johnson, Edward H. Adelson, and William T. Freeman) [Before 28/12/19]</li>
<li><a href="http://people.csail.mit.edu/jstraub/_pages/nyu-mmf-dataset/index.html" target="_blank" rel="noopener">NYU V2 Mixture of Manhattan Frames Dataset</a> - We provide the Mixture of Manhattan Frames (MMF) segmentation and MF  rotations on the full NYU depth dataset V2 by Silberman et al. (Straub,  Julian and Rosman, Guy and Freifeld, Oren and Leonard, John J. and  Fisher III, John W.) [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/opensurfaces/" target="_blank" rel="noopener">OpenSurfaces</a> - OpenSurfaces consists of tens of thousands of examples of surfaces  segmented from consumer photographs of interiors, and annotated with  material parameters, texture information, and contextual information .  (Kavita Bala et al.) [Before 28/12/19]</li>
<li><a href="http://www.robots.ox.ac.uk/~aarnab/bmvc_2015.html" target="_blank" rel="noopener">Oxford Audiovisual Segmentation Dataset</a> - Oxford Audiovisual Segmentation Dataset with Oxford Audiovisual  Segmentation Dataset including audio recordings of objects being struck  (Arnab, Sapienza, Golodetz, Miksik and Torr) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/drivableregion/" target="_blank" rel="noopener">Thermal Road Dataset</a> - Our thermal-road dataset provides around 6000 thermal-infrared images captured in the road scene with manually annotated ground-truth. (3500: general road, 1500: complicated road, 1000: off-road). (Jae Shin Yoon)  [Before 28/12/19]</li>
<li><a href="http://places2.csail.mit.edu/" target="_blank" rel="noopener">Places 2 Scene Recognition database</a> -365 scene categories and 8 millions of images (Zhou, Khosla, Lapedriza, Torralba and Oliva) [Before 28/12/19]</li>
<li><a href="http://places.csail.mit.edu/" target="_blank" rel="noopener">Places Scene Recognition database</a> - 205 scene categories and 2.5 millions of images (Zhou, Lapedriza, Xiao, Torralba, and Oliva) [Before 28/12/19]</li>
<li><a href="http://ivrl.epfl.ch/supplementary_material/cvpr11/" target="_blank" rel="noopener">RGB-NIR Scene Dataset</a> - 477 images in 9 categories captured in RGB and Near-infrared (NIR) (Brown and Susstrunk) [Before 28/12/19]</li>
<li><a href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2017" target="_blank" rel="noopener">RMS2017 - Reconstruction Meets Semantics outdoor dataset</a> - 500 semantically annotated images with poses and point cloud from a real garden (Tylecek, Sattler) [Before 28/12/19]</li>
<li><a href="https://gitlab.inf.ed.ac.uk/3DRMS/Challenge2018" target="_blank" rel="noopener">RMS2018 - Reconstruction Meets Semantics virtual dataset</a> - 30k semantically annotated images with poses and point cloud from 6 virtual gardens (Le, Tylecek) [Before 28/12/19]</li>
<li><a href="https://robotvault.bitbucket.io/scenenet-rgbd.html" target="_blank" rel="noopener">SceneNet RGB-D</a> - 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth including RGB and depth (McCormac, Handa, Leutenegger, Davison)  [Before 28/12/19]</li>
<li><a href="https://syns.soton.ac.uk" target="_blank" rel="noopener">Southampton-York Natural Scenes Dataset</a> 90 scenes, 25 indoor and outdoor scene categories, with spherical  LiDAR, HDR intensity, stereo intensity panorama. (Adams, Elder, Graf,  Leyland, Lugtigheid, Muryy) [Before 28/12/19]</li>
<li><a href="http://groups.csail.mit.edu/vision/SUN/" target="_blank" rel="noopener">SUN 2012</a> - 16,873 fully annotated scene images for scene categorization (Xiao et al) [Before 28/12/19]</li>
<li><a href="http://groups.csail.mit.edu/vision/SUN/" target="_blank" rel="noopener">SUN 397</a> - 397 scene categories for scene classification (Xiao et al) [Before 28/12/19]</li>
<li><a href="http://rgbd.cs.princeton.edu/" target="_blank" rel="noopener">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</a> - 10,000 RGB-D images, 146,617 2D polygons and 58,657 3D bounding boxes (Song, Lichtenberg, and Xiao) [Before 28/12/19]</li>
<li><a href="http://www.synthia-dataset.net" target="_blank" rel="noopener">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a href="http://www.cs.unc.edu/~jtighe/Papers/ECCV10/" target="_blank" rel="noopener">Sift Flow (also known as LabelMe Outdoor, LMO)</a> - 2688 images, mainly outdoor natural and urban (Tighe and Lazebnik) [Before 28/12/19]</li>
<li><a href="http://dags.stanford.edu/projects/scenedataset.html" target="_blank" rel="noopener">Stanford Background Dataset</a>  - 715 images of outdoor scenes containing at least one foreground object (Gould et al) [Before 28/12/19]</li>
<li><a href="https://collections.durham.ac.uk/catalog?utf8=✓&q=breckon" target="_blank" rel="noopener">Surface detection</a> - Real-time traversable surface detection by colour space fusion and  temporal analysis - Evaluation Dataset (Breckon, Toby P., Katramados,  Ioannis) [Before 28/12/19]</li>
<li><a href="http://taskonomy.stanford.edu/" target="_blank" rel="noopener">Taskonomy</a> - Over 4.5  million real images each with ground truth for 25 semantic, 2D, and 3D  tasks. (Zamir, Sax, Shen, Guibas, Malik, Savarese) [Before 28/12/19]</li>
<li><a href="https://github.com/marialeyva/TB_Places" target="_blank" rel="noopener">TB-Places</a> – a  data set of garden images for bechmarking algorithms for image retrieval and visual place recognition (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="https://www.research.ed.ac.uk/portal/en/datasets/trimbot2020-dataset-for-garden-navigation-and-bush-trimming(9f9de786-5e58-4bca-9279-f1d7ffddda41).html" target="_blank" rel="noopener">TrimBot2020 Dataset for Garden Navigation</a> – sensor RGBD data recorded from cameras and other sensors mounted on a robotic platform as well as additional external sensors capturing the  garden (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="https://www.pf.bgu.tum.de/pub/testdaten.html" target="_blank" rel="noopener">TUM City Campus</a> - Urban point clouds taken by Mobile Laser Scanning (MLS) for  classification, object extraction and change detection (Stilla, Hebel,  Xu, Gehrung) [3/1/20]</li>
<li><a href="https://ivi.fnwi.uva.nl/cv/intrinseg" target="_blank" rel="noopener">UVA Intrinsic Images and Semantic Segmentation Dataset</a> – RGB dataset with ground-truth albedo, shading, and semantic annotations (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="http://www.rovit.ua.es/dataset/vidrilo/" target="_blank" rel="noopener">ViDRILO</a> -  ViDRILO is a dataset containing 5 sequences of annotated RGB-D images  acquired with a mobile robot in two office buildings under challenging  lighting conditions. (Miguel Cazorla, J. Martinez-Gomez, M. Cazorla, I.  Garcia-Varea and V. Morell.) [Before 28/12/19]</li>
<li><a href="https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/" target="_blank" rel="noopener">Virtual Gallery</a> - a synthetic dataset that targets multiple challenges such as varying  lighting conditions and different occlusion levels for various tasks  such as depth estimation, instance segmentation and visual localization  (Weinzaepfel, Csurka, Cabon, Humenberger) [7/1/20]</li>
<li><a href="https://github.com/huangkuns/wireframe" target="_blank" rel="noopener">Wireframe dataset</a> - A set of RGB images of man-made scenes are annotated with junctions  and lines, which describes the large-scale geometry of the scenes.  (Huang et al.) [Before 28/12/19]</li>
</ol>
<h2 id="Segmentation-General"><a href="#Segmentation-General" class="headerlink" title="Segmentation (General)"></a>Segmentation (General)</h2><ol>
<li><a href="https://www.ime.usp.br/~eduardob/datasets/sky/" target="_blank" rel="noopener">A Dataset for Sky Segmentation</a> - sentence describing it: This Sky dataset was used to evaluate the  method IFT-SLIC and other superpixel algorithms, using the  superpixel-based sky segmentation method proposed by Juraj Kostolansky.  It contains a collection of 60 images based on the Caltech Airplanes  Side dataset by R. Fergus  with ground truth for sky segmentation.  (Eduardo B. Alexandre, Paulo A. V. Miranda, R. Fergus) [Before 28/12/19]</li>
<li><a href="https://doi.org/10.5281/zenodo.168158" target="_blank" rel="noopener">Aberystwyth Leaf Evaluation Dataset</a> - Timelapse plant images with hand marked up leaf-level segmentations  for some time steps, and biological data from plant sacrifice. (Bell,  Jonathan; Dee, Hannah M.) [Before 28/12/19]</li>
<li><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank" rel="noopener">ADE20K</a> - 22+K hierarchically segmented and labeled scene images (900 scene  categories, 3+K classes and subpart classes) (Zhou, Zhao, Puig, Fidler,  Barriuso, Torralba) [Before 28/12/19]</li>
<li><a href="http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB/index.html" target="_blank" rel="noopener">Alpert et al. Segmentation evaluation database</a> (Sharon Alpert, Meirav Galun, Ronen Basri, Achi Brandt) [Before 28/12/19]</li>
<li><a href="http://bmc.iut-auvergne.com/" target="_blank" rel="noopener">BMC (Background Model Challenge)</a> - A dataset for comparing background subtraction algorithms, composed of real and synthetic videos(Antoine) [Before 28/12/19]</li>
<li><a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" target="_blank" rel="noopener">Berkeley Segmentation Dataset and Benchmark</a> (David Martin and Charless Fowlkes) [Before 28/12/19]</li>
<li><a href="https://zenodo.org/record/495570#.WZGTYl2lilM" target="_blank" rel="noopener">CAD 120 affordance dataset</a> - Pixelwise affordance annotation in human context (Sawatzky, Srikantha, Gall) [Before 28/12/19]</li>
<li><a href="http://cvteam.net/projects/2018/MM/40Dataset.rar" target="_blank" rel="noopener">COLT</a> - The dataset contains 40 imagenet categories with manually annotated per-pixel object masks. (Jia Li) [Before 28/12/19]</li>
<li><a href="https://github.com/jkoteswarrao/Object-Co-skeletonization-with-Co-segmentation" target="_blank" rel="noopener">CO-SKEL dataset</a> - This dataset consists of categorized skeleton and segmentation masks  for evaluating co-skeletonization methods. (Koteswar Rao Jerripothula,  Jianfei Cai, Jiangbo Lu, Junsong Yuan) [Before 28/12/19]</li>
<li><a href="https://www.irit.fr/~Sylvie.Chambon/Crack_Detection_Database.html" target="_blank" rel="noopener">Crack detection on 2D pavement images</a> - five sets of pavement images that contain cracks with the manual  ground truth associated and 5 automatic segmentations obtained with  existing approaches (Sylvie Chambon) [Before 28/12/19]</li>
<li><a href="http://clopema.felk.cvut.cz/color_and_depth_dataset.html" target="_blank" rel="noopener">CTU Color and Depth Image Dataset of Spread Garments</a> - Images of spread garments with annotated corners. (Wagner, L., Krejov D., and Smutn V. (Czech Technical University in Prague)) [Before  28/12/19]</li>
<li><a href="http://clopema.felk.cvut.cz/garment_folding_photo_dataset.html" target="_blank" rel="noopener">CTU Garment Folding Photo Dataset</a> - Color and depth images from various stages of garment folding.  (Sushkov R., Melkumov I., Smutn y V. (Czech Technical University in  Prague)) [Before 28/12/19]</li>
<li><a href="http://www.cs.sfu.ca/~hamarneh/software/DeformIt/index.html" target="_blank" rel="noopener">DeformIt 2.0</a> - Image Data Augmentation Tool: Simulate novel images with ground truth segmentations from a single image-segmentation pair (Brian Booth and  Ghassan Hamarneh) [Before 28/12/19]</li>
<li><a href="https://better-flow.github.io/evimo/" target="_blank" rel="noopener">EVIMO</a> - Dataset  for motion segmentation, egomotion estimation and tracking using an  event camera; the dataset is collected with DAVIS 346C and provides 3D  poses for camera and independently moving objects, and pixelwise motion  segmentation masks. (Mitrokhin, Ye, Fermuller, Aloimonos, Delbruck)  [14/1/20]</li>
<li><a href="http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/segmentation/grabcut.htm" target="_blank" rel="noopener">GrabCut Image database</a> (C. Rother, V. Kolmogorov, A. Blake, M. Brown) [Before 28/12/19]</li>
<li><a href="http://medisp.bme.teiath.gr/hicl/index.html" target="_blank" rel="noopener">Histology Image Collection Library (HICL)</a> - The HICL is a compilation of 3870histopathological images (so far)  from various diseases, such as brain cancer,breast cancer and HPV (Human Papilloma Virus)-Cervical cancer. (Medical Image and Signal Processing  (MEDISP) Lab., Department of BiomedicalEngineering, School of  Engineering, University of West Attica) [Before 28/12/19]</li>
<li><a href="http://smartdoc.univ-lr.fr" target="_blank" rel="noopener">ICDAR’15 Smartphone document capture and OCR competition - challenge 1</a> - videos of documents filmed by a user with a smartphone to simulate  mobile document capture, and ground truth coordinates of the document  corners to detect. (Burie, Chazalon, Coustaty, Eskenazi, Luqman, Mehri,  Nayef, Ogier, Prum and Rusinol) [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/" target="_blank" rel="noopener">Intrinsic Images in the Wild (IIW)</a> -  Intrinsic Images in the Wild, is a large-scale, public dataset for  evaluating intrinsic image decompositions of indoor scenes (Sean Bell,  Kavita Bala, Noah Snavely) [Before 28/12/19]</li>
<li><a href="http://people.csail.mit.edu/brussell/research/LabelMe/intro.html" target="_blank" rel="noopener">LabelMe images database and online annotation tool</a> (Bryan Russell, Antonio Torralba, Kevin Murphy, William Freeman) [Before 28/12/19]</li>
<li><a href="http://www.lits-challenge.com" target="_blank" rel="noopener">LITS Liver Tumor Segmentation</a> - 130 3D CT scans with segmentations of the liver and liver tumor.  Public benchmark with leaderboard at Codalab.org (Patrick Christ)  [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/minc/" target="_blank" rel="noopener">Materials in Context (MINC)</a> - The Materials in Context Database (MINC) builds on OpenSurfaces, but  includes millions of point annotations of material labels. (Sean Bell,  Paul Upchurch, Noah Snavely, Kavita Bala) [Before 28/12/19]</li>
<li><a href="https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network" target="_blank" rel="noopener"> Multi-species fruit flower detection</a> - This dataset consists of four sets of flower images, from three  different tree species: apple, peach, and pear, and accompanying ground  truth images. (Philipe A. Dias, Amy Tabb, Henry Medeiros) [Before  28/12/19]</li>
<li><a href="http://www.vision.ime.usp.br/~lucyacm/thesis/coift.html" target="_blank" rel="noopener"> Objects with thin and elongated parts</a> - The three datasets used to evaluate our method Oriented Image  Foresting Transform with Connectivity Constraints, which contain objects with thin and elongated parts. These databases are composed of 280  public images of birds and insects with ground truths. (Lucy A. C.  Mansilla (IME-USP), Paulo A. V. Miranda) [Before 28/12/19]</li>
<li><a href="http://opensurfaces.cs.cornell.edu/publications/opensurfaces/" target="_blank" rel="noopener">OpenSurfaces</a> - OpenSurfaces consists of tens of thousands of examples of surfaces  segmented from consumer photographs of interiors, and annotated with  material parameters, texture information, and contextual information .  (Kavita Bala et al.) [Before 28/12/19]</li>
<li><a href="https://ikw.uos.de/~cv/projects/mm-mkv" target="_blank" rel="noopener">Osnabrück gaze tracking data</a> - 318 video sequences from several different gaze tracking data sets  with polygon based object annotation. (Schöning, Faion, Heidemann,  Krumnack, Gert, Açik, Kietzmann, Heidemann &amp; König) [Before  28/12/19]</li>
<li><a href="http://www.jifengdai.org/downloads/scribble_sup/" target="_blank" rel="noopener">PASCAL-Scribble Dataset</a> - Our  PASCAL-Scribble Dataset provides scribble-annotations on 59 object/stuff categories. (Di Lin) [Before 28/12/19]</li>
<li><a href="http://lrs.icg.tugraz.at/research/petroglyphsegmentation/" target="_blank" rel="noopener">PetroSurf3D</a> - 26 high resolution (sub-millimeter accuracy) 3D scans of rock art  with pixelwise labeling of petroglyphs for segmentation. (Poier, Seidl,  Zeppelzauer, Reinbacher, Schaich, Bellandi, Marretta, Bischof) [Before  28/12/19]</li>
<li><a href="http://sailvos.web.illinois.edu" target="_blank" rel="noopener">SAIL-VOS</a> - The  Semantic Amodal Instance Level Video Object Segmentation (SAIL-VOS)  dataset provides accurate ground truth annotations to develop methods  for reasoning about occluded parts of objects while enabling to take  temporal information into account (Hu, Chen, Hui, Huang, Schwing)  [29/12/19]</li>
<li><a href="http://doi.org/10.5281/zenodo.59019" target="_blank" rel="noopener">Shadow Detection/Texture Segmentation Computer Vision Dataset</a> - Video based sequences for shadow detection/suppression, with ground  truth (Newey, C., Jones, O., &amp; Dee, H. M.) [Before 28/12/19]</li>
<li><a href="http://www.synthia-dataset.net" target="_blank" rel="noopener">SYNTHIA</a> - Large set  (~half million) of virtual-world images for training autonomous cars to  see. (ADAS Group at Computer Vision Center) [Before 28/12/19]</li>
<li><a href="http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip" target="_blank" rel="noopener">Stony Brook University Shadow Dataset (SBU-Shadow5k)</a> - Large scale shadow detection dataset from a wide variety of scenes  and photo types, with human annotations (Tomas F.Y. Vicente, Le Hou,  Chen-Ping Yu, Minh Hoai, Dimitris Samaras) [Before 28/12/19]</li>
<li><a href="https://gitlab.com/nicstrisc/RUSTICO/tree/master/data" target="_blank" rel="noopener">TB-roses-v1</a> – data set of rose bush images with ground truth for evaluation of rose stem segmentation (TrimBot2020 consortium) [26/2/20]</li>
<li><a href="http://www.tromai.icoc.me/" target="_blank" rel="noopener">TRoM: Tsinghua Road Markings</a> - This is a dataset which contributes to the area of road marking  segmentation for Automated Driving and ADAS. (Xiaolong Liu, Zhidong  Deng, Lele Cao, Hongchao Lu) [Before 28/12/19]</li>
<li><a href="https://ivi.fnwi.uva.nl/cv/intrinseg" target="_blank" rel="noopener">UVA Intrinsic Images and Semantic Segmentation Dataset</a> - RGB dataset with ground-truth albedo, shading, and semantic  annotations (Baslamisli, Groenestege, Das, Le, Karaoglu, Gevers)&gt;  [Before 28/12/19]</li>
<li><a href="http://cvteam.net/projects/TIP18-VOS/VOS-Dataset.zip" target="_blank" rel="noopener">VOS</a> - A dataset with 200 Internet videos for video-based salient object  detection and segmentation. (Jia Li, Changqun Xia) [Before 28/12/19]</li>
<li><a href="http://cvteam.net/projects/CVPR17-ELE/XPIE.tar.gz" target="_blank" rel="noopener">XPIE</a> - An image dataset with 10000 images containing manually annotated  salient objects and 8596 containing no salient objects. (Jia Li,  Changqun Xia) [Before 28/12/19]</li>
</ol>
<h2 id="Simultaneous-Localization-and-Mapping"><a href="#Simultaneous-Localization-and-Mapping" class="headerlink" title="Simultaneous Localization and Mapping"></a>Simultaneous Localization and Mapping</h2><ol>
<li><a href="https://github.com/torrvision/CollaborativeSLAMDataset" target="_blank" rel="noopener">Collaborative SLAM Dataset (CSD)</a> - The dataset consists of four different subsets - Flat, House, Priory  and Lab - each containing several RGB-D sequences that can be  reconstructed and successfully relocalised against each other to form a  combined 3D model. Each sequence was captured using an Asus ZenFone AR,  and we provide an accurate local 6D pose for each RGB-D frame in the  dataset. We also provide the calibration parameters for the depth and  colour sensors, optimised global poses for the sequences in each subset, and a pre-built mesh of each sequence. (Golodetz, Cavallari, Lord,  Prisacariu, Murray, Torr) [Before 28/12/19]</li>
<li><a href="http://rpg.ifi.uzh.ch/davis_data.html" target="_blank" rel="noopener">Event-Camera Data for Pose Estimation, Visual Odometry, and SLAM</a>The data also include intensity images, inertial measurements, and ground  truth from a motion-capture system. (ETH) [Before 28/12/19]</li>
<li><a href="https://better-flow.github.io/evimo/" target="_blank" rel="noopener">EVIMO</a> - Dataset  for motion segmentation, egomotion estimation and tracking using an  event camera; the dataset is collected with DAVIS 346C and provides 3D  poses for camera and independently moving objects, and pixelwise motion  segmentation masks. (Mitrokhin, Ye, Fermuller, Aloimonos, Delbruck)  [14/1/20]</li>
<li><a href="https://github.com/facebookresearch/House3D" target="_blank" rel="noopener">House3D</a> -  House3D is a virtual 3D environment which consists of thousands of  indoor scenes equipped with a diverse set of scene types, layouts and  objects sourced from the SUNCG dataset. It consists of over 45k indoor  3D scenes, ranging from studios to two-storied houses with swimming  pools and fitness rooms. All 3D objects are fully annotated with  category labels. Agents in the environment have access to observations  of multiple modalities, including RGB images, depth, segmentation masks  and top-down 2D map views. The renderer runs at thousands frames per  second, making it suitable for large-scale RL training. (Yi Wu, Yuxin  Wu, Georgia Gkioxari, Yuandong Tian, facebook research) [Before  28/12/19]</li>
<li><a href="http://rpg.ifi.uzh.ch/datasets/mav_circle.tar.gz" target="_blank" rel="noopener">Indoor Dataset of Quadrotor with Down-Looking Camera</a> - This dataset contains the recording of the raw images, IMU  measurements as well as the ground truth poses of a quadrotor flying a  circular trajectory in an office size environment. (Scaramuzza, ETH  Zurich, University of Zurich) [Before 28/12/19]</li>
<li><a href="http://www.ok.sc.e.titech.ac.jp/INLOC/" target="_blank" rel="noopener">InLoc</a> -  Benchmark for evaluating the accuracy of 6DoF visual localization  algorithms in challenging indoor scenarios. (Hajime Taira, Masatoshi  Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic,  Tomas Pajdla, Akihiko Torii) [Before 28/12/19]</li>
<li><a href="http://visuallocalization.net/" target="_blank" rel="noopener">Long-term visual localization</a> - TBenchmark for evaluating visual localization and mapping algorithms  under various illumination and seasonal condition. (Torsten Sattler,  Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik  Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, Tomas Pajdla) [Before 28/12/19]</li>
<li><a href="https://hijeffery.github.io/PanoNavi/" target="_blank" rel="noopener">PanoNavi dataset</a> - A panoramic dataset for robot navigation, consisted of 5 videos lasting about 1 hour. (Lingyan Ran) [Before 28/12/19]</li>
<li><a href="http://www.rawseeds.org/home/category/benchmarking-toolkit/datasets/" target="_blank" rel="noopener">RAWSEEDS SLAM benchmark datasets</a> (Rawseeds Project) [Before 28/12/19]</li>
<li><a href="https://figshare.com/articles/Rijksmuseum_Challenge_2014/5660617" target="_blank" rel="noopener">Rijksmuseum Challenge 2014</a> - It consist of 100K art objects from the rijksmuseum and comes with an extensive xml files describing each object. (Thomas Mensink and Jan van Gemert) [Before 28/12/19]</li>
<li><a href="http://www.bici-lab.org/projects/visual-paths-navigation" target="_blank" rel="noopener">RSM dataset of Visual Paths</a> - Visual dataset of indoor spaces to benchmark localisation/navigation  methods. It consists of 1.5 km of corridors and indoor spaces with  ground truth for every frame, measured as distance in centimetres from  starting point. Includes a synthetically generated corridor for  benchmark. (Jose Rivera-Rubio, Ioannis Alexiou, Anil A. Bharath) [Before 28/12/19]</li>
<li><a href="https://daniilidis-group.github.io/mvsec/" target="_blank" rel="noopener">The Multi Vehicle Stereo Event Camera Dataset</a> - Multiple sequences containing a stereo pair of DAVIS 346b event  cameras with ground truth poses, depth maps and optical flow. (lex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, Kostas  Daniilidis) [Before 28/12/19]</li>
<li><a href="https://vision.in.tum.de/data/datasets/rgbd-dataset" target="_blank" rel="noopener">TUM RGB-D Benchmark</a> - Dataset and benchmark for the evaluation of RGB-D visual odometry and SLAM algorithms (BCrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard and Daniel Cremers) [Before 28/12/19]</li>
<li><a href="https://vision.in.tum.de/data/datasets/visual-inertial-dataset" target="_blank" rel="noopener">TUM VI Benchmark</a> - 28 sequences, indoor and outdoor, sensor data from stereo camera and  IMU, accurate ground truth at beginning and end segments. (David  Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, Joerg  Stueckler, Daniel Cremers) [Before 28/12/19]</li>
<li><a href="http://www.cvlibs.net/datasets/kitti/eval_odometry.php" target="_blank" rel="noopener">Visual Odometry / SLAM Evaluation</a> - The odometry benchmark consists of 22 stereo sequences (Andreas Geiger and Philip Lenz and Raquel Urtasun) [Before 28/12/19]</li>
<li><a href="https://www.hs-karlsruhe.de/odometry-data/" target="_blank" rel="noopener">Visual Odometry Dataset with Plenoptic and Stereo Data</a> - The dataset contains 11 sequences recorded by a hand-held platform  consisting of a plenoptic camera and a pair of stereo cameras. The  sequences comprising different indoor and outdoor sequences with  trajectory length ranging from 25 meters up to several hundred meters.  The recorded sequences show moving objects as well as changing lighting  conditions. (Niclas Zeller and Franz Quint, Hochschule Karlsruhe,  Karlsruhe University of Applied Sciences) [Before 28/12/19]</li>
</ol>
<h2 id="Surveillance-and-Tracking"><a href="#Surveillance-and-Tracking" class="headerlink" title="Surveillance and Tracking"></a>Surveillance and Tracking</h2><ol>
<li><a href="http://dixie.udg.edu/udgms/" target="_blank" rel="noopener">A collection of challenging motion segmentation benchmark datasets</a> - These datasets enclose real-life long and short sequences, with  increased number of motions and frames per sequence, and also real  distortions with missing data. The ground truth is provided on all the  frames of all the sequences. (Muhammad Habib Mahmood, Yago Diez, Joaquim Salvi, Xavier Llado) [Before 28/12/19]</li>
<li><a href="http://research.sethi.org/ricky/datasets/" target="_blank" rel="noopener">ATOMIC GROUP ACTIONS dataset</a> - (Ricky J. Sethi et al.) [Before 28/12/19]</li>
<li><a href="https://www.aiia.csd.auth.gr/LAB_PROJECTS/MULTIDRONE/AUTH_MULTIDRONE_Dataset.html" target="_blank" rel="noopener">AUT MULTIDRONE video dataset for racing bicycle detection/tracking from UAV footage</a> -  7 Youtube videos (resolution: 1920 x 1080) at 25fps (Mademlis) [Before 28/12/19]</li>
<li><a href="http://www.eecs.qmul.ac.uk/~andrea/avss2007_d.html" target="_blank" rel="noopener">AVSS07: Advanced Video and Signal based Surveillance 2007 datasets</a> (Andrea Cavallaro) [Before 28/12/19]</li>
<li><a href="http://www.idiap.ch/~odobez/RESSOURCES/DataRelease-TrafficJunction.php" target="_blank" rel="noopener">Activity modeling and abnormality detection dataset</a> - The dataset containes a 45 minutes video with annotated anomalies. (Jagan Varadarajan and Jean-Marc Odobez) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/backgroundsubtraction/test-sequences" target="_blank" rel="noopener">Background subtraction</a> - a list of datasets about background subtraction(Thierry BOUWMANS ) [Before 28/12/19]</li>
<li><a href="https://www.uow.edu.au/~wanqing/#Datasets" target="_blank" rel="noopener">CAMO-UOW Dataset</a> - 10 high resolution videos captured in real scenes for camouflaged  background subtraction (Shuai Li and Wanqing Li) [Before 28/12/19]</li>
<li><a href="http://rose1.ntu.edu.sg/Datasets/cctvFights.asp" target="_blank" rel="noopener">CCTV-Fights</a> - 1,000 videos picturing real-world fights, recorded from CCTVs or  mobile cameras, and temporally annotated at the frame level. (Mauricio  Perez, ROSE Lab, NTU) [Before 28/12/19]</li>
<li><a href="http://www.consortium.ri.cmu.edu/projSRD.php" target="_blank" rel="noopener">CMUSRD: Surveillance Research Dataset</a> - multi-camera video for indoor surveillance scenario (K. Hattori, H. Hattori, et al) [Before 28/12/19]</li>
<li><a href="http://vision.cs.duke.edu/DukeMTMC/" target="_blank" rel="noopener">DukeMTMC: Duke Multi-Target Multi-Camera tracking dataset</a> - 8 cameras, 85 min, 2m frames, 2000 people of video (Ergys Ristani,  Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo Tomasi) [Before  28/12/19]</li>
<li><a href="https://github.com/layumi/DukeMTMC-reID_evaluation" target="_blank" rel="noopener">DukeMTMC-reID</a> - A subset of the DukeMTMC for image-based person re-identification (8  cameras,16,522 training images of 702 identities, 2,228 query images of  the other 702 identities and 17,661 gallery images.) (Zheng, Zheng, and  Yang) [Before 28/12/19]</li>
<li><a href="http://www-sop.inria.fr/orion/ETISEO/download.htm" target="_blank" rel="noopener">ETISEO Video Surveillance Download Datasets</a> (INRIA Orion Team and others) [Before 28/12/19]</li>
<li><a href="http://cmp.felk.cvut.cz/fmo/" target="_blank" rel="noopener">FMO dataset</a> - FMO dataset contains annotated video sequences with Fast Moving Objects - objects  which move over a projected distance larger than their size in one  frame. (Denys Rozumnyi, Jan Kotera, Lukas Novotny, Ales Hrabalik, Filip  Sroubek, Jiri Matas) [Before 28/12/19]</li>
<li><a href="http://vislab.isr.ist.utl.pt/hda-dataset/" target="_blank" rel="noopener">HDA+ Multi-camera Surveillance Dataset</a> - video from a network of 18 heterogeneous cameras (different  resolutions and frame rates) distributed over 3 floors of a research  institute with 13 fully labeled sequences, 85 persons, and 64028  bounding boxes of persons. (D. Figueira, M. Taiana, A. Nambiar, J.  Nascimento and A. Bernardino) [Before 28/12/19]</li>
<li><a href="https://github.com/RichieZh/HIC" target="_blank" rel="noopener">Human click data</a> - 20K human clicks on a tracking target (including click errors) (Zhu and Porikli) [Before 28/12/19]</li>
<li><a href="http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html" target="_blank" rel="noopener">Immediacy Dataset</a> - This dataset is designed for estimation personal relationships. (Xiao Chu et al.) [Before 28/12/19]</li>
<li><a href="http://mahnob-db.eu/" target="_blank" rel="noopener">MAHNOB Databases</a> -including Laughter Database,HCI-tagging Database,MHI-Mimicry Database( M. Pantic. etc.) [Before 28/12/19]</li>
<li><a href="http://s.fhg.de/mini-rgbd" target="_blank" rel="noopener">Moving INfants In RGB-D (MINI-RGBD)</a> - A synthetic, realistic RGB-D data set for infant pose estimation  containing 12 sequences of moving infants with ground truth joint  positions. (N. Hesse, C. Bodensteiner, M. Arens, U. G. Hofmann, R.  Weinberger, A. S. Schroeder) [Before 28/12/19]</li>
<li><a href="https://www.pkuvmc.com/publications/msmt17.html" target="_blank" rel="noopener">MSMT17</a> - Person re-identification dataset. 180 hours of videos, 12 outdoor  cameras, 3 indoor cameras, and 12 time slots. (Wei Longhui, Zhang  Shiliang, Gao Wen, Tian Qi) [Before 28/12/19]</li>
<li><a href="https://multidrone.eu/" target="_blank" rel="noopener">MULTIDRONE boat detection/tracking</a> - 3 HD videos (720p - 1280 x 720) subsampled at 25 fps (Mademlis,) [Before 28/12/19]</li>
<li><a href="http://community.wvu.edu/~samotiian/datasets.html" target="_blank" rel="noopener">MVHAUS-PI</a> - a multi-view human interaction recognition dataset (Saeid et al.) [Before 28/12/19]</li>
<li><a href="http://ilt.u-bourgogne.fr/benezeth/projects/ICRA2014/index.html" target="_blank" rel="noopener">Multispectral visible-NIR video sequences</a> - Annotated multispectral video, visible + NIR (LE2I, Universit de Bourgogne) [Before 28/12/19]</li>
<li><a href="http://www.openvisor.org/" target="_blank" rel="noopener">Openvisor - Video surveillance Online Repository</a> (Univ of Modena and Reggio Emilia) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html" target="_blank" rel="noopener">Parking-Lot dataset</a> - Parking-Lot dataset is a car dataset which focus on moderate and  heavily occlusions on cars in the parking lot scenario. (B. Li, T.F. Wu  and S.C. Zhu) [Before 28/12/19]</li>
<li><a href="https://sites.google.com/site/pornographydatabase/" target="_blank" rel="noopener">Pornography Database</a> - The Pornography database is a pornography detection dataset  containing nearly 80 hours of 400 pornographic and 400 non-pornographic  videos extracted from pornography websites and Youtube. (Avila, Thome,  Cord, Valle, de Araujo) [Before 28/12/19]</li>
<li><a href="http://tracking.cs.princeton.edu/" target="_blank" rel="noopener">Princeton Tracking Benchmark</a>  - 100 RGBD tracking datasets (Song and Xiao) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/thospeda/downloads.html" target="_blank" rel="noopener">QMUL Junction Dataset 1 and 2</a> - Videos of busy road junctions. Supports anomaly detection tasks. (T. Hospedales Edinburgh/QMUL) [Before 28/12/19]</li>
<li><a href="http://www.eecs.qmul.ac.uk/~xx302/ProjectPage/TCSVT_15/index.html" target="_blank" rel="noopener">Queen Mary Multi-Camera Distributed Traffic Scenes Dataset (QMDTS)</a> - The QMDTS is collected from urban surveillance environment for the  study of  surveillance behaviours in distributed scenes. (Dr. Xun Xu.  Prof. Shaogang Gong and Dr. Timothy Hospedales) [Before 28/12/19]</li>
<li><a href="http://radprojectbismil.blogspot.co.uk/" target="_blank" rel="noopener">Road Anomaly Detection</a> - 22km, 11 vehicles, normal + 4 defect categories (Hameed, Mazhar, Hassan) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org" target="_blank" rel="noopener">S-Hock dataset</a> - A new Benchmark for Spectator Crowd Analysis. (Francesco Setti,  Davide Conigliaro, Paolo Rota, Chiara Bassetti, Nicola Conci, Nicu Sebe, Marco Cristani) [Before 28/12/19]</li>
<li><a href="http://tev.fbk.eu/salsa" target="_blank" rel="noopener">SALSA: Synergetic sociAL Scene Analysis</a> - A Novel Dataset for Multimodal Group Behavior Analysis(Xavier Alameda-Pineda etc.) [Before 28/12/19]</li>
<li><a href="http://scenebackgroundmodeling.net/" target="_blank" rel="noopener">SBMnet (Scene Background Modeling.NET)</a> - A dataset for testing background estimation algorithms(Jodoin, Maddalena, and Petrosino) [Before 28/12/19]</li>
<li><a href="http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html" target="_blank" rel="noopener">SBM-RGBD dataset</a> - 35 Kinect indoor RGBD videos to evaluate and compare scene background modelling methods for moving object detection (Camplani, Maddalena,  Moy?? Alcover, Petrosino, Salgado) [Before 28/12/19]</li>
<li><a href="http://uti.eu.com/pncd-scouter/rezultate-en.html" target="_blank" rel="noopener">SCOUTER</a> - video surveillance ground truthing (shifting perspectives, different  setups/lighting conditions, large variations of subject). 30 videos and  approximately 36,000 manually labeled frames. (Catalin Mitrea) [Before  28/12/19]</li>
<li><a href="http://best.sjtu.edu.cn/" target="_blank" rel="noopener">SJTU-BEST</a>One  surveillance-specified datasets platform with realistic, on-using  camera-captured, diverse set of surveillance images and videos (Shanghai Jiao Tong University) [Before 28/12/19]</li>
<li><a href="http://www.eecs.qmul.ac.uk/~andrea/spevi.html" target="_blank" rel="noopener">SPEVI: Surveillance Performance EValuation Initiative</a> (Queen Mary University London) [Before 28/12/19]</li>
<li><a href="http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/" target="_blank" rel="noopener">Shinpuhkan 2014</a> - A Person Re-identification dataset containing 22,000 images of 24  people captured by 16 cameras. (Yasutomo Kawanishi et al.) [Before  28/12/19]</li>
<li><a href="http://cvgl.stanford.edu/projects/uav_data/" target="_blank" rel="noopener">Stanford Drone Dataset</a> - 60 images and videos of various types of agents (not just  pedestrians, but also bicyclists, skateboarders, cars, buses, and golf  carts) that navigate in a real world outdoor environment such as a  university campus (Robicquet, Sadeghian, Alahi, Savarese) [Before  28/12/19]</li>
<li><a href="https://www.vis.uni-stuttgart.de/en/research/visual_analytics/visual-analytics-video-steams/stuttgart_artificial_background_subtraction_dataset/index.html" target="_blank" rel="noopener">Stuttgart Artificial Background Subtraction Dataset</a> [Before 28/12/19]</li>
<li><a href="https://www.dropbox.com/s/2rhugw868em465r/DSTdataset.zip?dl=0" target="_blank" rel="noopener">Tracking in extremely cluttered scenes</a> - this single object tracking dataset has 28 highly cluttered sequences with per frame annotation(Jingjing Xiao,Linbo Qiao,Rustam Stolkin,Ale  Leonardis) [Before 28/12/19]</li>
<li><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/tracking-net.org" target="_blank" rel="noopener">TrackingNet</a> - Large-scale dataset for tracking in the wild: more than 30k annotated sequences for training, more than 500 sequestered sequences for  testing, evaluation server and leaderboard for fair ranking. (Matthias  Muller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi and Bernard  Ghanem) [Before 28/12/19]</li>
<li><a href="http://crcv.ucf.edu/projects/real-world/" target="_blank" rel="noopener">UCF-Crime Dataset: Real-world Anomaly Detection in Surveillance Videos</a> - A large-scale dataset for real-world anomaly detection in  surveillance videos. It consists of 1900 long and untrimmed real-world  surveillance videos (of 128 hours), with 13 realistic anomalies such as  fighting, road accident, burglary, robbery, etc. as well as normal  activities. (Center for Research in Computer Vision, University of  Central Florida) [Before 28/12/19]</li>
<li><a href="http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html" target="_blank" rel="noopener">UCLA Aerial Event Dataset</a> - Human activities in aerial videos with annotations of people,  objects, social groups, activities and roles (Shu, Xie, Rothrock,  Todorovic, and Zhu) [Before 28/12/19]</li>
<li><a href="http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm" target="_blank" rel="noopener">UCSD Anomaly Detection Dataset</a> - a stationary camera mounted at an elevation, overlooking pedestrian  walkways, with unusual pedestrian or non-pedestrian motion. [Before  28/12/19]</li>
<li><a href="http://cvrr.ucsd.edu/bmorris/datasets/" target="_blank" rel="noopener">UCSD trajectory clustering and analysis datasets</a> - (Morris and Trivedi) [Before 28/12/19]</li>
<li><a href="http://research.sethi.org/ricky/datasets/" target="_blank" rel="noopener">USC Information Sciences Institute’s ATOMIC PAIR ACTIONS dataset</a> - (Ricky J. Sethi et al.) [Before 28/12/19]</li>
<li><a href="http://avires.dimi.uniud.it/papers/trclust/" target="_blank" rel="noopener">Udine Trajectory-based anomalous event detection dataset</a> - synthetic trajectory datasets with outliers (Univ of Udine Artificial Vision and Real Time Systems Laboratory) [Before 28/12/19]</li>
<li><a href="http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html" target="_blank" rel="noopener">Visual Tracker Benchmark</a> - 100 object tracking sequences with ground truth with <a href="http://www.visual-tracking.net" target="_blank" rel="noopener">Visual Tracker Benchmark evaluation</a>, including tracking results from a number of trackers (Wu, Lim, Yang) [Before 28/12/19]</li>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html" target="_blank" rel="noopener">WIDER Attribute Dataset</a> - WIDER Attribute is a large-scale human attribute dataset, with 13789  images belonging to 30 scene categories, and 57524 human bounding boxes  each annotated with 14 binary attributes. (Li, Yining and Huang, Chen  and Loy, Chen Change and Tang, Xiaoou) [Before 28/12/19]</li>
</ol>
]]></content>
      <categories>
        <category>Repost</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Machine Learning</tag>
        <tag>Dataset</tag>
        <tag>CV</tag>
      </tags>
  </entry>
</search>
